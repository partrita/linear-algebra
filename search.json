[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Linear Algebra",
    "section": "",
    "text": "목차\n\n제1장. 벡터, 스칼라, 그리고 기하학\n\n스칼라, 벡터, 그리고 좌표계 (무엇이며 왜 중요한가)\n벡터 표기법, 성분, 그리고 화살표 (벡터 읽고 쓰기)\n벡터 덧셈과 스칼라 곱셈 (두 가지 기본 동작)\n선형 결합과 생성(Span) (기존 벡터로 새 벡터 만들기)\n길이(노름)와 거리 (얼마나 크고 얼마나 먼가)\n내적 (대수적 및 기하학적 관점)\n벡터 사이의 각도와 코사인 (정렬 측정하기)\n투영과 분해 (방향에 따라 나누기)\n코시-슈바르츠 및 삼각 부등식 (두 가지 근본적인 경계)\nℝ²/ℝ³에서의 정규직교 집합 (이미 알고 있는 좋은 기저들)\n\n\n\n제2장. 행렬과 기본 연산\n\n표로서의 행렬과 기계로서의 행렬 (두 가지 사고 모델)\n행렬 모양, 인덱싱, 그리고 블록 뷰 (구조 보기)\n행렬 덧셈과 스칼라 곱셈 (성분별 규칙)\n행렬-벡터 곱 (열의 선형 결합)\n행렬-행렬 곱 (선형 단계의 구성)\n항등행렬, 역행렬, 그리고 전치행렬 (세 명의 특별한 친구들)\n대칭, 대각, 삼각, 그리고 치환 행렬 (특별한 가족들)\n대각합(Trace)과 기본 행렬 성질 (빠른 불변량)\n아핀 변환과 동차 좌표 (평행이동 포함)\n행렬 계산 (비용 계산과 간단한 가속)\n\n\n\n제3장. 선형 시스템과 소거법\n\n방정식에서 행렬로 (확장 및 인코딩)\n기본 행 연산 (해를 유지하는 합법적 이동)\n행 사다리꼴 및 기약 행 사다리꼴 (목표 형태)\n피벗, 자유 변수, 그리고 선행 1 (해 읽기)\n일관된 시스템 풀기 (유일해 vs 무한해)\n불일치 감지 (해가 없을 때)\n손으로 하는 가우스 소거법 (규율 있는 절차)\n후방 대입과 해 집합 (깔끔하게 마무리하기)\n랭크(Rank)와 그 첫 번째 의미 (정보로서의 피벗)\nLU 분해 (L과 U로 포착된 소거)\n\n\n\n제4장. 벡터 공간과 부분 공간\n\n벡터 공간의 공리 (“공간”의 진정한 의미)\n부분 공간, 열 공간, 그리고 영 공간 (해가 사는 곳)\n생성(Span)과 생성 집합 (공간의 범위)\n선형 독립과 종속 (비중복 vs 중복)\n기저와 좌표 (모든 벡터에 고유하게 이름 붙이기)\n차원 (얼마나 많은 방향인가)\n랭크-널리티 정리 (더해지는 차원들)\n기저에 대한 좌표 (“자” 바꾸기)\n기저 변환 행렬 (좌표계 사이 이동)\n아핀 부분 공간 (원점을 지나지 않는 선과 평면)\n\n\n\n제5장. 선형 변환과 구조\n\n선형 변환 (선과 합 보존)\n선형 맵의 행렬 표현 (기저 선택)\n커널(Kernel)과 이미지(Image) (사라지는 입력; 도달할 수 있는 출력)\n가역성과 동형 사상 (완벽하게 되돌릴 수 있는 맵)\n합성, 거듭제곱, 그리고 반복 (계속해서 수행하기)\n닮음과 켤레 (같은 작용, 다른 기저)\n투영과 반사 (멱등 및 대합 맵)\n회전과 전단 (기하학적 직관)\n랭크와 연산자 관점 (소거법 너머의 랭크)\n블록 행렬과 블록 맵 (분할 정복 구조)\n\n\n\n제6장. 행렬식(Determinant)과 부피\n\n면적, 부피, 그리고 부호 있는 스케일 인자 (기하학적 진입점)\n선형 규칙을 통한 행렬식 (다중 선형성, 부호, 정규화)\n행렬식과 기본 행 연산 (각 이동이 행렬식을 어떻게 바꾸는가)\n삼각 행렬과 대각선의 곱 (빠른 승리)\ndet(AB) = det(A)det(B) (곱셈의 마법)\n가역성과 0인 행렬식 (납작함 vs 꽉 찬 부피)\n여인수 전개 (라플라스 방법)\n치환과 부호 (조합론적 핵심)\n크라메르의 규칙 (행렬식으로 풀기, 그리고 쓰지 말아야 할 때)\n실전에서의 행렬식 계산 (LU 사용, 안정성 유의)\n\n\n\n제7장. 고유값, 고유벡터, 그리고 동역학\n\n고유값과 고유벡터 (가만히 있는 방향들)\n특성 다항식 (고유값이 나오는 곳)\n대수적 vs 기하학적 중복도 (얼마나 많고 얼마나 독립적인가)\n대각화 (행렬이 단순해질 때)\n행렬의 거듭제곱 (고유값을 통한 장기적 거동)\n실수 vs 복소수 스펙트럼 (회전과 진동)\n결함 행렬과 조르당 표준형 엿보기 (대각화가 실패할 때)\n안정성과 스펙트럼 반경 (성장, 감쇠, 또는 진동)\n마르코프 체인과 정상 상태 (선형대수학으로서의 확률)\n선형 미분 시스템 (고유 분해를 통한 해)\n\n\n\n제8장. 직교성, 최소 제곱, 그리고 QR\n\n내적 너머의 내적 (각도의 맞춤형 개념)\n직교성과 정규직교 기저 (수직의 힘)\n그람-슈미트 과정 (정규직교 기저 구성하기)\n부분 공간으로의 직교 투영 (최단 거리 원리)\n최소 제곱 문제 (정확한 해가 불가능할 때 맞추기)\n정규 방정식과 잔차의 기하학 (왜 작동하는가)\nQR 분해 (직교성을 통한 안정적인 최소 제곱)\n직교 행렬 (길이를 보존하는 변환)\n푸리에 관점 (직교 파동으로의 확장)\n다항식 및 다중 특징 최소 제곱 (더 유연하게 맞추기)\n\n\n\n제9장. SVD, PCA, 그리고 조건화\n\n특이값과 SVD (보편적 분해)\nSVD의 기하학 (회전 + 늘리기)\n고유 분해와의 관계 (ATA 및 AAT)\n저랭크 근사 (최고의 작은 모델)\n주성분 분석 (분산과 방향)\n유사 역행렬 (Moore–Penrose)과 불량 조건 시스템 풀기\n조건화와 민감도 (오차가 어떻게 증폭되는가)\n행렬 노름과 특이값 (크기 제대로 측정하기)\n정규화 (불안정성을 길들이는 릿지/티호노프)\n랭크를 드러내는 QR과 실용적 진단 (랭크란 정말 무엇인가)\n\n\n\n제10장. 응용과 계산\n\n2D/3D 기하학 파이프라인 (카메라, 회전, 그리고 변환)\n컴퓨터 그래픽스와 로봇 공학 (작동 중인 동차 트릭)\n그래프, 인접성, 그리고 라플라시안 (행렬을 통한 네트워크)\n선형 연산으로서의 데이터 전처리 (중심화, 백색화, 스케일링)\n선형 회귀와 분류 (모델에서 행렬로)\n실전에서의 PCA (차원 축소 워크플로우)\n추천 시스템과 저랭크 모델 (누락된 항목 채우기)\n페이지랭크와 무작위 보행 (고유벡터로 순위 매기기)\n수치 선형대수학 필수 요소 (부동 소수점, BLAS/LAPACK)\n캡스톤 문제 세트와 다음 단계 (숙달로 가는 로드맵)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>목차</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html",
    "href": "books/en-US/book.html",
    "title": "The Book",
    "section": "",
    "text": "Overtune\nA soft opening, an invitation into the world of vectors and spaces, where each step begins a journey.\n1. Geometry’s Dawn\n2. Invitation to Learn\n3. Light and Shadow\n4. The Seed of Structure\n5. Whisper of Algebra\n6. Beginner’s Welcome\n7. Eternal Path",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-1.-vectors-scalars-and-geometry",
    "href": "books/en-US/book.html#chapter-1.-vectors-scalars-and-geometry",
    "title": "The Book",
    "section": "Chapter 1. Vectors, scalars, and geometry",
    "text": "Chapter 1. Vectors, scalars, and geometry\n\nOpening\nArrows in the air,\ndirections whisper softly—\nthe plane comes alive.\n\n\n1. Scalars, Vectors, and Coordinate Systems\nWhen we begin learning linear algebra, everything starts with the simplest building blocks: scalars and vectors. A scalar is just a single number, like 3, –7, or π. It carries only magnitude and no direction. Scalars are what we use for counting, measuring length, or scaling other objects up and down. A vector, by contrast, is an ordered collection of numbers. You can picture it as an arrow pointing somewhere in space, or simply as a list like (2, 5) in 2D or (1, –3, 4) in 3D. Where scalars measure “how much,” vectors measure both “how much” and “which way.”\n\nCoordinate Systems\nTo talk about vectors, we need a coordinate system. Imagine laying down two perpendicular axes on a sheet of paper: the x-axis (left to right) and the y-axis (up and down). Every point on the sheet can be described with two numbers: how far along the x-axis, and how far along the y-axis. This pair of numbers is a vector in 2D. Add a z-axis pointing up from the page, and you have 3D space. Each coordinate system gives us a way to describe vectors numerically, even though the underlying “space” is the same.\n\n\nVisualizing Scalars vs. Vectors\n\nA scalar is like a single tick mark on a ruler.\nA vector is like an arrow that starts at the origin (0, 0, …) and ends at the point defined by its components. For example, the vector (3, 4) in 2D points from the origin to the point 3 units along the x-axis and 4 units along the y-axis.\n\n\n\nWhy Start Here?\nUnderstanding the difference between scalars and vectors is the foundation for everything else in linear algebra. Every concept-matrices, linear transformations, eigenvalues-eventually reduces to how we manipulate vectors and scale them with scalars. Without this distinction, the rest of the subject would have no anchor.\n\n\nWhy It Matters\nNearly every field of science and engineering depends on this idea. Physics uses vectors for velocity, acceleration, and force. Computer graphics uses them to represent points, colors, and transformations. Data science treats entire datasets as high-dimensional vectors. By mastering scalars and vectors early, you unlock the language in which modern science and technology are written.\n\n\nTry It Yourself\n\nDraw an x- and y-axis on a piece of paper. Plot the vector (2, 3).\nNow draw the vector (–1, 4). Compare their directions and lengths.\nThink: which of these two vectors points “more upward”? Which is “longer”?\n\nThese simple experiments already give you intuition for the operations you’ll perform again and again in linear algebra.\n\n\n\n2. Vector Notation, Components, and Arrows\nLinear algebra gives us powerful ways to describe and manipulate vectors, but before we can do anything with them, we need a precise notation system. Notation is not just cosmetic-it tells us how to read, write, and think about vectors clearly and unambiguously. In this section, we’ll explore how vectors are written, how their components are represented, and how we can interpret them visually as arrows.\n\nWriting Vectors\nVectors are usually denoted by lowercase letters in bold (like \\(\\mathbf{v}, \\mathbf{w}, \\mathbf{x}\\))\nor with an arrow overhead (like \\(\\vec{v}\\)).\nFor instance, the vector \\(\\mathbf{v} = (2, 5)\\) is the same as \\(\\vec{v} = (2, 5)\\).\nThe style depends on context: mathematicians often use bold, physicists often use arrows.\nIn handwritten notes, people sometimes underline vectors (e.g., \\(\\underline{v}\\)) to avoid confusion with scalars.\nThe important thing is to distinguish vectors from scalars at a glance.\n\n\nComponents of a Vector\nA vector in two dimensions has two components, written as \\((x, y)\\).\nIn three dimensions, it has three components: \\((x, y, z)\\).\nMore generally, an \\(n\\)-dimensional vector has \\(n\\) components: \\((v_1, v_2, \\ldots, v_n)\\).\nEach component tells us how far the vector extends along one axis of the coordinate system.\nFor example:\n\n\\(\\mathbf{v} = (3, 4)\\) means the vector extends 3 units along the \\(x\\)-axis and 4 units along the \\(y\\)-axis.\n\\(\\mathbf{w} = (-2, 0, 5)\\) means the vector extends \\(-2\\) units along the \\(x\\)-axis, \\(0\\) along the \\(y\\)-axis, and 5 along the \\(z\\)-axis.\n\nWe often refer to the \\(i\\)-th component of a vector \\(\\mathbf{v}\\) as \\(v_i\\).\nSo, for \\(\\mathbf{v} = (3, 4, 5)\\), we have \\(v_1 = 3\\), \\(v_2 = 4\\), \\(v_3 = 5\\).\n\n\nColumn vs. Row Vectors\nVectors can be written in two common ways:\n\nAs a row vector: \\((v_1, v_2, v_3)\\)\n\nAs a column vector:\n\\[\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\\]\n\nBoth represent the same abstract object.\nRow vectors are convenient for quick writing, while column vectors are essential when we start multiplying by matrices, because the dimensions must align.\n\n\nVectors as Arrows\nThe most intuitive way to picture a vector is as an arrow:\n\nIt starts at the origin (0, 0, …).\nIt ends at the point given by its components.\n\nFor example, the vector (2, 3) in 2D is drawn as an arrow from (0, 0) to (2, 3). The arrow has both direction (where it points) and magnitude (its length). This geometric picture makes abstract algebraic manipulations much easier to grasp.\n\n\nPosition Vectors vs. Free Vectors\nThere are two common interpretations of vectors:\n\nPosition vector - a vector that points from the origin to a specific point in space. Example: (2, 3) is the position vector for the point (2, 3).\nFree vector - an arrow with length and direction, but not tied to a specific starting point. For instance, an arrow of length 5 pointing northeast can be drawn anywhere, but it still represents the same vector.\n\nIn linear algebra, we often treat vectors as free vectors, because their meaning does not depend on where they are drawn.\n\n\nExample: Reading a Vector\nSuppose u = (–3, 2).\n\nThe first component (–3) means move 3 units left along the x-axis.\nThe second component (2) means move 2 units up along the y-axis. So the arrow points to the point (–3, 2). Even without a diagram, the components tell us exactly what the arrow would look like.\n\n\n\nWhy It Matters\nClear notation is the backbone of linear algebra. Without it, equations quickly become unreadable, and intuition about direction and size is lost. The way we write vectors determines how easily we can connect the algebra (numbers and symbols) to the geometry (arrows and spaces). This dual perspective-symbolic and visual-is what makes linear algebra powerful and practical.\n\n\nTry It Yourself\n\nWrite down the vector (4, –1). Draw it on graph paper.\nRewrite the same vector as a column vector.\nTranslate the vector (4, –1) by moving its starting point to (2, 3) instead of the origin. Notice that the arrow looks the same-it just starts elsewhere.\nFor a harder challenge: draw the 3D vector (2, –1, 3). Even if you can’t draw perfectly in 3D, try to show each component along the x, y, and z axes.\n\nBy practicing both the notation and the arrow picture, you’ll develop fluency in switching between abstract symbols and concrete visualizations. This skill will make every later concept in linear algebra far more intuitive.\n\n\n\n3. Vector Addition and Scalar Multiplication\nOnce we know how to describe vectors with components and arrows, the next step is to learn how to combine them. Two fundamental operations form the backbone of linear algebra: adding vectors together and scaling vectors with numbers (scalars). These two moves, though simple, generate everything else we’ll build later. With them, we can describe motion, forces, data transformations, and more.\n\nVector Addition in Coordinates\nSuppose we have two vectors in 2D:\n\\(\\mathbf{u} = (u_1, u_2), \\quad \\mathbf{v} = (v_1, v_2)\\).\nTheir sum is defined as:\n\\[\n\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, \\; u_2 + v_2).\n\\]\nIn words, you add corresponding components.\nThis works in higher dimensions too:\n\\[\n(u_1, u_2, \\ldots, u_n) + (v_1, v_2, \\ldots, v_n) = (u_1 + v_1, \\; u_2 + v_2, \\; \\ldots, \\; u_n + v_n).\n\\]\nExample: \\[\n(2, 3) + (-1, 4) = (2 - 1, \\; 3 + 4) = (1, 7).\n\\]\n\n\nVector Addition as Geometry\nThe geometric picture is even more illuminating. If you draw vector u as an arrow, then place the tail of v at the head of u, the arrow from the start of u to the head of v is u + v. This is called the tip-to-tail rule. The parallelogram rule is another visualization: place u and v tail-to-tail, form a parallelogram, and the diagonal is their sum.\nExample: u = (3, 1), v = (2, 2). Draw both from the origin. Their sum (5, 3) is exactly the diagonal of the parallelogram they span.\n\n\nScalar Multiplication in Coordinates\nScalars stretch or shrink vectors.\nIf \\(\\mathbf{u} = (u_1, u_2, \\ldots, u_n)\\) and \\(c\\) is a scalar, then:\n\\[\nc \\cdot \\mathbf{u} = (c \\cdot u_1, \\; c \\cdot u_2, \\; \\ldots, \\; c \\cdot u_n).\n\\]\nExample:\n\\[\n2 \\cdot (3, 4) = (6, 8).\n\\]\n\\[\n(-1) \\cdot (3, 4) = (-3, -4).\n\\]\nMultiplying by a positive scalar stretches or compresses the arrow while keeping the direction the same. Multiplying by a negative scalar flips the arrow to point the opposite way.\n\n\nScalar Multiplication as Geometry\nImagine the vector (1, 2). Draw it on graph paper: it goes right 1, up 2. Now double it: (2, 4). The arrow points in the same direction but is twice as long. Halve it: (0.5, 1). It’s the same direction but shorter. Negate it: (–1, –2). Now the arrow points backward.\nThis geometric picture explains why we call these numbers “scalars”: they scale the vector.\n\n\nCombining Both: Linear Combinations\nVector addition and scalar multiplication are not just separate tricks-they combine to form the heart of linear algebra: linear combinations.\nA linear combination of vectors \\(u\\) and \\(v\\) is any vector of the form\n\\(a \\cdot u + b \\cdot v\\), where \\(a\\) and \\(b\\) are scalars.\nExample:\nIf \\(u = (1, 0)\\) and \\(v = (0, 1)\\), then\n\\(3 \\cdot u + 2 \\cdot v = (3, 2)\\).\nThis shows how any point on the grid can be reached by scaling and adding these two basic vectors.\nThat’s the essence of constructing spaces.\n\n\nAlgebraic Properties\nVector addition and scalar multiplication obey rules that mirror arithmetic with numbers:\n\nCommutativity: \\(u + v = v + u\\)\n\nAssociativity: \\((u + v) + w = u + (v + w)\\)\n\nDistributivity over scalars: \\(c \\cdot (u + v) = c \\cdot u + c \\cdot v\\)\n\nDistributivity over numbers: \\((a + b) \\cdot u = a \\cdot u + b \\cdot u\\)\n\nThese rules are not trivial bookkeeping - they guarantee that linear algebra behaves predictably,\nwhich is why it works as the language of science.\n\n\nWhy It Matters\nWith only these two operations-addition and scaling-you can already describe lines, planes, and entire spaces. Any system that grows by combining influences, like physics, economics, or machine learning, is built on these simple rules. Later, when we define matrix multiplication, dot products, and eigenvalues, they all reduce to repeated patterns of adding and scaling vectors.\n\n\nTry It Yourself\n\nAdd (2, 3) and (–1, 4). Draw the result on graph paper.\nMultiply (1, –2) by 3, and then add (0, 5). What is the final vector?\nFor a deeper challenge: Let u = (1, 2) and v = (2, –1). Sketch all vectors of the form a·u + b·v for integer values of a, b between –2 and 2. Notice the grid of points you create-that’s the span of these two vectors.\n\nThis simple practice shows you how combining two basic vectors through addition and scaling generates a whole structured space, the first glimpse of linear algebra’s real power.\n\n\n\n4. Linear Combinations and Span\nAfter learning to add vectors and scale them, the natural next question is: what can we build from these two operations? The answer is the concept of linear combinations, which leads directly to one of the most fundamental ideas in linear algebra: the span of a set of vectors. These ideas tell us not only what individual vectors can do, but how groups of vectors can shape entire spaces.\n\nWhat Is a Linear Combination?\nA linear combination is any vector formed by multiplying vectors with scalars and then adding the results together.\nFormally, given vectors \\(v_1, v_2, \\ldots, v_k\\) and scalars \\(a_1, a_2, \\ldots, a_k\\), a linear combination looks like:\n\\[\na_1 \\cdot v_1 + a_2 \\cdot v_2 + \\cdots + a_k \\cdot v_k.\n\\]\nThis is nothing more than repeated addition and scaling, but the idea is powerful because it describes how vectors combine to generate new ones.\nExample:\nLet \\(u = (1, 0)\\) and \\(v = (0, 1)\\). Then any linear combination \\(a \\cdot u + b \\cdot v = (a, b)\\).\nThis shows that every point in the 2D plane can be expressed as a linear combination of these two simple vectors.\n\n\nGeometric Meaning\nLinear combinations are about mixing directions and magnitudes. Each vector acts like a “directional ingredient,” and the scalars control how much of each ingredient you use.\n\nWith one vector: You can only reach points on a single line through the origin.\nWith two non-parallel vectors in 2D: You can reach every point in the plane.\nWith three non-coplanar vectors in 3D: You can reach all of 3D space.\n\nThis progression shows that the power of linear combinations depends not just on the vectors themselves but on how they relate to each other.\n\n\nThe Span of a Set of Vectors\nThe span of a set of vectors is the collection of all possible linear combinations of them.\nIt answers the question: “What space do these vectors generate?”\nNotation:\n\\[\n\\text{Span}\\{v_1, v_2, \\ldots, v_k\\} =\n\\{a_1 v_1 + a_2 v_2 + \\cdots + a_k v_k \\;|\\; a_i \\in \\mathbb{R}\\}.\n\\]\nExamples:\n\n\\(\\text{Span}\\{(1, 0)\\}\\) = all multiples of \\((1, 0)\\), which is the \\(x\\)-axis.\n\n\\(\\text{Span}\\{(1, 0), (0, 1)\\}\\) = all of \\(\\mathbb{R}^2\\), the entire plane.\n\n\\(\\text{Span}\\{(1, 2), (2, 4)\\}\\) = just the line through \\((1, 2)\\), because the second vector is a multiple of the first.\n\nSo the span depends heavily on whether the vectors add new directions or just duplicate what’s already there.\n\n\nParallel and Independent Vectors\nIf vectors point in the same or opposite directions (one is a scalar multiple of another), then their span is just a line. They don’t add any new coverage of space. But if they point in different directions, they open up new dimensions. This leads to the critical idea of linear independence, which we’ll explore later: vectors are independent if none of them is a linear combination of the others.\n\n\nVisualizing Span in Different Dimensions\n\nIn 2D:\n\nOne vector spans a line.\nTwo independent vectors span the whole plane.\n\nIn 3D:\n\nOne vector spans a line.\nTwo independent vectors span a plane.\nThree independent vectors span all of 3D space.\n\nIn higher dimensions: The same pattern continues. A set of k independent vectors spans a k-dimensional subspace inside the larger space.\n\n\n\nAlgebraic Properties\n\nThe span of vectors always includes the zero vector, because you can choose all scalars = 0.\nThe span is always a subspace, meaning it’s closed under addition and scalar multiplication. If you add two vectors in the span, the result stays in the span.\nThe span grows when you add new independent vectors, but not if the new vector is just a combination of the old ones.\n\n\n\nWhy It Matters\nLinear combinations and span are the foundation for almost everything else in linear algebra:\n\nThey define what it means for vectors to be independent or dependent.\nThey form the basis for solving linear systems (solutions are often described as spans).\nThey explain how dimensions arise in vector spaces.\nThey underpin practical methods like principal component analysis, where data is projected onto the span of a few important vectors.\n\nIn short, the span tells us the “reach” of a set of vectors, and linear combinations are the mechanism to explore that reach.\n\n\nTry It Yourself\n\nTake vectors (1, 0) and (0, 1). Write down three different linear combinations and plot them. What shape do you notice?\nTry vectors (1, 2) and (2, 4). Write down three different linear combinations. Plot them. What’s different from the previous case?\nIn 3D, consider (1, 0, 0) and (0, 1, 0). Describe their span. Add (0, 0, 1). How does the span change?\nChallenge: Pick vectors (1, 2, 3) and (4, 5, 6). Do they span a plane or all of 3D space? How can you tell?\n\nBy experimenting with simple examples, you’ll see clearly how the idea of span captures the richness or limitations of combining vectors.\n\n\n\n5. Length (Norm) and Distance\nSo far, vectors have been arrows with direction and components. To compare them more meaningfully, we need ways to talk about how long they are and how far apart they are. These notions are formalized through the norm of a vector (its length) and the distance between vectors. These concepts tie together the algebra of components and the geometry of space.\n\nThe Length (Norm) of a Vector\nThe norm of a vector measures its magnitude, or how long the arrow is.\nFor a vector \\(v = (v_1, v_2, \\ldots, v_n)\\) in \\(n\\)-dimensional space, its norm is defined as:\n\\[\n\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n\\]\nThis formula comes directly from the Pythagorean theorem: the length of the hypotenuse equals the square root of the sum of squares of the legs.\nIn 2D, this is the familiar distance formula between the origin and a point.\nExamples:\n\nFor \\(v = (3, 4)\\):\n\\[\n\\|v\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5.\n\\]\nFor \\(w = (1, -2, 2)\\):\n\\[\n\\|w\\| = \\sqrt{1^2 + (-2)^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3.\n\\]\n\n\n\nUnit Vectors\nA unit vector is a vector whose length is exactly 1.\nThese are important because they capture direction without scaling.\nTo create a unit vector from any nonzero vector, divide by its norm:\n\\[\nu = \\frac{v}{\\|v\\|}.\n\\]\nExample:\nFor \\(v = (3, 4)\\), the unit vector is\n\\[\nu = \\left(\\tfrac{3}{5}, \\tfrac{4}{5}\\right).\n\\]\nThis points in the same direction as \\((3, 4)\\) but has length 1.\nUnit vectors are like pure directions.\nThey’re especially useful for projections, defining coordinate systems, and normalizing data.\n\n\nDistance Between Vectors\nThe distance between two vectors \\(u\\) and \\(v\\) is defined as the length of their difference:\n\\[\n\\text{dist}(u, v) = \\|u - v\\|.\n\\]\nExample:\nLet \\(u = (2, 1)\\) and \\(v = (5, 5)\\). Then\n\\[\nu - v = (-3, -4).\n\\]\nIts norm is\n\\[\n\\sqrt{(-3)^2 + (-4)^2} = \\sqrt{9 + 16} = 5.\n\\]\nSo the distance is 5. This matches our intuition: the straight-line distance between points \\((2, 1)\\) and \\((5, 5)\\).\n\n\nGeometric Interpretation\n\nThe norm tells you how far a point is from the origin.\nThe distance tells you how far two points are from each other.\n\nBoth are computed with the same formula-the square root of sums of squares-but applied in slightly different contexts.\n\n\nDifferent Kinds of Norms\nThe formula above defines the Euclidean norm (or \\(\\ell_2\\) norm), the most common one.\nBut in linear algebra, other norms are also useful:\n\n\\(\\ell_1\\) norm:\n\\[\n\\|v\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|\n\\]\n(sum of absolute values).\n\\(\\ell_\\infty\\) norm:\n\\[\n\\|v\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\n\\]\n(largest component).\n\nThese norms change the geometry of “length” and “distance.” For example, in the ℓ₁ norm, the unit circle is shaped like a diamond; in the ℓ∞ norm, it looks like a square.\n\n\nAlgebraic Properties\nNorms and distances satisfy critical properties that make them consistent measures:\n\nNon-negativity: \\(\\|v\\| \\geq 0\\), and \\(\\|v\\| = 0\\) only if \\(v = 0\\).\n\nHomogeneity: \\(\\|c \\cdot v\\| = |c| \\, \\|v\\|\\) (scaling affects length predictably).\n\nTriangle inequality: \\(\\|u + v\\| \\leq \\|u\\| + \\|v\\|\\) (the direct path is shortest).\n\nSymmetry (for distance): \\(\\text{dist}(u, v) = \\text{dist}(v, u)\\).\n\nThese properties are why norms and distances are robust tools across mathematics.\n\n\nWhy It Matters\nUnderstanding length and distance is the first step toward geometry in higher dimensions. These notions:\n\nAllow us to compare vectors quantitatively.\nForm the basis of concepts like angles, orthogonality, and projections.\nUnderpin optimization problems (e.g., “find the closest vector” is central to machine learning).\nDefine the geometry of spaces, which changes dramatically depending on which norm you use.\n\n\n\nTry It Yourself\n\nCompute the norm of (6, 8). Then divide by the norm to find its unit vector.\nFind the distance between (1, 1, 1) and (4, 5, 6).\nCompare the Euclidean and Manhattan (ℓ₁) distances between (0, 0) and (3, 4). Which one matches your intuition if you were walking along a city grid?\nChallenge: For vectors u = (2, –1, 3) and v = (–2, 0, 1), compute ‖u – v‖. Then explain what this distance means geometrically.\n\nBy working through these examples, you’ll see how norms and distances make abstract vectors feel as real as points and arrows you can measure in everyday life.\n\n\n\n6. Dot Product (Algebraic and Geometric Views)\nThe dot product is one of the most fundamental operations in linear algebra. It looks like a simple formula, but it unlocks the ability to measure angles, detect orthogonality, project one vector onto another, and compute energy or work in physics. Understanding it requires seeing both the algebraic view (a formula on components) and the geometric view (a way to compare directions).\n\nAlgebraic Definition\nFor two vectors of the same dimension, \\(u = (u_1, u_2, \\ldots, u_n)\\) and \\(v = (v_1, v_2, \\ldots, v_n)\\), the dot product is defined as:\n\\[\nu \\cdot v = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\n\\]\nThis is simply multiplying corresponding components and summing the results.\nExamples:\n\n\\((2, 3) \\cdot (4, 5) = (2 \\times 4) + (3 \\times 5) = 8 + 15 = 23\\)\n\n\\((1, -2, 3) \\cdot (0, 4, -1) = (1 \\times 0) + (-2 \\times 4) + (3 \\times -1) = 0 - 8 - 3 = -11\\)\n\nNotice that the dot product is always a scalar, not a vector.\n\n\nGeometric Definition\nThe dot product can also be defined in terms of vector length and angle:\n\\[\nu \\cdot v = \\|u\\| \\, \\|v\\| \\cos(\\theta),\n\\]\nwhere \\(\\theta\\) is the angle between \\(u\\) and \\(v\\) (\\(0^\\circ \\leq \\theta \\leq 180^\\circ\\)).\nThis formula tells us:\n\nIf the angle is acute (less than \\(90^\\circ\\)), \\(\\cos(\\theta) &gt; 0\\), so the dot product is positive.\n\nIf the angle is right (exactly \\(90^\\circ\\)), \\(\\cos(\\theta) = 0\\), so the dot product is 0.\n\nIf the angle is obtuse (greater than \\(90^\\circ\\)), \\(\\cos(\\theta) &lt; 0\\), so the dot product is negative.\n\nThus, the sign of the dot product encodes directional alignment.\n\n\nConnecting the Two Definitions\nAt first glance, the algebraic sum of products and the geometric length–angle formula seem unrelated. But they are equivalent. To see why, consider the law of cosines applied to a triangle formed by u, v, and u – v. Expanding both sides leads directly to the equivalence between the two formulas. This dual interpretation is what makes the dot product so powerful: it is both a computation rule and a geometric measurement.\n\n\nOrthogonality\nTwo vectors are orthogonal (perpendicular) if and only if their dot product is zero:\n\\[\nu \\cdot v = 0 \\;\\;\\Longleftrightarrow\\;\\; \\theta = 90^\\circ.\n\\]\nThis gives us an algebraic way to check for perpendicularity without drawing diagrams.\nExample:\n\\((2, 1) \\cdot (-1, 2) = (2 \\times -1) + (1 \\times 2) = -2 + 2 = 0\\),\nso the vectors are orthogonal.\n\n\nProjections\nThe dot product also provides a way to project one vector onto another.\nThe scalar projection of \\(u\\) onto \\(v\\) is:\n\\[\n\\text{proj}_{\\text{scalar}}(u \\text{ onto } v) = \\frac{u \\cdot v}{\\|v\\|}.\n\\]\nThe vector projection is then:\n\\[\n\\text{proj}_{\\text{vector}}(u \\text{ onto } v) = \\frac{u \\cdot v}{\\|v\\|^2} \\, v.\n\\]\nThis allows us to decompose vectors into “parallel” and “perpendicular” components, which is central in geometry, physics, and data analysis.\n\n\nExamples\n\nCompute \\(u = (3, 4)\\) and \\(v = (4, 3)\\).\n\nDot product: \\((3 \\times 4) + (4 \\times 3) = 12 + 12 = 24\\).\n\nNorms: \\(\\|u\\| = 5\\), \\(\\|v\\| = 5\\).\n\n\\(\\cos(\\theta) = \\tfrac{24}{5 \\times 5} = \\tfrac{24}{25} \\approx 0.96\\), so \\(\\theta \\approx 16^\\circ\\).\nThese vectors are nearly parallel.\n\nCompute \\(u = (1, 2, -1)\\) and \\(v = (2, -1, 1)\\).\n\nDot product: \\((1 \\times 2) + (2 \\times -1) + (-1 \\times 1) = 2 - 2 - 1 = -1\\).\n\nNorms: \\(\\|u\\| = \\sqrt{6}\\), \\(\\|v\\| = \\sqrt{6}\\).\n\n\\(\\cos(\\theta) = \\tfrac{-1}{\\sqrt{6} \\times \\sqrt{6}} = -\\tfrac{1}{6}\\), so \\(\\theta \\approx 99.6^\\circ\\).\nSlightly obtuse.\n\n\n\n\nPhysical Interpretation\nIn physics, the dot product computes work:\n\\[\n\\text{Work} = \\text{Force} \\cdot \\text{Displacement}\n           = \\|\\text{Force}\\| \\, \\|\\text{Displacement}\\| \\cos(\\theta).\n\\]\nOnly the component of the force in the direction of motion contributes. If you push straight down on a box while trying to move it horizontally, the dot product is zero: no work is done in the direction of motion.\n\n\nAlgebraic Properties\n\nCommutative: \\(u \\cdot v = v \\cdot u\\)\n\nDistributive: \\(u \\cdot (v + w) = u \\cdot v + u \\cdot w\\)\n\nScalar compatibility: \\((c \\cdot u) \\cdot v = c \\,(u \\cdot v)\\)\n\nNon-negativity: \\(v \\cdot v = \\|v\\|^2 \\geq 0\\)\n\nThese guarantee that the dot product behaves consistently and meshes with the structure of vector spaces.\n\n\nWhy It Matters\nThe dot product is the first bridge between algebra and geometry. It:\n\nDefines angles and orthogonality in higher dimensions.\nPowers projections and decompositions, which underlie least squares, regression, and data fitting.\nAppears in physics as energy, power, and work.\nServes as the kernel of many machine learning methods (e.g., similarity measures in high-dimensional spaces).\n\nWithout the dot product, linear algebra would lack a way to connect numbers with geometry and meaning.\n\n\nTry It Yourself\n\nCompute (2, –1) · (–3, 4). Then find the angle between them.\nCheck if (1, 2, 3) and (2, 4, 6) are orthogonal. What does the dot product tell you?\nFind the projection of (3, 1) onto (1, 2). Draw the original vector, the projection, and the perpendicular component.\nIn physics terms: Suppose a 10 N force is applied at 60° to the direction of motion, and the displacement is 5 m. How much work is done?\n\nThese exercises reveal the dual power of the dot product: as a formula to compute and as a geometric tool to interpret.\n\n\n\n7. Angles Between Vectors and Cosine\nHaving defined the dot product, we are now ready to measure angles between vectors. In everyday life, angles tell us how two lines or directions relate-whether they point the same way, are perpendicular, or are opposed. In linear algebra, the dot product and cosine function give us a precise, generalizable way to define angles in any dimension, not just in 2D or 3D. This section explores how we compute, interpret, and apply vector angles.\n\nThe Definition of an Angle Between Vectors\nFor two nonzero vectors \\(u\\) and \\(v\\), the angle \\(\\theta\\) between them is defined by:\n\\[\n\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\, \\|v\\|}.\n\\]\nThis formula comes directly from the geometric definition of the dot product.\nRearranging gives:\n\\[\n\\theta = \\arccos\\!\\left(\\frac{u \\cdot v}{\\|u\\| \\, \\|v\\|}\\right).\n\\]\nKey points:\n\n\\(\\theta\\) is always between \\(0^\\circ\\) and \\(180^\\circ\\) (or \\(0\\) and \\(\\pi\\) radians).\n\nThe denominator normalizes the dot product by dividing by the product of lengths, so the result is dimensionless and always between \\(-1\\) and \\(1\\).\n\nThe cosine value directly encodes alignment: positive, zero, or negative.\n\n\n\nInterpretation of Cosine Values\nThe cosine tells us about the directional relationship:\n\n\\(\\cos(\\theta) = 1 \\;\\;\\Rightarrow\\;\\; \\theta = 0^\\circ\\) → vectors point in exactly the same direction.\n\n\\(\\cos(\\theta) = 0 \\;\\;\\Rightarrow\\;\\; \\theta = 90^\\circ\\) → vectors are orthogonal (perpendicular).\n\n\\(\\cos(\\theta) = -1 \\;\\;\\Rightarrow\\;\\; \\theta = 180^\\circ\\) → vectors point in exactly opposite directions.\n\n\\(\\cos(\\theta) &gt; 0\\) → acute angle → vectors point more “together” than apart.\n\n\\(\\cos(\\theta) &lt; 0\\) → obtuse angle → vectors point more “against” each other.\n\nThus, the cosine compresses geometric alignment into a single number.\n\n\nExamples\n\n\\(u = (1, 0), \\; v = (0, 1)\\)\n\nDot product: \\(1 \\times 0 + 0 \\times 1 = 0\\)\n\nNorms: \\(1\\) and \\(1\\)\n\n\\(\\cos(\\theta) = 0 \\;\\Rightarrow\\; \\theta = 90^\\circ\\)\nThe vectors are perpendicular, as expected.\n\n\\(u = (2, 3), \\; v = (4, 6)\\)\n\nDot product: \\((2 \\times 4) + (3 \\times 6) = 8 + 18 = 26\\)\n\nNorms: \\(\\sqrt{2^2 + 3^2} = \\sqrt{13}\\), and \\(\\sqrt{4^2 + 6^2} = \\sqrt{52} = 2\\sqrt{13}\\)\n\n\\(\\cos(\\theta) = \\tfrac{26}{\\sqrt{13} \\cdot 2\\sqrt{13}} = \\tfrac{26}{26} = 1\\)\n\n\\(\\theta = 0^\\circ\\)\nThese vectors are multiples, so they align perfectly.\n\n\\(u = (1, 1), \\; v = (-1, 1)\\)\n\nDot product: \\((1 \\times -1) + (1 \\times 1) = -1 + 1 = 0\\)\n\n\\(\\cos(\\theta) = 0 \\;\\Rightarrow\\; \\theta = 90^\\circ\\)\nThe vectors are perpendicular, forming diagonals of a square.\n\n\n\n\nAngles in Higher Dimensions\nThe beauty of the formula is that it works in any dimension.\nEven in \\(\\mathbb{R}^{100}\\) or higher, we can define the angle between two vectors using only their dot product and norms.\nWhile we cannot visualize the geometry directly in high dimensions, the cosine formula still captures how aligned two directions are:\n\\[\n\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\, \\|v\\|}.\n\\]\nThis is critical in machine learning, where data often lives in very high-dimensional spaces.\n\n\nCosine Similarity\nThe cosine of the angle between two vectors is often called cosine similarity. It is widely used in data analysis and machine learning to measure how similar two data vectors are, independent of their magnitude.\n\nIn text mining, documents are turned into word-frequency vectors. Cosine similarity measures how “close in topic” two documents are, regardless of length.\nIn recommendation systems, cosine similarity compares user preference vectors to suggest similar users or items.\n\nThis demonstrates how a geometric concept extends far beyond pure math.\n\n\nOrthogonality Revisited\nThe angle formula reinforces the special role of orthogonality.\nIf \\(\\cos(\\theta) = 0\\), then \\(u \\cdot v = 0\\).\nThis means the dot product not only computes length but also serves as a direct test for perpendicularity.\nThis algebraic shortcut is far easier than manually checking geometric right angles.\n\n\nAngles and Projections\nAngles are closely tied to projections.\nThe length of the projection of \\(u\\) onto \\(v\\) is \\(\\|u\\|\\cos(\\theta)\\).\nIf the angle is small, the projection is large — most of \\(u\\) lies in the direction of \\(v\\).\nIf the angle is close to \\(90^\\circ\\), the projection shrinks toward zero.\nThus, the cosine acts as a scaling factor between directions.\n\n\nWhy It Matters\nAngles between vectors provide:\n\nA way to generalize geometry beyond 2D/3D.\nA measure of similarity in high-dimensional data.\nThe foundation for orthogonality, projections, and decomposition of spaces.\nA tool for optimization: in gradient descent, for example, the angle between the gradient and step direction determines how effectively we reduce error.\n\nWithout the ability to measure angles, we could not connect algebraic manipulations with geometric intuition or practical applications.\n\n\nTry It Yourself\n\nCompute the angle between (2, 1) and (1, –1). Interpret the result.\nFind two vectors in 3D that form a 60° angle. Verify using the cosine formula.\nConsider word vectors for “cat” and “dog” in a machine learning model. Why might cosine similarity be a better measure of similarity than Euclidean distance?\nChallenge: In \\(\\mathbb{R}^3\\), find a vector orthogonal to both (1, 2, 3) and (3, 2, 1). What angle does it make with each of them?\n\nBy experimenting with these problems, you will see how angles provide the missing link between algebraic formulas and geometric meaning in linear algebra.\n\n\n\n8. Projections and Decompositions\nIn earlier sections, we saw how the dot product measures alignment and how the cosine formula gives us angles between vectors. The next natural step is to use these tools to project one vector onto another. Projection is a way to “shadow” one vector onto the direction of another, splitting vectors into meaningful parts: one along a given direction and one perpendicular to it. This is the essence of decomposition, and it is everywhere in linear algebra, geometry, physics, and data science.\n\nScalar Projection\nThe scalar projection of a vector \\(u\\) onto a vector \\(v\\) measures how much of \\(u\\) lies in the direction of \\(v\\). It is given by:\n\\[\n\\text{proj}_{\\text{scalar}}(u \\text{ onto } v) = \\frac{u \\cdot v}{\\|v\\|}.\n\\]\n\nIf this value is positive, \\(u\\) has a component pointing in the same direction as \\(v\\).\n\nIf it is negative, \\(u\\) points partly in the opposite direction.\n\nIf it is zero, \\(u\\) is completely perpendicular to \\(v\\).\n\nExample:\n\\(u = (3, 4)\\), \\(v = (1, 0)\\).\nDot product: \\((3 \\times 1 + 4 \\times 0) = 3\\).\n\\(\\|v\\| = 1\\).\nSo the scalar projection is \\(3\\). This tells us \\(u\\) has a “shadow” of length \\(3\\) on the \\(x\\)-axis.\n\n\nVector Projection\nThe vector projection gives the actual arrow in the direction of \\(v\\) that corresponds to this scalar amount:\n\\[\n\\text{proj}_{\\text{vector}}(u \\text{ onto } v) = \\frac{u \\cdot v}{\\|v\\|^2} \\, v.\n\\]\nThis formula normalizes \\(v\\) into a unit vector, then scales it by the scalar projection.\nThe result is a new vector lying along \\(v\\), capturing exactly the “parallel” part of \\(u\\).\nExample:\n\\(u = (3, 4)\\), \\(v = (1, 2)\\)\n\nDot product: \\(3 \\times 1 + 4 \\times 2 = 3 + 8 = 11\\)\n\nNorm squared of \\(v\\): \\((1^2 + 2^2) = 5\\)\n\nCoefficient: \\(11 / 5 = 2.2\\)\n\nProjection vector: \\(2.2 \\cdot (1, 2) = (2.2, 4.4)\\)\n\nSo the part of \\((3, 4)\\) in the direction of \\((1, 2)\\) is \\((2.2, 4.4)\\).\n\n\nPerpendicular Component\nOnce we have the projection, we can find the perpendicular component (often called the rejection) simply by subtracting:\n\\[\nu_{\\perp} = u - \\text{proj}_{\\text{vector}}(u \\text{ onto } v).\n\\]\nThis gives the part of \\(u\\) that is entirely orthogonal to \\(v\\).\nExample continued:\n\\(u_{\\perp} = (3, 4) - (2.2, 4.4) = (0.8, -0.4)\\)\nCheck:\n\\((0.8, -0.4) \\cdot (1, 2) = 0.8 \\times 1 + (-0.4) \\times 2 = 0.8 - 0.8 = 0\\).\nIndeed, orthogonal.\n\n\nGeometric Picture\nProjection is like dropping a perpendicular from one vector onto another. Imagine shining a light perpendicular to v: the shadow of u on the line spanned by v is the projection. This visualization explains why projections split vectors naturally into two pieces:\n\nParallel part: Along the line of v.\nPerpendicular part: Orthogonal to v, forming a right angle.\n\nTogether, these two parts reconstruct the original vector exactly.\n\n\nDecomposition of Vectors\nEvery vector \\(u\\) can be decomposed relative to another vector \\(v\\) into two parts:\n\\[\nu = \\text{proj}_{\\text{vector}}(u \\text{ onto } v) + \\big(u - \\text{proj}_{\\text{vector}}(u \\text{ onto } v)\\big).\n\\]\nThis decomposition is unique and geometrically meaningful.\nIt generalizes to subspaces: we can project onto entire planes or higher-dimensional spans, splitting a vector into a “within-subspace” part and a “perpendicular-to-subspace” part.\n\n\nApplications\n\nPhysics (Work and Forces): Work is the projection of force onto displacement. Only the part of the force in the direction of motion contributes. Example: Pushing on a sled partly sideways wastes effort-the sideways component projects to zero.\nGeometry and Engineering: Projections are used in CAD (computer-aided design) to flatten 3D objects onto 2D surfaces, like blueprints or shadows.\nComputer Graphics: Rendering 3D scenes onto a 2D screen is fundamentally a projection process.\nData Science: Projecting high-dimensional data onto a lower-dimensional subspace (like the first two principal components in PCA) makes patterns visible while preserving as much information as possible.\nSignal Processing: Decomposition into projections onto sine and cosine waves forms the basis of Fourier analysis, which powers audio, image, and video compression.\n\n\n\nAlgebraic Properties\n\nProjections are linear: proj(u + w) = proj(u) + proj(w).\nThe perpendicular part is always orthogonal to the direction of projection.\nThe decomposition is unique: no other pair of parallel and perpendicular vectors will reconstruct u.\nThe projection operator onto a unit vector v̂ satisfies: proj(u) = (v̂ v̂ᵀ)u, showing how projection can be expressed in matrix form.\n\n\n\nWhy It Matters\nProjection is not just a geometric trick; it is the core of many advanced topics:\n\nLeast squares regression is finding the projection of a data vector onto the span of predictor vectors.\nOrthogonal decompositions like Gram–Schmidt and QR factorization rely on projections to build orthogonal bases.\nOptimization methods often involve projecting guesses back onto feasible sets.\nMachine learning uses projections constantly to reduce dimensions, compare vectors, and align features.\n\nWithout projection, we could not cleanly separate influence along directions or reduce complexity in structured ways.\n\n\nTry It Yourself\n\nProject (2, 3) onto (1, 0). What does the perpendicular component look like?\nProject (3, 1) onto (2, 2). Verify the perpendicular part is orthogonal.\nDecompose (5, 5, 0) into parallel and perpendicular parts relative to (1, 0, 0).\nChallenge: Write the projection matrix for projecting onto (1, 2). Apply it to (3, 4). Does it match the formula?\n\nThrough these exercises, you will see that projection is more than an operation-it is a lens through which we decompose, interpret, and simplify vectors and spaces.\n\n\n\n9. Cauchy–Schwarz and Triangle Inequalities\nLinear algebra is not only about operations with vectors-it also involves understanding the fundamental relationships between them. Two of the most important results in this regard are the Cauchy–Schwarz inequality and the triangle inequality. These are cornerstones of vector spaces because they establish precise boundaries for lengths, angles, and inner products. Without them, the geometry of linear algebra would fall apart.\n\nThe Cauchy–Schwarz Inequality\nFor any two vectors \\(u\\) and \\(v\\) in \\(\\mathbb{R}^n\\), the Cauchy–Schwarz inequality states:\n\\[\n|u \\cdot v| \\leq \\|u\\| \\, \\|v\\|.\n\\]\nThis means that the absolute value of the dot product of two vectors is always less than or equal to the product of their lengths.\nEquality holds if and only if u and v are linearly dependent (i.e., one is a scalar multiple of the other).\n\nWhy It Is True\nRecall the geometric formula for the dot product:\n\\[\nu \\cdot v = \\|u\\| \\, \\|v\\| \\cos(\\theta).\n\\]\nSince \\(-1 \\leq \\cos(\\theta) \\leq 1\\), the magnitude of the dot product cannot exceed \\(\\|u\\| \\, \\|v\\|\\).\nThis is exactly the inequality.\n\n\nExample\nLet \\(u = (3, 4)\\) and \\(v = (-4, 3)\\).\n\nDot product: \\((3 \\times -4) + (4 \\times 3) = -12 + 12 = 0\\)\n\nNorms: \\(\\|u\\| = 5\\), \\(\\|v\\| = 5\\)\n\nProduct of norms: \\(25\\)\n\n\\(|u \\cdot v| = 0 \\leq 25\\), which satisfies the inequality\n\nEquality does not hold since they are not multiples - they are perpendicular.\n\n\nIntuition\nThe inequality tells us that two vectors can never “overlap” more strongly than the product of their magnitudes. If they align perfectly, the overlap is maximum (equality). If they’re perpendicular, the overlap is zero.\nThink of it as: “the shadow of one vector on another can never be longer than the vector itself.”\n\n\n\nThe Triangle Inequality\nFor any vectors \\(u\\) and \\(v\\), the triangle inequality states:\n\\[\n\\|u + v\\| \\leq \\|u\\| + \\|v\\|.\n\\]\nThis mirrors the geometric fact that in a triangle, any side is at most as long as the sum of the other two sides.\n\nExample\nLet \\(u = (1, 2)\\) and \\(v = (3, 4)\\).\n\n\\(\\|u + v\\| = \\|(4, 6)\\| = \\sqrt{16 + 36} = \\sqrt{52} \\approx 7.21\\)\n\n\\(\\|u\\| + \\|v\\| = \\sqrt{5} + 5 \\approx 2.24 + 5 = 7.24\\)\n\nIndeed, \\(7.21 \\leq 7.24\\), very close in this case.\n\n\nEquality Case\nThe triangle inequality becomes equality when the vectors point in exactly the same direction (or are scalar multiples with nonnegative coefficients). For example, (1, 1) and (2, 2) produce equality because adding them gives a vector whose length equals the sum of their lengths.\n\n\n\nExtensions\n\nThese inequalities hold in all inner product spaces, not just ℝⁿ. This means they apply to functions, sequences, and more abstract mathematical objects.\nIn Hilbert spaces (infinite-dimensional generalizations), they remain just as essential.\n\n\n\nWhy They Matter\n\nThey guarantee that the dot product and norm are well-behaved and geometrically meaningful.\nThey ensure that the norm satisfies the requirements of a distance measure: nonnegativity, symmetry, and triangle inequality.\nThey underpin the validity of projections, orthogonality, and least squares methods.\nThey are essential in proving convergence of algorithms, error bounds, and stability in numerical linear algebra.\n\nWithout these inequalities, we could not trust that the geometry of vector spaces behaves consistently.\n\n\nTry It Yourself\n\nVerify Cauchy–Schwarz for (2, –1, 3) and (–1, 4, 0). Compute both sides.\nTry the triangle inequality for (–3, 4) and (5, –12). Does equality hold?\nFind two vectors where Cauchy–Schwarz is an equality. Explain why.\nChallenge: Prove the triangle inequality in \\(\\mathbb{R}^2\\) using only the Pythagorean theorem and algebra, without relying on dot products.\n\nWorking through these problems will show you why these inequalities are not abstract curiosities but the structural glue of linear algebra’s geometry.\n\n\n\n10. Orthonormal sets in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\)\nUp to now, we’ve discussed vectors, their lengths, angles, and how to project one onto another. A natural culmination of these ideas is the concept of orthonormal sets. These are collections of vectors that are not only orthogonal (mutually perpendicular) but also normalized (each of length 1). Orthonormal sets form the cleanest, most efficient coordinate systems in linear algebra. They are the mathematical equivalent of having rulers at right angles, perfectly calibrated to unit length.\n\nOrthogonal and Normalized\nLet’s break the term “orthonormal” into two parts:\n\nOrthogonal: Two vectors \\(u\\) and \\(v\\) are orthogonal if \\(u \\cdot v = 0\\).\nIn \\(\\mathbb{R}^2\\), this means the vectors meet at a right angle.\nIn \\(\\mathbb{R}^3\\), it means they form perpendicular directions.\nNormalized: A vector \\(v\\) is normalized if its length is \\(1\\), i.e., \\(\\|v\\| = 1\\).\nSuch vectors are called unit vectors.\n\nWhen we combine both conditions, we get orthonormal vectors: vectors that are both perpendicular to each other and have unit length.\n\n\nOrthonormal Sets in \\(\\mathbb{R}^2\\)\nIn two dimensions, an orthonormal set typically consists of two vectors.\nA classic example is:\n\\(e_1 = (1, 0), \\quad e_2 = (0, 1)\\)\n\nDot product: \\(e_1 \\cdot e_2 = (1 \\times 0 + 0 \\times 1) = 0 \\;\\;\\Rightarrow\\;\\;\\) orthogonal\n\nLengths: \\(\\|e_1\\| = 1\\), \\(\\|e_2\\| = 1 \\;\\;\\Rightarrow\\;\\;\\) normalized\n\nThus, \\(\\{e_1, e_2\\}\\) is an orthonormal set.\nIn fact, this is the standard basis for \\(\\mathbb{R}^2\\).\nAny vector \\((x, y)\\) can be written as \\(x e_1 + y e_2\\).\nThis is the simplest coordinate system.\n\n\nOrthonormal Sets in \\(\\mathbb{R}^3\\)\nIn three dimensions, an orthonormal set usually has three vectors.\nThe standard basis is:\n\\(e_1 = (1, 0, 0), \\quad e_2 = (0, 1, 0), \\quad e_3 = (0, 0, 1)\\)\n\nEach pair has dot product zero, so they are orthogonal\n\nEach has length \\(1\\), so they are normalized\n\nTogether, they span all of \\(\\mathbb{R}^3\\)\n\nGeometrically, they correspond to the \\(x\\)-, \\(y\\)-, and \\(z\\)-axes in 3D space.\nAny vector \\((x, y, z)\\) can be written as a linear combination \\(x e_1 + y e_2 + z e_3\\).\n\n\nBeyond the Standard Basis\nThe standard basis is not the only orthonormal set. For example:\n\\(u = \\left(\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right), \\quad\nv = \\left(-\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right)\\)\n\nDot product: \\((\\tfrac{1}{\\sqrt{2}})(-\\tfrac{1}{\\sqrt{2}}) + (\\tfrac{1}{\\sqrt{2}})(\\tfrac{1}{\\sqrt{2}}) = -\\tfrac{1}{2} + \\tfrac{1}{2} = 0\\)\n\nLengths: \\(\\sqrt{(\\tfrac{1}{\\sqrt{2}})^2 + (\\tfrac{1}{\\sqrt{2}})^2} = \\sqrt{\\tfrac{1}{2} + \\tfrac{1}{2}} = 1\\)\n\nSo \\(\\{u, v\\}\\) is also orthonormal in \\(\\mathbb{R}^2\\).\nThese vectors are rotated \\(45^\\circ\\) relative to the standard axes.\nSimilarly, in \\(\\mathbb{R}^3\\), you can construct rotated orthonormal sets (such as unit vectors along diagonals), as long as the conditions of perpendicularity and unit length hold.\n\n\nProperties of Orthonormal Sets\n\nSimplified coordinates: If \\(\\{v_1, \\ldots, v_k\\}\\) is an orthonormal set, then for any vector \\(u\\) in their span, the coefficients are easy to compute:\n\\[\nc_i = u \\cdot v_i\n\\]\nThis is much simpler than solving systems of equations.\nPythagorean theorem generalized: If vectors are orthonormal, the squared length of their sum is the sum of the squares of their coefficients.\nFor example, if \\(u = a v_1 + b v_2\\), then\n\\[\n\\|u\\|^2 = a^2 + b^2\n\\]\nProjection is easy: Projecting onto an orthonormal set is straightforward — just take dot products.\nMatrices become nice: When vectors form the columns of a matrix, orthonormality makes that matrix an orthogonal matrix, which has special properties: its transpose equals its inverse, and it preserves lengths and angles.\n\n\n\nImportance in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\)\n\nIn geometry, orthonormal bases correspond to coordinate axes.\nIn physics, they represent independent directions of motion or force.\nIn computer graphics, orthonormal sets define camera axes and object rotations.\nIn engineering, they simplify stress, strain, and rotation analysis.\n\nEven though \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) are relatively simple, the same ideas extend naturally to higher dimensions, where visualization is impossible but the algebra is identical.\n\n\nWhy Orthonormal Sets Matter\nOrthonormality is the gold standard for building bases in linear algebra:\n\nIt makes calculations fast and simple.\nIt ensures numerical stability in computations (important in algorithms and simulations).\nIt underpins key decompositions like QR factorization, singular value decomposition (SVD), and spectral theorems.\nIt provides the cleanest way to think about space: orthogonal, independent directions scaled to unit length.\n\nWhenever possible, mathematicians and engineers prefer orthonormal bases over arbitrary ones.\n\n\nTry It Yourself\n\nVerify that (3/5, 4/5) and (–4/5, 3/5) form an orthonormal set in \\(\\mathbb{R}^2\\).\nConstruct three orthonormal vectors in \\(\\mathbb{R}^3\\) that are not the standard basis. Hint: start with (1/√2, 1/√2, 0) and build perpendiculars.\nFor u = (2, 1), compute its coordinates relative to the orthonormal set {(1/√2, 1/√2), (–1/√2, 1/√2)}.\nChallenge: Prove that if {v₁, …, vₖ} is orthonormal, then the matrix with these as columns is orthogonal, i.e., QᵀQ = I.\n\nThrough these exercises, you will see how orthonormal sets make every aspect of linear algebra-from projections to decompositions-simpler, cleaner, and more powerful.\n\n\nClosing\nLengths, angles revealed,\nprojections trace hidden lines,\nclarity takes shape.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-2.-matrices-and-basic-operations",
    "href": "books/en-US/book.html#chapter-2.-matrices-and-basic-operations",
    "title": "The Book",
    "section": "Chapter 2. Matrices and basic operations",
    "text": "Chapter 2. Matrices and basic operations\n\nOpening\nRows and columns meet,\nwoven grids of silent rules,\nmachines of order.\n\n\n11. Matrices as Tables and as Machines\nThe next stage in our journey is to move from vectors to matrices. A matrix may look like just a rectangular array of numbers, but in linear algebra it plays two distinct and equally important roles:\n\nAs a table of numbers, storing data, coefficients, or geometric patterns in a compact form.\nAs a machine that transforms vectors into other vectors, capturing the essence of linear transformations.\n\nBoth views are valid, and learning to switch between them is crucial to building intuition.\n\nMatrices as Tables\nAt the most basic level, a matrix is a grid of numbers arranged into rows and columns.\n\nA \\(2 \\times 2\\) matrix has 2 rows and 2 columns:\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\]\nA \\(3 \\times 2\\) matrix has 3 rows and 2 columns:\n\\[\nB = \\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22} \\\\\nb_{31} & b_{32}\n\\end{bmatrix}\n\\]\n\nEach entry \\(a_{ij}\\) or \\(b_{ij}\\) tells us the number in the i-th row and j-th column. The rows of a matrix can represent constraints, equations, or observations; the columns can represent features, variables, or directions.\nIn this sense, matrices are data containers, organizing information efficiently. That’s why matrices show up in spreadsheets, statistics, computer graphics, and scientific computing.\n\n\nMatrices as Machines\nThe deeper view of a matrix is as a function from vectors to vectors. If x is a column vector, then multiplying A·x produces a new vector.\nFor example:\n\\[\nA = \\begin{bmatrix}\n2 & 0 \\\\\n1 & 3\n\\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix}\n4 \\\\\n5\n\\end{bmatrix}.\n\\]\nMultiplying:\n\\[\nA\\mathbf{x} = \\begin{bmatrix}\n2×4 + 0×5 \\\\\n1×4 + 3×5\n\\end{bmatrix}\n= \\begin{bmatrix}\n8 \\\\\n19\n\\end{bmatrix}.\n\\]\nHere, the matrix is acting as a machine that takes input (4, 5) and outputs (8, 19). The “machine rules” are encoded in the rows of A.\n\n\nColumn View of Matrix Multiplication\nAnother way to see it: multiplying A·x is the same as taking a linear combination of A’s columns.\nIf\n\\[\nA = \\begin{bmatrix}\na_1 & a_2\n\\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix},\n\\]\nthen:\n\\[\nA\\mathbf{x} = x_1 a_1 + x_2 a_2.\n\\]\nSo the vector x tells the machine “how much” of each column to mix together. This column view is critical-it connects matrices to span, dimension, and basis ideas we saw earlier.\n\n\nThe Duality of Tables and Machines\n\nAs a table, a matrix is a static object: numbers written in rows and columns.\nAs a machine, the same numbers become instructions for transforming vectors.\n\nThis duality is not just conceptual-it’s the key to understanding why linear algebra is so powerful. A dataset, once stored as a table, can be interpreted as a transformation. Likewise, a transformation, once understood, can be encoded as a table.\n\n\nExamples in Practice\n\nPhysics: A stress–strain matrix is a table of coefficients. But it also acts as a machine that transforms applied forces into deformations.\nComputer Graphics: A 2D rotation matrix is a machine that spins vectors, but it can be stored in a simple 2×2 table.\nEconomics: Input–output models use matrices as tables of production coefficients. Applying them to demand vectors transforms them into resource requirements.\n\n\n\nGeometric Intuition\nEvery 2×2 or 3×3 matrix corresponds to some linear transformation in the plane or space. Examples:\n\nScaling: \\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\\) doubles lengths.\nReflection: \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\) flips across the x-axis.\nRotation: \\(\\begin{bmatrix} \\cos θ & -\\sin θ \\\\ \\sin θ & \\cos θ \\end{bmatrix}\\) rotates vectors by θ.\n\nThese are not just tables of numbers-they are precise, reusable machines.\n\n\nWhy This Matters\nThis section sets the stage for all matrix theory:\n\nThinking of matrices as tables helps in data interpretation and organization.\nThinking of matrices as machines helps in understanding linear transformations, eigenvalues, and decompositions.\nMost importantly, learning to switch between the two perspectives makes linear algebra both concrete and abstract-bridging computation with geometry.\n\n\n\nTry It Yourself\n\nWrite a 2×3 matrix and identify its rows and columns. What might they represent in a real-world dataset?\nMultiply \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) by \\(\\begin{bmatrix} 2 \\\\ –1 \\end{bmatrix}\\). Interpret the result using both the row and column views.\nConstruct a matrix that scales vectors by 2 along the x-axis and reflects them across the y-axis. Test it on (1, 1).\nChallenge: Show how the same 3×3 rotation matrix can be viewed as a data table of cosines/sines and as a machine that turns input vectors.\n\nBy mastering both perspectives, you’ll see matrices not just as numbers but as dynamic objects that encode and execute transformations.\n\n\n\n12. Matrix Shapes, Indexing, and Block Views\nMatrices come in many shapes and sizes, and the way we label their entries matters. This section is about learning how to read and write matrices carefully, how to work with rows and columns, and how to use block structure to simplify problems. These seemingly simple ideas are what allow us to manipulate large systems with precision and efficiency.\n\nShapes of Matrices\nThe shape of a matrix is given by its number of rows and columns:\n\nA m×n matrix has m rows and n columns.\nRows run horizontally, columns run vertically.\nSquare matrices have m = n; rectangular matrices have m ≠ n.\n\nExamples:\n\nA 2×3 matrix:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]\nA 3×2 matrix:\n\\[\n\\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\n\\]\n\nShape matters because it determines whether certain operations (like multiplication) are possible.\n\n\nIndexing: The Language of Entries\nEach entry in a matrix has two indices: one for its row, one for its column.\n\n\\(a_{ij}\\) = entry in row i, column j.\nThe first index always refers to the row, the second to the column.\n\nFor example, in\n\\[\nA = \\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9\n\\end{bmatrix},\n\\]\nwe have:\n\n\\(a_{11} = 1\\), \\(a_{23} = 8\\), \\(a_{32} = 6\\).\n\nIndexing is the grammar of matrix language. Without it, we can’t specify positions or write formulas clearly.\n\n\nRows and Columns as Vectors\nEvery row and every column of a matrix is itself a vector.\n\nThe i-th row is written as \\(A_{i,*}\\).\nThe j-th column is written as \\(A_{*,j}\\).\n\nExample: From the matrix above,\n\nFirst row: (1, 4, 7).\nSecond column: (4, 5, 6).\n\nThis duality is powerful: rows often represent constraints or equations, while columns represent directions or features. Later, when we interpret matrix–vector products, we’ll see that multiplying A·x means combining columns, while multiplying yᵀ·A means combining rows.\n\n\nSubmatrices\nSometimes we want just part of a matrix. A submatrix is formed by selecting certain rows and columns.\nExample: From\n\\[\nB = \\begin{bmatrix}\n2 & 4 & 6 \\\\\n1 & 3 & 5 \\\\\n7 & 8 & 9\n\\end{bmatrix},\n\\]\nthe submatrix of the first two rows and last two columns is:\n\\[\n\\begin{bmatrix}\n4 & 6 \\\\\n3 & 5\n\\end{bmatrix}.\n\\]\nSubmatrices allow us to zoom in and isolate parts of a problem.\n\n\nBlock Matrices: Dividing to Conquer\nLarge matrices can often be broken into blocks, which are smaller submatrices arranged inside. This is like dividing a spreadsheet into quadrants.\nFor example:\n\\[\nC = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix},\n\\]\nwhere each \\(A_{ij}\\) is itself a smaller matrix.\nThis structure is useful in:\n\nComputation: Algorithms often process blocks instead of individual entries.\nTheory: Many proofs and factorizations rely on viewing a matrix in blocks (e.g., LU, QR, Schur decomposition).\nApplications: Partitioning data tables into logical sections.\n\nExample: Splitting a 4×4 matrix into four 2×2 blocks helps us treat it as a “matrix of matrices.”\n\n\nSpecial Shapes\nSome shapes of matrices are so common they deserve names:\n\nRow vector: 1×n matrix.\nColumn vector: n×1 matrix.\nDiagonal matrix: Nonzero entries only on the diagonal.\nIdentity matrix: Square diagonal matrix with 1’s on the diagonal.\nZero matrix: All entries are 0.\n\nRecognizing these shapes saves time and clarifies reasoning.\n\n\nWhy It Matters\nCareful attention to matrix shapes, indexing, and block views ensures:\n\nPrecision: We can describe positions unambiguously.\nStructure awareness: Recognizing patterns (diagonal, triangular, block) leads to more efficient computations.\nScalability: Block partitioning is the foundation of modern numerical linear algebra libraries, where matrices are too large to handle entry by entry.\nGeometry: Rows and columns as vectors connect matrix structure to span, basis, and dimension.\n\nThese basic tools prepare us for multiplication, transformations, and factorization.\n\n\nTry It Yourself\n\nWrite a 3×4 matrix and label the entry in row 2, column 3.\nExtract a 2×2 submatrix from the corners of a 4×4 matrix of your choice.\nBreak a 6×6 matrix into four 3×3 blocks. How would you represent it compactly?\nChallenge: Given\n\\[\nD = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12\n\\end{bmatrix},\n\\]\nwrite it as a block matrix with a 2×2 block in the top-left, a 2×2 block in the top-right, and a 1×4 block in the bottom row.\n\nBy practicing with shapes, indexing, and blocks, you’ll develop the ability to navigate matrices not just as raw grids of numbers but as structured objects ready for deeper algebraic and geometric insights.\n\n\n\n13. Matrix Addition and Scalar Multiplication\nBefore exploring matrix–vector and matrix–matrix multiplication, it is essential to understand the simplest operations we can perform with matrices: addition and scalar multiplication. These operations extend the rules we learned for vectors, but now applied to entire grids of numbers. Although straightforward, they are the foundation for more complex algebraic manipulations and help establish the idea of matrices as elements of a vector space.\n\nMatrix Addition: Entry by Entry\nIf two matrices \\(A\\) and \\(B\\) have the same shape (same number of rows and columns), we can add them by adding corresponding entries.\nFormally: If\n\\[\nA = [a_{ij}], \\quad B = [b_{ij}],\n\\]\nthen\n\\[\nA + B = [a_{ij} + b_{ij}].\n\\]\nExample:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n14 & 16 & 18\n\\end{bmatrix}.\n\\]\nKey point: Addition is only defined if the matrices are the same shape. A 2×3 matrix cannot be added to a 3×2 matrix.\n\n\nScalar Multiplication: Scaling Every Entry\nA scalar multiplies every entry of a matrix.\nFormally: For scalar \\(c\\) and matrix \\(A = [a_{ij}]\\),\n\\[\ncA = [c \\cdot a_{ij}].\n\\]\nExample:\n\\[\n3 \\cdot\n\\begin{bmatrix}\n2 & -1 \\\\\n0 & 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6 & -3 \\\\\n0 & 12\n\\end{bmatrix}.\n\\]\nThis mirrors vector scaling: stretching or shrinking the whole matrix by a constant factor.\n\n\nProperties of Addition and Scalar Multiplication\nThese two operations satisfy familiar algebraic properties that make the set of all m×n matrices into a vector space:\n\nCommutativity: \\(A + B = B + A\\).\nAssociativity: \\((A + B) + C = A + (B + C)\\).\nAdditive identity: \\(A + 0 = A\\), where 0 is the zero matrix.\nAdditive inverse: For every \\(A\\), there exists \\(-A\\) such that \\(A + (-A) = 0\\).\nDistributivity: \\(c(A + B) = cA + cB\\).\nCompatibility: \\((c + d)A = cA + dA\\).\nScalar associativity: \\((cd)A = c(dA)\\).\nUnit scalar: \\(1A = A\\).\n\nThese guarantee that working with matrices feels like working with numbers and vectors, only in a higher-level setting.\n\n\nMatrix Arithmetic as Table Operations\nFrom the table view, addition and scalar multiplication are just simple bookkeeping: line up two tables of the same shape and add entry by entry; multiply the whole table by a constant.\nExample: Imagine two spreadsheets of monthly expenses. Adding them gives combined totals. Multiplying by 12 converts a monthly table into a yearly estimate.\n\n\nMatrix Arithmetic as Machine Operations\nFrom the machine view, these operations adjust the behavior of linear transformations:\n\nAdding matrices corresponds to adding their effects when applied to vectors.\nScaling a matrix scales the effect of the transformation.\n\nExample: Let \\(A\\) rotate vectors slightly, and \\(B\\) stretch vectors. The matrix \\(A + B\\) represents a transformation that applies both influences together. Scaling by 2 doubles the effect of the transformation.\n\n\nSpecial Case: Zero and Identity\n\nZero matrix: All entries are 0. Adding it to any matrix changes nothing.\nScalar multiples of the identity: \\(cI\\) scales every vector by c when applied. For example, \\(2I\\) doubles every vector’s length.\n\nThese act as neutral or scaling elements in matrix arithmetic.\n\n\nGeometric Intuition\n\nIn \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\), adding transformation matrices is like superimposing geometric effects: e.g., one matrix shears, another rotates, their sum mixes both.\nScaling a transformation makes its action stronger or weaker. Doubling a shear makes it twice as pronounced.\n\nThis shows that even before multiplication, addition and scaling already have geometric meaning.\n\n\nWhy It Matters\nThough simple, these operations:\n\nDefine matrices as elements of vector spaces.\nLay the groundwork for linear combinations of matrices, critical in eigenvalue problems, optimization, and control theory.\nEnable modular problem-solving: break big transformations into smaller ones and recombine them.\nAppear everywhere in practice, from combining datasets to scaling transformations.\n\nWithout addition and scalar multiplication, we could not treat matrices systematically as algebraic objects.\n\n\nTry It Yourself\n\nAdd\n\n\\[\n\\begin{bmatrix}\n2 & 0 \\\\\n1 & 3\n\\end{bmatrix}\n\\quad \\text{and} \\quad\n\\begin{bmatrix}\n-2 & 5 \\\\\n4 & -3\n\\end{bmatrix}.\n\\]\n\nMultiply\n\n\\[\n\\begin{bmatrix}\n1 & -1 & 2 \\\\\n0 & 3 & 4\n\\end{bmatrix}\n\\]\nby –2.\n\nShow that (A + B) + C = A + (B + C) with explicit 2×2 matrices.\nChallenge: Construct two 3×3 matrices A and B such that A + B = 0. What does that tell you about B?\n\nBy practicing these fundamentals, you will see that even the most basic operations on matrices already build the algebraic backbone for deeper results like matrix multiplication, transformations, and factorization.\n\n\n\n14. Matrix–Vector Product (Linear Combinations of Columns)\nWe now arrive at one of the most important operations in all of linear algebra: the matrix–vector product. This operation takes a matrix \\(A\\) and a vector x, and produces a new vector. While the computation is straightforward, its interpretations are deep: it can be seen as combining rows, as combining columns, or as applying a linear transformation. This is the operation that connects matrices to the geometry of vector spaces.\n\nThe Algebraic Rule\nSuppose \\(A\\) is an \\(m \\times n\\) matrix, and x is a vector in \\(\\mathbb{R}^n\\). The product \\(A\\mathbf{x}\\) is a vector in \\(\\mathbb{R}^m\\), defined as:\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix},\n\\quad\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix}.\n\\]\nThen:\n\\[\nA\\mathbf{x} =\n\\begin{bmatrix}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\\\\n\\vdots \\\\\na_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n\n\\end{bmatrix}.\n\\]\nEach entry of the output is a dot product between one row of \\(A\\) and the vector x.\n\n\nRow View: Dot Products\nFrom the row perspective, \\(A\\mathbf{x}\\) is computed row by row:\n\nTake each row of \\(A\\).\nDot it with x.\nThat result becomes one entry of the output.\n\nExample:\n\\[\nA =\n\\begin{bmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n-1 & 2\n\\end{bmatrix}, \\quad\n\\mathbf{x} =\n\\begin{bmatrix}\n5 \\\\\n-1\n\\end{bmatrix}.\n\\]\n\nFirst row dot x: \\(2(5) + 1(-1) = 9\\).\nSecond row dot x: \\(3(5) + 4(-1) = 11\\).\nThird row dot x: \\((-1)(5) + 2(-1) = -7\\).\n\nSo:\n\\[\nA\\mathbf{x} =\n\\begin{bmatrix}\n9 \\\\ 11 \\\\ -7\n\\end{bmatrix}.\n\\]\n\n\nColumn View: Linear Combinations\nFrom the column perspective, \\(A\\mathbf{x}\\) is a linear combination of the columns of A.\nIf\n\\[\nA =\n\\begin{bmatrix}\n| & | &  & | \\\\\na_1 & a_2 & \\cdots & a_n \\\\\n| & | &  & |\n\\end{bmatrix},\n\\quad\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\]\nthen:\n\\[\nA\\mathbf{x} = x_1 a_1 + x_2 a_2 + \\cdots + x_n a_n.\n\\]\nThat is: multiply each column of \\(A\\) by the corresponding entry in x, then add them up.\nThis interpretation connects directly to the idea of span: the set of all vectors \\(A\\mathbf{x}\\) as x varies is exactly the span of the columns of \\(A\\).\n\n\nThe Machine View: Linear Transformations\nThe machine view ties everything together: multiplying a vector by a matrix means applying the linear transformation represented by the matrix.\n\nIf \\(A\\) is a 2×2 rotation matrix, then \\(A\\mathbf{x}\\) rotates the vector x.\nIf \\(A\\) is a scaling matrix, then \\(A\\mathbf{x}\\) stretches or shrinks x.\nIf \\(A\\) is a projection matrix, then \\(A\\mathbf{x}\\) projects x onto a line or plane.\n\nThus, the algebraic definition encodes geometric and functional meaning.\n\n\nExamples of Geometric Action\n\nScaling:\n\n\\[\nA = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\nThen \\(A\\mathbf{x}\\) doubles the length of any vector x.\n\nReflection:\n\n\\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n\\]\nThis flips vectors across the x-axis.\n\nRotation by θ:\n\n\\[\nA = \\begin{bmatrix} \\cosθ & -\\sinθ \\\\ \\sinθ & \\cosθ \\end{bmatrix}.\n\\]\nThis rotates vectors counterclockwise by θ in the plane.\n\n\nWhy It Matters\nThe matrix–vector product is the building block of everything in linear algebra:\n\nIt defines the action of a matrix as a linear map.\nIt connects directly to span and dimension (columns generate all possible outputs).\nIt underpins solving linear systems, eigenvalue problems, and decompositions.\nIt is the engine of computation in applied mathematics, from computer graphics to machine learning (e.g., neural networks compute billions of matrix–vector products).\n\n\n\nTry It Yourself\n\nCompute\n\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n0 \\\\\n1\n\\end{bmatrix}.\n\\]\n\nExpress the result of the above product as a linear combination of the columns of the matrix.\nConstruct a 2×2 matrix that reflects vectors across the line \\(y = x\\). Test it on (1, 0) and (0, 1).\nChallenge: For a 3×3 matrix, show that the set of all possible \\(A\\mathbf{x}\\) (as x varies) is exactly the column space of \\(A\\).\n\nBy mastering both the computational rules and the interpretations of the matrix–vector product, you will gain the most important insight in linear algebra: matrices are not just tables-they are engines that transform space.\n\n\n\n15. Matrix–Matrix Product (Composition of Linear Steps)\nHaving understood how a matrix acts on a vector, the next natural step is to understand how one matrix can act on another. This leads us to the matrix–matrix product, a rule for combining two matrices into a single new matrix. Though the arithmetic looks complicated at first, the underlying idea is elegant: multiplying two matrices represents composing two linear transformations.\n\nThe Algebraic Rule\nSuppose \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix. Their product \\(C = AB\\) is an \\(m \\times p\\) matrix defined by:\n\\[\nc_{ij} = \\sum_{k=1}^n a_{ik} b_{kj}.\n\\]\nThat is: each entry of \\(C\\) is the dot product of the i-th row of \\(A\\) with the j-th column of \\(B\\).\n\n\nExample: A 2×3 times a 3×2\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}, \\quad\nB =\n\\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}.\n\\]\nProduct: \\(C = AB\\) will be 2×2.\n\n\\(c_{11} = 1\\cdot 7 + 2\\cdot 9 + 3\\cdot 11 = 58\\).\n\\(c_{12} = 1\\cdot 8 + 2\\cdot 10 + 3\\cdot 12 = 64\\).\n\\(c_{21} = 4\\cdot 7 + 5\\cdot 9 + 6\\cdot 11 = 139\\).\n\\(c_{22} = 4\\cdot 8 + 5\\cdot 10 + 6\\cdot 12 = 154\\).\n\nSo:\n\\[\nC =\n\\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}.\n\\]\n\n\nColumn View: Linear Combinations of Columns\nFrom the column perspective, \\(AB\\) is computed by applying \\(A\\) to each column of \\(B\\).\nIf \\(B = [b_1 \\; b_2 \\; \\cdots \\; b_p]\\), then:\n\\[\nAB = [A b_1 \\; A b_2 \\; \\cdots \\; A b_p].\n\\]\nThat is: multiply \\(A\\) by each column of \\(B\\). This is often the simplest way to think of the product.\n\n\nRow View: Linear Combinations of Rows\nFrom the row perspective, each row of \\(AB\\) is formed by combining rows of \\(B\\) using coefficients from a row of \\(A\\). This dual view is less common but equally useful, especially in proofs and algorithms.\n\n\nThe Machine View: Composition of Transformations\nThe most important interpretation is the machine view: multiplying matrices corresponds to composing transformations.\n\nIf \\(A\\) maps \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) and \\(B\\) maps \\(\\mathbb{R}^p \\to \\mathbb{R}^n\\), then \\(AB\\) maps \\(\\mathbb{R}^p \\to \\mathbb{R}^m\\).\nIn words: do \\(B\\) first, then \\(A\\).\n\nExample:\n\nLet \\(B\\) rotate vectors by 90°.\nLet \\(A\\) scale vectors by 2.\nThen \\(AB\\) rotates and then scales-both steps combined into a single transformation.\n\n\n\nGeometric Examples\n\nScaling then rotation:\n\n\\[\nA = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\nThen \\(AB\\) scales vectors by 2 after rotating them 90°.\n\nProjection then reflection: If \\(B\\) projects onto the x-axis and \\(A\\) reflects across the y-axis, then \\(AB\\) represents “project then reflect.”\n\n\n\nProperties of Matrix Multiplication\n\nAssociative: \\((AB)C = A(BC)\\).\nDistributive: \\(A(B + C) = AB + AC\\).\nNot commutative: In general, \\(AB \\neq BA\\). Order matters!\nIdentity: \\(AI = IA = A\\).\n\nThese properties highlight that while multiplication is structured, it is not symmetric. The order encodes the order of operations in transformations.\n\n\nWhy It Matters\nMatrix multiplication is the core of linear algebra because:\n\nIt encodes function composition in algebraic form.\nIt provides a way to capture multiple transformations in a single matrix.\nIt underpins algorithms in computer graphics, robotics, statistics, and machine learning.\nIt reveals deeper structure, like commutativity failing, which reflects real-world order of operations.\n\nAlmost every application of linear algebra-solving equations, computing eigenvalues, training neural networks-relies on efficient matrix multiplication.\n\n\nTry It Yourself\n\nCompute\n\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n2 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n4 & 5 \\\\\n6 & 7\n\\end{bmatrix}.\n\\]\n\nShow that \\(AB \\neq BA\\) for the matrices\n\n\\[\nA = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix},\n\\quad\nB = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\n\nConstruct two 2×2 matrices where \\(AB = BA\\). Why does commutativity happen here?\nChallenge: If \\(A\\) is a projection and \\(B\\) is a rotation, compute \\(AB\\) and \\(BA\\). Do they represent the same geometric operation?\n\nThrough these perspectives, the matrix–matrix product shifts from being a mechanical formula to being a language for combining linear steps-each product telling the story of “do this, then that.”\n\n\n\n16. Identity, Inverse, and Transpose\nWith addition, scalar multiplication, and matrix multiplication in place, we now introduce three special operations and objects that form the backbone of matrix algebra: the identity matrix, the inverse of a matrix, and the transpose of a matrix. Each captures a fundamental principle-neutrality, reversibility, and symmetry-and together they provide the algebraic structure that makes linear algebra so powerful.\n\nThe Identity Matrix\nThe identity matrix is the matrix equivalent of the number 1 in multiplication.\n\nDefinition: The identity matrix \\(I_n\\) is the \\(n \\times n\\) matrix with 1’s on the diagonal and 0’s everywhere else.\n\nExample (3×3):\n\\[\nI_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\nProperty: For any \\(n \\times n\\) matrix \\(A\\),\n\\[\nAI_n = I_nA = A.\n\\]\nMachine view: \\(I\\) does nothing-it maps every vector to itself.\n\n\n\nThe Inverse of a Matrix\nThe inverse is the matrix equivalent of the reciprocal of a number.\n\nDefinition: For a square matrix \\(A\\), its inverse \\(A^{-1}\\) is the matrix such that\n\\[\nAA^{-1} = A^{-1}A = I.\n\\]\nNot all matrices have inverses. A matrix is invertible if and only if it is square and its determinant is nonzero.\n\nExample:\n\\[\nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 1\n\\end{bmatrix},\n\\quad\nA^{-1} = \\begin{bmatrix}\n1 & -1 \\\\\n-1 & 2\n\\end{bmatrix}.\n\\]\nCheck:\n\\[\nAA^{-1} = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & -1 \\\\\n-1 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = I.\n\\]\n\nMachine view: Applying \\(A\\) transforms a vector. Applying \\(A^{-1}\\) undoes that transformation, restoring the original input.\n\n\n\nNon-Invertible Matrices\nSome matrices cannot be inverted. These are called singular.\n\nExample:\n\\[\nB = \\begin{bmatrix}\n2 & 4 \\\\\n1 & 2\n\\end{bmatrix}.\n\\]\nHere, the second column is a multiple of the first. The transformation squashes vectors into a line, losing information-so it cannot be reversed.\n\nThis ties invertibility to geometry: a transformation that collapses dimensions cannot be undone.\n\n\nThe Transpose of a Matrix\nThe transpose reflects a matrix across its diagonal.\n\nDefinition: For \\(A = [a_{ij}]\\),\n\\[\nA^T = [a_{ji}].\n\\]\nIn words: rows become columns, columns become rows.\n\nExample:\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix},\n\\quad\nA^T = \\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}.\n\\]\n\nProperties:\n\n\\((A^T)^T = A\\).\n\\((A + B)^T = A^T + B^T\\).\n\\((cA)^T = cA^T\\).\n\\((AB)^T = B^T A^T\\) (note the reversed order!).\n\n\n\n\nSymmetric and Orthogonal Matrices\nTwo important classes emerge from the transpose:\n\nSymmetric matrices: \\(A = A^T\\). Example:\n\\[\n\\begin{bmatrix}\n2 & 3 \\\\\n3 & 5\n\\end{bmatrix}.\n\\]\nThese have beautiful properties: real eigenvalues and orthogonal eigenvectors.\nOrthogonal matrices: \\(Q^TQ = I\\). Their columns form an orthonormal set, and they represent pure rotations/reflections.\n\n\n\nWhy It Matters\n\nThe identity guarantees a neutral element for multiplication.\nThe inverse provides a way to solve equations \\(A\\mathbf{x} = \\mathbf{b}\\) via \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).\nThe transpose ties matrices to geometry, inner products, and symmetry.\nTogether, they form the algebraic foundation for deeper topics: determinants, eigenvalues, factorizations, and numerical methods.\n\nWithout these tools, matrix algebra would lack structure and reversibility.\n\n\nTry It Yourself\n\nCompute the transpose of\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n-3 & 4 & 5\n\\end{bmatrix}.\n\\]\n\nVerify that \\((AB)^T = B^TA^T\\) for\n\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 4 & 0 \\\\ 5 & 6 \\end{bmatrix}.\n\\]\n\nFind the inverse of\n\n\\[\n\\begin{bmatrix}\n3 & 2 \\\\\n1 & 1\n\\end{bmatrix}.\n\\]\n\nChallenge: Show that if \\(Q\\) is orthogonal, then \\(Q^{-1} = Q^T\\). Interpret this geometrically as saying “rotations can be undone by transposing.”\n\nThrough these exercises, you’ll see how identity, inverse, and transpose anchor the structure of linear algebra, providing neutrality, reversibility, and symmetry in every calculation.\n\n\n\n17. Symmetric, Diagonal, Triangular, and Permutation Matrices\nNot all matrices are created equal-some have special shapes or patterns that give them unique properties. These structured matrices are the workhorses of linear algebra: they simplify computation, reveal geometry, and form the building blocks for algorithms. In this section, we study four especially important classes: symmetric, diagonal, triangular, and permutation matrices.\n\nSymmetric Matrices\nA matrix is symmetric if it equals its transpose:\n\\[\nA = A^T.\n\\]\nExample:\n\\[\n\\begin{bmatrix}\n2 & 3 & 4 \\\\\n3 & 5 & 6 \\\\\n4 & 6 & 9\n\\end{bmatrix}.\n\\]\n\nGeometric meaning: Symmetric matrices represent linear transformations that have no “handedness.” They often arise in physics (energy, covariance, stiffness).\nAlgebraic fact: Symmetric matrices have real eigenvalues and an orthonormal basis of eigenvectors. This property underpins the spectral theorem, one of the pillars of linear algebra.\n\n\n\nDiagonal Matrices\nA matrix is diagonal if all non-diagonal entries are zero.\n\\[\nD = \\begin{bmatrix}\nd_1 & 0 & 0 \\\\\n0 & d_2 & 0 \\\\\n0 & 0 & d_3\n\\end{bmatrix}.\n\\]\n\nMultiplying by \\(D\\) scales each coordinate separately.\nComputations with diagonals are lightning fast:\n\nAdding: add diagonal entries.\nMultiplying: multiply diagonal entries.\nInverting: invert each diagonal entry (if nonzero).\n\n\nExample:\n\\[\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2x \\\\\n3y\n\\end{bmatrix}.\n\\]\nThis is why diagonalization is so valuable: turning a general matrix into a diagonal one simplifies everything.\n\n\nTriangular Matrices\nA matrix is upper triangular if all entries below the main diagonal are zero, and lower triangular if all entries above the diagonal are zero.\n\nUpper triangular example:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix}.\n\\]\nLower triangular example:\n\\[\n\\begin{bmatrix}\n7 & 0 & 0 \\\\\n8 & 9 & 0 \\\\\n10 & 11 & 12\n\\end{bmatrix}.\n\\]\n\nWhy they matter:\n\nDeterminant = product of diagonal entries.\nEasy to solve systems by substitution (forward or backward).\nEvery square matrix can be factored into triangular matrices (LU decomposition).\n\n\n\nPermutation Matrices\nA permutation matrix is obtained by permuting the rows (or columns) of an identity matrix.\nExample:\n\\[\nP = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\nMultiplying by \\(P\\):\n\nOn the left, permutes the rows of a matrix.\nOn the right, permutes the columns of a matrix.\n\nPermutation matrices are used in pivoting strategies in elimination, ensuring numerical stability in solving systems. They are also orthogonal: \\(P^{-1} = P^T\\).\n\n\nConnections Between Them\n\nA diagonal matrix is a special case of triangular (both upper and lower).\nSymmetric matrices often become diagonal under orthogonal transformations.\nPermutation matrices help reorder triangular or diagonal matrices without breaking their structure.\n\nTogether, these classes show that structure leads to simplicity-many computational algorithms exploit these patterns for speed and stability.\n\n\nWhy It Matters\n\nSymmetric matrices guarantee stable and interpretable eigen-decompositions.\nDiagonal matrices make computation effortless.\nTriangular matrices are the backbone of elimination and factorization methods.\nPermutation matrices preserve structure while reordering, critical for algorithms.\n\nAlmost every advanced method in numerical linear algebra relies on reducing general matrices into one of these structured forms.\n\n\nTry It Yourself\n\nVerify that\n\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}\n\\]\nis symmetric. Find its transpose.\n\nCompute the determinant of\n\n\\[\n\\begin{bmatrix}\n3 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 5\n\\end{bmatrix}.\n\\]\n\nSolve\n\n\\[\n\\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 5 & 2 \\\\\n0 & 0 & 4\n\\end{bmatrix}\n\\mathbf{x} =\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\nusing back substitution.\n\nConstruct a 4×4 permutation matrix that swaps the first and last rows. Apply it to a 4×1 vector of your choice.\n\nBy exploring these four structured families, you’ll start to see that not all matrices are messy-many have order hidden in their arrangement, and exploiting that order is the key to both theoretical understanding and efficient computation.\n\n\n\n18. Trace and Basic Matrix Properties\nSo far we have studied shapes, multiplication rules, and special classes of matrices. In this section we introduce a simple but surprisingly powerful quantity: the trace of a matrix. Along with it, we review a set of basic matrix properties that provide shortcuts, invariants, and insights into how matrices behave.\n\nDefinition of the Trace\nFor a square matrix \\(A = [a_{ij}]\\) of size \\(n \\times n\\), the trace is the sum of the diagonal entries:\n\\[\n\\text{tr}(A) = a_{11} + a_{22} + \\cdots + a_{nn}.\n\\]\nExample:\n\\[\nA = \\begin{bmatrix}\n2 & 5 & 7 \\\\\n0 & 3 & 1 \\\\\n4 & 6 & 8\n\\end{bmatrix},\n\\quad\n\\text{tr}(A) = 2 + 3 + 8 = 13.\n\\]\nThe trace extracts a single number summarizing the “diagonal content” of a matrix.\n\n\nProperties of the Trace\nThe trace is linear and interacts nicely with multiplication and transposition:\n\nLinearity:\n\n\\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\).\n\\(\\text{tr}(cA) = c \\cdot \\text{tr}(A)\\).\n\nCyclic Property:\n\n\\(\\text{tr}(AB) = \\text{tr}(BA)\\), as long as the products are defined.\nMore generally, \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\).\nBut in general, \\(\\text{tr}(AB) \\neq \\text{tr}(A)\\text{tr}(B)\\).\n\nTranspose Invariance:\n\n\\(\\text{tr}(A^T) = \\text{tr}(A)\\).\n\nSimilarity Invariance:\n\nIf \\(B = P^{-1}AP\\), then \\(\\text{tr}(B) = \\text{tr}(A)\\).\nThis means the trace is a similarity invariant, depending only on the linear transformation, not the basis.\n\n\n\n\nTrace and Eigenvalues\nOne of the most important connections is between the trace and eigenvalues:\n\\[\n\\text{tr}(A) = \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n,\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues of \\(A\\) (counting multiplicity).\nThis links the simple diagonal sum to the deep spectral properties of the matrix.\nExample:\n\\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix}, \\quad\n\\text{tr}(A) = 4, \\quad\n\\lambda_1 = 1, \\; \\lambda_2 = 3, \\quad \\lambda_1 + \\lambda_2 = 4.\n\\]\n\n\nOther Basic Matrix Properties\nAlongside the trace, here are some important algebraic facts that every student of linear algebra must know:\n\nDeterminant vs. Trace:\n\nFor 2×2 matrices, \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), \\(\\text{tr}(A) = a + d\\), \\(\\det(A) = ad - bc\\).\nTogether, trace and determinant encode the eigenvalues: roots of \\(x^2 - \\text{tr}(A)x + \\det(A) = 0\\).\n\nNorms and Inner Products:\n\nThe Frobenius norm is defined using the trace: \\(\\|A\\|_F = \\sqrt{\\text{tr}(A^TA)}\\).\n\nOrthogonal Invariance:\n\nFor any orthogonal matrix \\(Q\\), \\(\\text{tr}(Q^TAQ) = \\text{tr}(A)\\).\n\n\n\n\nGeometric and Practical Meaning\n\nThe trace of a transformation can be seen as the sum of its action along the coordinate axes.\nIn physics, the trace of the stress tensor measures pressure.\nIn probability, the trace of a covariance matrix is the total variance of a system.\nIn statistics and machine learning, the trace is often used as a measure of overall “size” or complexity of a model.\n\n\n\nWhy It Matters\nThe trace is deceptively simple but incredibly powerful:\n\nIt connects directly to eigenvalues, forming a bridge between raw matrix entries and spectral theory.\nIt is invariant under similarity, making it a reliable measure of a transformation independent of basis.\nIt shows up in optimization, physics, statistics, and quantum mechanics.\nIt simplifies computations: many proofs in linear algebra reduce to trace properties.\n\n\n\nTry It Yourself\n\nCompute the trace of\n\n\\[\n\\begin{bmatrix}\n4 & 2 & 0 \\\\\n-1 & 3 & 5 \\\\\n7 & 6 & 1\n\\end{bmatrix}.\n\\]\n\nVerify that \\(\\text{tr}(AB) = \\text{tr}(BA)\\) for\n\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 4 & 0 \\\\ 5 & 6 \\end{bmatrix}.\n\\]\n\nFor the 2×2 matrix\n\n\\[\n\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix},\n\\]\ncompute its eigenvalues and check that their sum equals the trace.\n\nChallenge: Show that the total variance of a dataset with covariance matrix \\(\\Sigma\\) is equal to \\(\\text{tr}(\\Sigma)\\).\n\nMastering the trace and its properties will prepare you for the next leap: understanding how matrices interact with volume, orientation, and determinants.\n\n\n\n19. Affine Transforms and Homogeneous Coordinates\nUp to now, matrices have been used to describe linear transformations: scaling, rotating, reflecting, projecting. But real-world geometry often involves more than just linear effects-it includes translations (shifts) as well. A pure linear map cannot move the origin, so to handle translations (and combinations of them with rotations, scalings, and shears), we extend our toolkit to affine transformations. The secret weapon that makes this work is the idea of homogeneous coordinates.\n\nWhat is an Affine Transformation?\nAn affine transformation is any map of the form:\n\\[\nf(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b},\n\\]\nwhere \\(A\\) is a matrix (linear part) and \\(\\mathbf{b}\\) is a vector (translation part).\n\n\\(A\\) handles scaling, rotation, reflection, shear, or projection.\n\\(\\mathbf{b}\\) shifts everything by a constant amount.\n\nExamples in 2D:\n\nRotate by 90° and then shift right by 2.\nStretch vertically by 3 and shift upward by 1.\n\nAffine maps preserve parallel lines and ratios of distances along lines, but not necessarily angles or lengths.\n\n\nWhy Linear Maps Alone Aren’t Enough\nIf we only use a 2×2 matrix in 2D or 3×3 in 3D, the origin always stays fixed. That’s a limitation: real-world movements (like moving a shape from one place to another) require shifting the origin too. To capture both linear and translational effects uniformly, we need a clever trick.\n\n\nHomogeneous Coordinates\nThe trick is to add one extra coordinate.\n\nIn 2D, a point \\((x, y)\\) becomes \\((x, y, 1)\\).\nIn 3D, a point \\((x, y, z)\\) becomes \\((x, y, z, 1)\\).\n\nThis new representation is called homogeneous coordinates. It allows us to fold translations into matrix multiplication.\n\n\nAffine Transform as a Matrix in Homogeneous Form\nIn 2D:\n\\[\n\\begin{bmatrix}\na & b & t_x \\\\\nc & d & t_y \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nax + by + t_x \\\\\ncx + dy + t_y \\\\\n1\n\\end{bmatrix}.\n\\]\nHere,\n\nThe 2×2 block \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is the linear part.\nThe last column \\(\\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}\\) is the translation.\n\nSo with one unified matrix, we can handle both linear transformations and shifts.\n\n\nExamples in 2D\n\nTranslation by (2, 3):\n\n\\[\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & 3 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\nScaling by 2 in x and 3 in y, then shifting by (–1, 4):\n\n\\[\n\\begin{bmatrix}\n2 & 0 & -1 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\nRotation by 90° and shift right by 5:\n\n\\[\n\\begin{bmatrix}\n0 & -1 & 5 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\n\nHomogeneous Coordinates in 3D\nIn 3D, affine transformations use 4×4 matrices. The upper-left 3×3 block handles rotation, scaling, or shear; the last column encodes translation.\nExample: translation by (2, –1, 4):\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & 4 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n\\]\nThis formulation is universal in computer graphics and robotics.\n\n\nWhy It Matters\n\nUnified representation: Using homogeneous coordinates, we can treat translations as matrices, enabling consistent matrix multiplication for all transformations.\nPracticality: This approach underpins 3D graphics pipelines, animation, CAD, robotics, and computer vision.\nComposability: Multiple affine transformations can be combined into a single homogeneous matrix by multiplying them.\nGeometry preserved: Affine maps preserve straight lines and parallelism, essential in engineering and design.\n\n\n\nTry It Yourself\n\nWrite the homogeneous matrix that reflects across the x-axis and then shifts up by 3. Apply it to \\((2, 1)\\).\nConstruct a 4×4 homogeneous matrix that rotates around the z-axis by 90° and translates by (1, 2, 0).\nShow that multiplying two 3×3 homogeneous matrices in 2D yields another valid affine transform.\nChallenge: Prove that affine maps preserve parallel lines by applying a general affine matrix to two parallel lines and checking their slopes.\n\nMastering affine transformations and homogeneous coordinates bridges the gap between pure linear algebra and real-world geometry, giving you the mathematical foundation behind computer graphics, robotics, and spatial modeling.\n\n\n\n20. Computing with Matrices (Cost Counts and Simple Speedups)\nThus far, we have studied what matrices are and what they represent. But in practice, working with matrices also means thinking about computation-how much work operations take, how algorithms can be sped up, and why structure matters. This section introduces the basic ideas of computational cost in matrix operations, simple strategies for efficiency, and why these considerations are crucial in modern applications.\n\nCounting Operations: The Cost Model\nThe simplest way to measure the cost of a matrix operation is to count the basic arithmetic operations (additions and multiplications).\n\nMatrix–vector product: For an \\(m \\times n\\) matrix and an \\(n \\times 1\\) vector:\n\nEach of the \\(m\\) output entries requires \\(n\\) multiplications and \\(n-1\\) additions.\nTotal cost ≈ \\(2mn\\) operations.\n\nMatrix–matrix product: For an \\(m \\times n\\) matrix times an \\(n \\times p\\) matrix:\n\nEach of the \\(mp\\) entries requires \\(n\\) multiplications and \\(n-1\\) additions.\nTotal cost ≈ \\(2mnp\\) operations.\n\nGaussian elimination (solving \\(Ax=b\\)): For an \\(n \\times n\\) system:\n\nRoughly \\(\\tfrac{2}{3}n^3\\) operations.\n\n\nThese counts show how quickly costs grow with dimension. Doubling \\(n\\) makes the work 8 times larger for elimination.\n\n\nWhy Cost Counts Matter\n\nScalability: Small problems (2×2 or 3×3) are trivial, but modern datasets involve matrices with millions of rows. Knowing the cost is essential.\nFeasibility: Some exact algorithms become impossible for very large matrices. Approximation methods are used instead.\nOptimization: Engineers and scientists design specialized algorithms to reduce costs by exploiting structure (sparsity, symmetry, triangular form).\n\n\n\nSimple Speedups with Structure\n\nDiagonal Matrices: Multiplying by a diagonal matrix costs only \\(n\\) operations (scale each component).\nTriangular Matrices: Solving triangular systems requires only \\(\\tfrac{1}{2}n^2\\) operations (substitution), far cheaper than general elimination.\nSparse Matrices: If most entries are zero, we skip multiplications by zero. For large sparse systems, cost scales with the number of nonzeros, not \\(n^2\\).\nBlock Matrices: Breaking matrices into blocks allows algorithms to reuse optimized small-matrix routines (common in BLAS libraries).\n\n\n\nMemory Considerations\nCost is not only arithmetic: storage also matters.\n\nA dense \\(n \\times n\\) matrix requires \\(n^2\\) entries of memory.\nSparse storage formats (like CSR, COO) record only nonzero entries and their positions, saving massive space.\nMemory access speed can dominate arithmetic cost in large computations.\n\n\n\nParallelism and Hardware\nModern computing leverages hardware for speed:\n\nVectorization (SIMD): Perform many multiplications at once.\nParallelization: Split work across many CPU cores.\nGPUs: Specialize in massive parallel matrix–vector and matrix–matrix operations (critical in deep learning).\n\nThis is why linear algebra libraries (BLAS, LAPACK, cuBLAS) are indispensable: they squeeze performance from hardware.\n\n\nWhy It Matters\n\nEfficiency: Understanding cost lets us choose the right algorithm for the job.\nAlgorithm design: Structured matrices (diagonal, sparse, orthogonal) make computations much faster and more stable.\nApplications: Every field that uses matrices-graphics, optimization, statistics, AI-relies on efficient computation.\nFoundations: Later topics like LU/QR/SVD factorization are motivated by balancing cost and stability.\n\n\n\nTry It Yourself\n\nCompute the number of operations required for multiplying a 1000×500 matrix with a 500×200 matrix. Compare with multiplying a 1000×1000 dense matrix by a vector.\nShow how solving a 3×3 triangular system is faster than Gaussian elimination. Count the exact multiplications and additions.\nConstruct a sparse 5×5 matrix with only 7 nonzero entries. Estimate the cost of multiplying it by a vector versus a dense 5×5 matrix.\nChallenge: Suppose you need to store a 1,000,000×1,000,000 dense matrix. Estimate how much memory (in bytes) it would take if each entry is 8 bytes. Could it fit on a laptop? Why do sparse formats save the day?\n\nBy learning to count costs and exploit structure, you prepare yourself not only to understand matrices abstractly but also to use them effectively in real-world, large-scale problems. This balance between theory and computation is at the heart of modern linear algebra.\n\n\nClosing\nPatterns intertwine,\ntransformations gently fold,\nstructure in the square.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-3.-linear-systems-and-elimination",
    "href": "books/en-US/book.html#chapter-3.-linear-systems-and-elimination",
    "title": "The Book",
    "section": "Chapter 3. Linear Systems and Elimination",
    "text": "Chapter 3. Linear Systems and Elimination\n\n21. From Equations to Matrices\nLinear algebra often begins with systems of equations-collections of unknowns linked by linear relationships. While these systems can be solved directly using substitution or elimination, they quickly become messy when there are many variables. The key insight of linear algebra is that all systems of linear equations can be captured compactly by matrices and vectors. This section explains how we move from equations written out in words and symbols to the matrix form that powers computation.\n\nA Simple Example\nConsider this system of two equations in two unknowns:\n\\[\n\\begin{cases}  \n2x + y = 5 \\\\  \n3x - y = 4  \n\\end{cases}\n\\]\nAt first glance, this is just algebra: two equations, two unknowns. But notice the structure: each equation is a sum of multiples of the variables, set equal to a constant. This pattern-linear combinations of unknowns equal to a result-is exactly what matrices capture.\n\n\nWriting in Coefficient Table Form\nExtract the coefficients of each variable from the system:\n\nFirst equation: coefficients are \\(2\\) for \\(x\\), \\(1\\) for \\(y\\).\nSecond equation: coefficients are \\(3\\) for \\(x\\), \\(-1\\) for \\(y\\).\n\nArrange these coefficients in a rectangular array:\n\\[\nA = \\begin{bmatrix}  \n2 & 1 \\\\  \n3 & -1  \n\\end{bmatrix}.\n\\]\nThis matrix \\(A\\) is called the coefficient matrix.\nNext, write the unknowns as a vector:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}.\n\\]\nFinally, write the right-hand side constants as another vector:\n\\[\n\\mathbf{b} = \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix}.\n\\]\nNow the entire system can be written in a single line:\n\\[\nA\\mathbf{x} = \\mathbf{b}.\n\\]\n\n\nWhy This is Powerful\nThis compact form hides no information; it is equivalent to the original equations. But it gives us enormous advantages:\n\nClarity: We see the structure clearly-the system is “matrix times vector equals vector.”\nScalability: Whether we have 2 equations or 2000, the same notation applies.\nTools: All the machinery of matrix operations (elimination, inverses, decompositions) now becomes available.\nGeometry: The matrix equation \\(A\\mathbf{x} = \\mathbf{b}\\) means: combine the columns of \\(A\\) (scaled by entries of x) to land on b.\n\n\n\nA Larger Example\nSystem of three equations in three unknowns:\n\\[\n\\begin{cases}  \nx + 2y - z = 2 \\\\  \n2x - y + 3z = 1 \\\\  \n3x + y + 2z = 4  \n\\end{cases}\n\\]\n\nCoefficient matrix:\n\\[\nA = \\begin{bmatrix}  \n1 & 2 & -1 \\\\  \n2 & -1 & 3 \\\\  \n3 & 1 & 2  \n\\end{bmatrix}.\n\\]\nUnknown vector:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}.\n\\]\nConstant vector:\n\\[\n\\mathbf{b} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 4 \\end{bmatrix}.\n\\]\n\nMatrix form:\n\\[\nA\\mathbf{x} = \\mathbf{b}.\n\\]\nThis single equation captures three equations and three unknowns in one object.\n\n\nRow vs. Column View\n\nRow view: Each row of \\(A\\) dotted with x gives one equation.\nColumn view: The entire system means b is a linear combination of the columns of \\(A\\).\n\nFor the 2×2 case earlier:\n\\[\nA\\mathbf{x} = \\begin{bmatrix} 2 & 1 \\\\ 3 & -1 \\end{bmatrix}  \n\\begin{bmatrix} x \\\\ y \\end{bmatrix}  \n= x \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} + y \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}.\n\\]\nSo solving the system means finding scalars \\(x, y\\) that combine the columns of \\(A\\) to reach \\(\\mathbf{b}\\).\n\n\nAugmented Matrix Form\nSometimes we want to save space further. We can put the coefficients and constants side by side in an augmented matrix:\n\\[\n[A | \\mathbf{b}] =  \n\\begin{bmatrix}  \n2 & 1 & | & 5 \\\\  \n3 & -1 & | & 4  \n\\end{bmatrix}.\n\\]\nThis form is especially useful for elimination methods, where we manipulate rows without writing variables at each step.\n\n\nWhy It Matters\nThis step-rewriting equations as matrix form-is the gateway into linear algebra. Once you can do it, you no longer think of systems of equations as isolated lines on paper, but as a unified object that can be studied with general tools. It opens the door to:\n\nGaussian elimination,\nrank and null space,\ndeterminants,\neigenvalues,\noptimization methods.\n\nEvery major idea flows from this compact representation.\n\n\nTry It Yourself\n\nWrite the system\n\\[\n\\begin{cases}  \n4x - y = 7 \\\\  \n-2x + 3y = 5  \n\\end{cases}\n\\]\nin matrix form.\nFor the system\n\\[\n\\begin{cases}  \nx + y + z = 6 \\\\  \n2x - y + z = 3 \\\\  \nx - y - z = -2  \n\\end{cases}\n\\]\nbuild the coefficient matrix, unknown vector, and constant vector.\nExpress the augmented matrix for the above system.\nChallenge: Interpret the system in column view. What does it mean geometrically to express \\((6, 3, -2)\\) as a linear combination of the columns of the coefficient matrix?\n\nBy practicing these rewrites, you will see that linear algebra is not about juggling many equations-it is about seeing structure in one compact equation. This step transforms scattered equations into the language of matrices, where the real power begins.\n\n\n\n22. Row Operations\nOnce a system of linear equations has been expressed as a matrix, the next step is to simplify that matrix into a form where the solutions become clear. The main tool for this simplification is the set of elementary row operations. These operations allow us to manipulate the rows of a matrix in systematic ways that preserve the solution set of the corresponding system of equations.\n\nThe Three Types of Row Operations\nThere are exactly three types of legal row operations, each with a clear algebraic meaning:\n\nRow Swapping (\\(R_i \\leftrightarrow R_j\\)): Exchange two rows. This corresponds to reordering equations in a system. Since the order of equations doesn’t change the solutions, this operation is always valid.\nExample:\n\\[\n\\begin{bmatrix}  \n2 & 1 & | & 5 \\\\  \n3 & -1 & | & 4  \n\\end{bmatrix}  \n\\quad \\longrightarrow \\quad  \n\\begin{bmatrix}  \n3 & -1 & | & 4 \\\\  \n2 & 1 & | & 5  \n\\end{bmatrix}.\n\\]\nRow Scaling (\\(R_i \\to cR_i, \\; c \\neq 0\\)): Multiply all entries in a row by a nonzero constant. This is like multiplying both sides of an equation by the same number, which doesn’t change its truth.\nExample:\n\\[\n\\begin{bmatrix}  \n2 & 1 & | & 5 \\\\  \n3 & -1 & | & 4  \n\\end{bmatrix}  \n\\quad \\longrightarrow \\quad  \n\\begin{bmatrix}  \n1 & \\tfrac{1}{2} & | & \\tfrac{5}{2} \\\\  \n3 & -1 & | & 4  \n\\end{bmatrix}.\n\\]\nRow Replacement (\\(R_i \\to R_i + cR_j\\)): Add a multiple of one row to another. This corresponds to replacing one equation with a linear combination of itself and another, a fundamental elimination step.\nExample:\n\\[\n\\begin{bmatrix}  \n2 & 1 & | & 5 \\\\  \n3 & -1 & | & 4  \n\\end{bmatrix}  \n\\quad \\overset{R_2 \\to R_2 - \\tfrac{3}{2}R_1}{\\longrightarrow} \\quad  \n\\begin{bmatrix}  \n2 & 1 & | & 5 \\\\  \n0 & -\\tfrac{5}{2} & | & -\\tfrac{7}{2}  \n\\end{bmatrix}.\n\\]\n\n\n\nWhy These Are the Only Allowed Operations\nThese three operations are the backbone of elimination because they do not alter the solution set of the system. Each is equivalent to applying an invertible transformation:\n\nRow swaps are reversible (swap back).\nRow scalings by \\(c\\) can be undone by scaling by \\(1/c\\).\nRow replacements can be undone by adding the opposite multiple.\n\nThus, each operation is invertible, and the transformed system is always equivalent to the original.\n\n\nRow Operations as Matrices\nEach elementary row operation can itself be represented by multiplying on the left with a special matrix called an elementary matrix.\nFor example:\n\nSwapping rows 1 and 2 in a 2×2 system is done by\n\\[\nE = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\nScaling row 1 by 3 in a 2×2 system is done by\n\\[\nE = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n\\]\n\nThis perspective is crucial later for factorization methods like LU decomposition, where elimination is expressed as a product of elementary matrices.\n\n\nStep-by-Step Example\nSystem:\n\\[\n\\begin{cases}  \nx + 2y = 4 \\\\  \n3x + 4y = 10  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 2 & | & 4 \\\\  \n3 & 4 & | & 10  \n\\end{bmatrix}.\n\\]\n\nEliminate the \\(3x\\) under the first pivot: \\(R_2 \\to R_2 - 3R_1\\).\n\\[\n\\begin{bmatrix}  \n1 & 2 & | & 4 \\\\  \n0 & -2 & | & -2  \n\\end{bmatrix}.\n\\]\nScale the second row: \\(R_2 \\to -\\tfrac{1}{2}R_2\\).\n\\[\n\\begin{bmatrix}  \n1 & 2 & | & 4 \\\\  \n0 & 1 & | & 1  \n\\end{bmatrix}.\n\\]\nEliminate above the pivot: \\(R_1 \\to R_1 - 2R_2\\).\n\\[\n\\begin{bmatrix}  \n1 & 0 & | & 2 \\\\  \n0 & 1 & | & 1  \n\\end{bmatrix}.\n\\]\n\nSolution: \\(x = 2, \\; y = 1\\).\n\n\nGeometry of Row Operations\nRow operations do not alter the solution space:\n\nSwapping rows reorders equations but keeps the same lines or planes.\nScaling rows rescales equations but leaves their geometric set unchanged.\nAdding rows corresponds to combining constraints, but the shared intersection (solution set) is preserved.\n\nThus, row operations act like “reshaping the system” while leaving the intersection intact.\n\n\nWhy It Matters\nRow operations are the essential moves in solving linear systems by hand or computer. They:\n\nMake elimination systematic.\nPreserve solution sets while simplifying structure.\nLay the groundwork for echelon forms, rank, and factorization.\nProvide the mechanical steps that computers automate in Gaussian elimination.\n\n\n\nTry It Yourself\n\nApply row operations to reduce\n\\[\n\\begin{bmatrix}  \n2 & 1 & | & 7 \\\\  \n1 & -1 & | & 1  \n\\end{bmatrix}\n\\]\nto a form where the solution is obvious.\nShow explicitly why swapping two equations in a system doesn’t change its solutions.\nConstruct the elementary matrix for “add –2 times row 1 to row 3” in a 3×3 system.\nChallenge: Prove that any elementary row operation corresponds to multiplication by an invertible matrix.\n\nMastering these operations equips you with the mechanical and conceptual foundation for the next stage: systematically reducing matrices to row-echelon form.\n\n\n\n23. Row-Echelon and Reduced Row-Echelon Forms\nAfter introducing row operations, the natural question is: what are we trying to achieve by performing them? The answer is to transform a matrix into a standardized, simplified form where the solutions to the corresponding system of equations can be read off directly. Two such standardized forms are central in linear algebra: row-echelon form (REF) and reduced row-echelon form (RREF).\n\nRow-Echelon Form (REF)\nA matrix is in row-echelon form if:\n\nAll nonzero rows are above any rows of all zeros.\nIn each nonzero row, the first nonzero entry (called the leading entry or pivot) is to the right of the leading entry of the row above it.\nAll entries below a pivot are zero.\n\nExample of REF:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 3 & | & 4 \\\\  \n0 & 1 & -1 & | & 2 \\\\  \n0 & 0 & 5 & | & -3 \\\\  \n0 & 0 & 0 & | & 0  \n\\end{bmatrix}.\n\\]\nHere, the pivots are the first 1 in row 1, the 1 in row 2, and the 5 in row 3. Each pivot is to the right of the one above it, and all entries below pivots are zero.\n\n\nReduced Row-Echelon Form (RREF)\nA matrix is in reduced row-echelon form if, in addition to the rules of REF:\n\nEach pivot is equal to 1.\nEach pivot is the only nonzero entry in its column (everything above and below pivots is zero).\n\nExample of RREF:\n\\[\n\\begin{bmatrix}  \n1 & 0 & 0 & | & 3 \\\\  \n0 & 1 & 0 & | & -2 \\\\  \n0 & 0 & 1 & | & 1  \n\\end{bmatrix}.\n\\]\nThis form is so simplified that solutions can be read directly: here, \\(x=3\\), \\(y=-2\\), \\(z=1\\).\n\n\nRelationship Between REF and RREF\n\nREF is easier to reach-it only requires eliminating entries below pivots.\nRREF requires going further-clearing entries above pivots and scaling pivots to 1.\nEvery matrix can be reduced to REF (many possible versions), but RREF is unique: no matter how you proceed, if you carry out all row operations fully, you end with the same RREF.\n\n\n\nExample: Step-by-Step to RREF\nSystem:\n\\[\n\\begin{cases}  \nx + 2y + z = 4 \\\\  \n2x + 5y + z = 7 \\\\  \n3x + 6y + 2z = 10  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n2 & 5 & 1 & | & 7 \\\\  \n3 & 6 & 2 & | & 10  \n\\end{bmatrix}.\n\\]\n\nEliminate below first pivot (the 1 in row 1, col 1):\n\n\\(R_2 \\to R_2 - 2R_1\\)\n\\(R_3 \\to R_3 - 3R_1\\)\n\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n0 & 1 & -1 & | & -1 \\\\  \n0 & 0 & -1 & | & -2  \n\\end{bmatrix}.\n\\]\nThis is now in REF.\nScale pivots and eliminate above them:\n\n\\(R_3 \\to -R_3\\) to make pivot 1.\n\\(R_2 \\to R_2 + R_3\\).\n\\(R_1 \\to R_1 - R_2 - R_3\\).\n\nFinal:\n\\[\n\\begin{bmatrix}  \n1 & 0 & 0 & | & 2 \\\\  \n0 & 1 & 0 & | & 1 \\\\  \n0 & 0 & 1 & | & 2  \n\\end{bmatrix}.\n\\]\n\nSolution: \\(x=2, y=1, z=2\\).\n\n\nGeometry of REF and RREF\n\nREF corresponds to simplifying the system step by step, making it “triangular” so variables can be solved one after another.\nRREF corresponds to a system that is fully disentangled-each variable isolated, with its value or free-variable relationship explicitly visible.\n\n\n\nWhy It Matters\n\nREF is the foundation of Gaussian elimination, the workhorse algorithm for solving systems.\nRREF gives complete clarity: unique representation of solution sets, revealing free and pivot variables.\nRREF underlies algorithms in computer algebra systems, symbolic solvers, and educational tools.\nUnderstanding these forms builds intuition for rank, null space, and solution structure.\n\n\n\nTry It Yourself\n\nReduce\n\\[\n\\begin{bmatrix}  \n2 & 4 & | & 6 \\\\  \n1 & 3 & | & 5  \n\\end{bmatrix}\n\\]\nto REF, then RREF.\nFind the RREF of\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 3 \\\\  \n2 & 3 & 4 & | & 8 \\\\  \n1 & 2 & 3 & | & 5  \n\\end{bmatrix}.\n\\]\nExplain why two different elimination sequences can lead to different REF but the same RREF.\nChallenge: Prove that every matrix has a unique RREF by considering the effect of row operations systematically.\n\nReaching row-echelon and reduced row-echelon forms transforms messy systems into structured ones, turning algebraic clutter into an organized path to solutions.\n\n\n\n24. Pivots, Free Variables, and Leading Ones\nWhen reducing a matrix to row-echelon or reduced row-echelon form, certain positions in the matrix take on a special importance. These are the pivots-the leading nonzero entries in each row. Around them, the entire solution structure of a linear system is organized. Understanding pivots, the variables they anchor, and the freedom that arises from non-pivot columns is essential to solving linear equations systematically.\n\nWhat is a Pivot?\nIn row-echelon form, a pivot is the first nonzero entry in a row, moving from left to right. After scaling in reduced row-echelon form, each pivot is set to exactly 1.\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 0 & | & 5 \\\\  \n0 & 1 & 3 & | & -2 \\\\  \n0 & 0 & 0 & | & 0  \n\\end{bmatrix}\n\\]\n\nPivot in row 1: the 1 in column 1.\nPivot in row 2: the 1 in column 2.\nColumn 3 has no pivot.\n\nColumns with pivots are pivot columns. Columns without pivots correspond to free variables.\n\n\nPivot Variables vs. Free Variables\n\nPivot variables: Variables that align with pivot columns. They are determined by the equations.\nFree variables: Variables that align with non-pivot columns. They are unconstrained and can take arbitrary values.\n\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 0 & 2 & | & 3 \\\\  \n0 & 1 & -1 & | & 4  \n\\end{bmatrix}.\n\\]\nThis corresponds to:\n\\[\nx_1 + 2x_3 = 3, \\quad x_2 - x_3 = 4.\n\\]\nHere:\n\n\\(x_1\\) and \\(x_2\\) are pivot variables (from pivot columns 1 and 2).\n\\(x_3\\) is a free variable.\n\nThus, \\(x_1\\) and \\(x_2\\) depend on \\(x_3\\):\n\\[\nx_1 = 3 - 2x_3, \\quad x_2 = 4 + x_3, \\quad x_3 \\text{ free}.\n\\]\nThe solution set is infinite, described by the freedom in \\(x_3\\).\n\n\nGeometric Meaning\n\nPivot variables represent coordinates that are “pinned down.”\nFree variables correspond to directions along which the solution can extend infinitely.\n\nIn 2D:\n\nIf there is one pivot variable and one free variable, solutions form a line. In 3D:\nTwo pivots, one free → solutions form a line.\nOne pivot, two free → solutions form a plane.\n\nThus, the number of free variables determines the dimension of the solution set.\n\n\nRank and Free Variables\nThe number of pivot columns equals the rank of the matrix.\nIf the coefficient matrix \\(A\\) is \\(m \\times n\\):\n\nRank = number of pivots.\nNumber of free variables = \\(n - \\text{rank}(A)\\).\n\nThis is the rank–nullity connection in action:\n\\[\n\\text{number of variables} = \\text{rank} + \\text{nullity}.\n\\]\n\n\nStep-by-Step Example\nSystem:\n\\[\n\\begin{cases}  \nx + 2y + z = 4 \\\\  \n2x + 5y + z = 7  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n2 & 5 & 1 & | & 7  \n\\end{bmatrix}.\n\\]\nReduce:\n\n\\(R_2 \\to R_2 - 2R_1\\) →\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n0 & 1 & -1 & | & -1  \n\\end{bmatrix}.\n\\]\n\nNow:\n\nPivot columns: 1 and 2 → variables \\(x, y\\).\nFree column: 3 → variable \\(z\\).\n\nSolution:\n\\[\nx = 4 - 2y - z, \\quad y = -1 + z, \\quad z \\text{ free}.\n\\]\nSubstitute:\n\\[\n(x, y, z) = (6 - 3z, \\; -1 + z, \\; z).\n\\]\nSolutions form a line in 3D parameterized by \\(z\\).\n\n\nWhy Leading Ones Matter\nIn RREF, each pivot is scaled to 1, making it easy to isolate pivot variables. Without leading ones, equations may still be correct but harder to interpret.\nFor example:\n\\[\n\\begin{bmatrix}  \n2 & 0 & | & 6 \\\\  \n0 & -3 & | & 9  \n\\end{bmatrix}\n\\]\nbecomes\n\\[\n\\begin{bmatrix}  \n1 & 0 & | & 3 \\\\  \n0 & 1 & | & -3  \n\\end{bmatrix}.\n\\]\nThe solutions are immediately visible: \\(x=3, y=-3\\).\n\n\nWhy It Matters\n\nIdentifying pivots shows which variables are determined and which are free.\nThe number of pivots defines rank, a central concept in linear algebra.\nFree variables determine whether the system has a unique solution, infinitely many, or none.\nLeading ones in RREF give immediate transparency to the solution set.\n\n\n\nTry It Yourself\n\nReduce\n\\[\n\\begin{bmatrix}  \n1 & 3 & 1 & | & 5 \\\\  \n2 & 6 & 2 & | & 10  \n\\end{bmatrix}\n\\]\nand identify pivot and free variables.\nFor the system\n\\[\nx + y + z = 2, \\quad 2x + 3y + 5z = 7,\n\\]\nwrite the RREF and express the solution with free variables.\nCompute the rank and number of free variables of a 3×5 matrix with two pivot columns.\nChallenge: Show that if the number of pivots equals the number of variables, the system has either no solution or a unique solution, but never infinitely many.\n\nUnderstanding pivots and free variables provides the key to classifying solution sets: unique, infinite, or none. This classification lies at the heart of solving linear systems.\n\n\n\n25. Solving Consistent Systems\nA system of linear equations is called consistent if it has at least one solution. Consistency is the first property to check when working with a system, because before worrying about uniqueness or parametrization, we must know whether a solution exists at all. This section explains how to recognize consistent systems, how to solve them using row-reduction, and how to describe their solutions in terms of pivots and free variables.\n\nWhat Consistency Means\nGiven a system \\(A\\mathbf{x} = \\mathbf{b}\\):\n\nConsistent: At least one solution \\(\\mathbf{x}\\) satisfies the system.\nInconsistent: No solution exists.\n\nConsistency depends on the relationship between the vector \\(\\mathbf{b}\\) and the column space of \\(A\\):\n\\[\n\\mathbf{b} \\in \\text{Col}(A) \\quad \\iff \\quad \\text{system is consistent}.\n\\]\nIf \\(\\mathbf{b}\\) cannot be written as a linear combination of the columns of \\(A\\), the system has no solution.\n\n\nChecking Consistency with Row Reduction\nTo test consistency, reduce the augmented matrix \\([A | \\mathbf{b}]\\) to row-echelon form.\n\nIf you find a row of the form:\n\\[\n[0 \\;\\; 0 \\;\\; \\dots \\;\\; 0 \\;|\\; c], \\quad c \\neq 0,\n\\]\nthen the system is inconsistent (contradiction: 0 = c).\nIf no such contradiction appears, the system is consistent.\n\n\n\nExample 1: Consistent System with Unique Solution\nSystem:\n\\[\n\\begin{cases}  \nx + y = 2 \\\\  \nx - y = 0  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 2 \\\\  \n1 & -1 & | & 0  \n\\end{bmatrix}.\n\\]\nRow reduce:\n\n\\(R_2 \\to R_2 - R_1\\):\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 2 \\\\  \n0 & -2 & | & -2  \n\\end{bmatrix}.\n\\]\n\\(R_2 \\to -\\tfrac{1}{2}R_2\\):\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 2 \\\\  \n0 & 1 & | & 1  \n\\end{bmatrix}.\n\\]\n\\(R_1 \\to R_1 - R_2\\):\n\\[\n\\begin{bmatrix}  \n1 & 0 & | & 1 \\\\  \n0 & 1 & | & 1  \n\\end{bmatrix}.\n\\]\n\nSolution: \\(x = 1, \\; y = 1\\). Unique solution.\n\n\nExample 2: Consistent System with Infinitely Many Solutions\nSystem:\n\\[\n\\begin{cases}  \nx + y + z = 3 \\\\  \n2x + 2y + 2z = 6  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 3 \\\\  \n2 & 2 & 2 & | & 6  \n\\end{bmatrix}.\n\\]\nRow reduce:\n\n\\(R_2 \\to R_2 - 2R_1\\):\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 3 \\\\  \n0 & 0 & 0 & | & 0  \n\\end{bmatrix}.\n\\]\n\nNo contradiction, so consistent. Solution:\n\\[\nx = 3 - y - z, \\quad y \\text{ free}, \\quad z \\text{ free}.\n\\]\nThe solution set is a plane in \\(\\mathbb{R}^3\\).\n\n\nExample 3: Inconsistent System (for contrast)\nSystem:\n\\[\n\\begin{cases}  \nx + y = 1 \\\\  \nx + y = 2  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 1 \\\\  \n1 & 1 & | & 2  \n\\end{bmatrix}.\n\\]\nRow reduce:\n\n\\(R_2 \\to R_2 - R_1\\):\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 1 \\\\  \n0 & 0 & | & 1  \n\\end{bmatrix}.\n\\]\n\nContradiction: \\(0 = 1\\). Inconsistent, no solution.\n\n\nGeometric Interpretation of Consistency\n\nIn 2D:\n\nTwo lines intersect at a point → consistent, unique solution.\nTwo lines overlap → consistent, infinitely many solutions.\nTwo lines are parallel and distinct → inconsistent, no solution.\n\nIn 3D:\n\nThree planes intersect at a point → unique solution.\nPlanes intersect along a line or coincide → infinitely many solutions.\nPlanes fail to meet (like a triangular “gap”) → no solution.\n\n\n\n\nPivot Structure and Solutions\n\nUnique solution: Every variable is a pivot variable (no free variables).\nInfinitely many solutions: At least one free variable exists, but no contradiction.\nNo solution: Contradictory row appears in augmented matrix.\n\n\n\nWhy It Matters\n\nConsistency is the first checkpoint in solving systems.\nThe classification into unique, infinite, or none underpins all of linear algebra.\nUnderstanding consistency ties algebra (row operations) to geometry (intersections of lines, planes, hyperplanes).\nThese ideas scale: in data science and engineering, checking whether equations are consistent is equivalent to asking if a model fits observed data.\n\n\n\nTry It Yourself\n\nReduce the augmented matrix\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 5 \\\\  \n2 & 4 & 2 & | & 10 \\\\  \n3 & 6 & 3 & | & 15  \n\\end{bmatrix}\n\\]\nand determine if the system is consistent.\nClassify the system as having unique, infinite, or no solutions:\n\\[\n\\begin{cases}  \nx + y + z = 2 \\\\  \nx - y + z = 0 \\\\  \n2x + 0y + 2z = 3  \n\\end{cases}\n\\]\nExplain geometrically what it means when the augmented matrix has a contradictory row.\nChallenge: Show algebraically that a system is consistent if and only if \\(\\mathbf{b}\\) lies in the span of the columns of \\(A\\).\n\nConsistent systems mark the balance point between algebraic rules and geometric reality: they are where equations and space meet in harmony.\n\n\n\n26. Detecting Inconsistency\nNot every system of linear equations has a solution. Some are inconsistent, meaning the equations contradict one another and no vector \\(\\mathbf{x}\\) can satisfy them all at once. Detecting such inconsistency early is crucial: it saves wasted effort trying to solve an impossible system and reveals important geometric and algebraic properties.\n\nWhat Inconsistency Looks Like Algebraically\nConsider the system:\n\\[\n\\begin{cases}  \nx + y = 1 \\\\  \nx + y = 3  \n\\end{cases}\n\\]\nClearly, the two equations cannot both be true. In augmented matrix form:\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 1 \\\\  \n1 & 1 & | & 3  \n\\end{bmatrix}.\n\\]\nRow reduction gives:\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 1 \\\\  \n0 & 0 & | & 2  \n\\end{bmatrix}.\n\\]\nThe bottom row says \\(0 = 2\\), a contradiction. This is the hallmark of inconsistency: a row of zeros in the coefficient part, with a nonzero constant in the augmented part.\n\n\nGeneral Rule for Detection\nA system \\(A\\mathbf{x} = \\mathbf{b}\\) is inconsistent if, after row reduction, the augmented matrix contains a row of the form:\n\\[\n[0 \\;\\; 0 \\;\\; \\dots \\;\\; 0 \\;|\\; c], \\quad c \\neq 0.\n\\]\nThis indicates that all variables vanish from the equation, leaving an impossible statement like \\(0 = c\\).\n\n\nExample 1: Parallel Lines in 2D\n\\[\n\\begin{cases}  \nx + y = 2 \\\\  \n2x + 2y = 5  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 2 \\\\  \n2 & 2 & | & 5  \n\\end{bmatrix}.\n\\]\nRow reduce:\n\n\\(R_2 \\to R_2 - 2R_1\\):\n\n\\[\n\\begin{bmatrix}  \n1 & 1 & | & 2 \\\\  \n0 & 0 & | & 1  \n\\end{bmatrix}.\n\\]\nContradiction: no solution. Geometrically, the two equations are parallel lines that never intersect.\n\n\nExample 2: Contradictory Planes in 3D\n\\[\n\\begin{cases}  \nx + y + z = 1 \\\\  \n2x + 2y + 2z = 2 \\\\  \nx + y + z = 3  \n\\end{cases}\n\\]\nThe first and third equations already conflict: the same plane equation is forced to equal two different constants.\nAugmented matrix reduces to:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 1 \\\\  \n0 & 0 & 0 & | & 0 \\\\  \n0 & 0 & 0 & | & 2  \n\\end{bmatrix}.\n\\]\nContradiction: no solution. The “planes” fail to intersect in common.\n\n\nGeometry of Inconsistency\n\nIn 2D: Inconsistent systems correspond to parallel lines with different intercepts.\nIn 3D: They correspond to planes that are parallel but offset, or planes arranged in a way that leaves a “gap” (no shared intersection).\nIn higher dimensions: Inconsistency means the target vector \\(\\mathbf{b}\\) lies outside the column space of \\(A\\).\n\n\n\nRank Test for Consistency\nAnother way to detect inconsistency is using ranks.\n\nLet \\(\\text{rank}(A)\\) be the number of pivots in the coefficient matrix.\nLet \\(\\text{rank}([A|\\mathbf{b}])\\) be the number of pivots in the augmented matrix.\n\nRule:\n\nIf \\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}])\\), the system is consistent.\nIf \\(\\text{rank}(A) &lt; \\text{rank}([A|\\mathbf{b}])\\), the system is inconsistent.\n\nThis rank condition is fundamental and works in any dimension.\n\n\nWhy It Matters\n\nInconsistency reveals overdetermined or contradictory data in real problems (physics, engineering, statistics).\nThe ability to detect inconsistency quickly through row reduction or rank saves computation.\nIt connects geometry (non-intersecting spaces) with algebra (contradictory rows).\nIt prepares the way for least-squares methods, where inconsistent systems are approximated instead of solved exactly.\n\n\n\nTry It Yourself\n\nReduce the augmented matrix\n\n\\[\n\\begin{bmatrix}  \n1 & -1 & | & 2 \\\\  \n2 & -2 & | & 5  \n\\end{bmatrix}\n\\]\nand decide if the system is consistent.\n\nShow geometrically why the system\n\n\\[\nx + y = 0, \\quad x + y = 1\n\\]\nis inconsistent.\n\nUse the rank test to check consistency of\n\n\\[\n\\begin{cases}  \nx + y + z = 2 \\\\  \n2x + 2y + 2z = 4 \\\\  \n3x + 3y + 3z = 5  \n\\end{cases}\n\\]\n\nChallenge: Explain why \\(\\text{rank}(A) &lt; \\text{rank}([A|\\mathbf{b}])\\) implies inconsistency, using the concept of the column space.\n\nDetecting inconsistency is not just about spotting contradictions-it connects algebra, geometry, and linear transformations, showing exactly when a system cannot possibly fit together.\n\n\n\n27. Gaussian Elimination by Hand\nGaussian elimination is the systematic procedure for solving systems of linear equations by using row operations to simplify the augmented matrix. The goal is to transform the system into row-echelon form (REF) and then use back substitution to find the solutions. This method is the backbone of linear algebra computations and is the foundation of most computer algorithms for solving linear systems.\n\nThe Big Idea\n\nRepresent the system as an augmented matrix.\nUse row operations to eliminate variables step by step, moving left to right, top to bottom.\nStop when the matrix is in REF.\nSolve the triangular system by back substitution.\n\n\n\nStep-by-Step Recipe\nSuppose we have \\(n\\) equations with \\(n\\) unknowns.\n\nChoose a pivot in the first column (a nonzero entry). If needed, swap rows to bring a nonzero entry to the top.\nEliminate below the pivot by subtracting multiples of the pivot row from lower rows so that all entries below the pivot become zero.\nMove to the next row and next column, pick the next pivot, and repeat elimination.\nContinue until all pivots are in stair-step form (REF).\nUse back substitution to solve for the unknowns starting from the bottom row.\n\n\n\nExample 1: A 2×2 System\nSystem:\n\\[\n\\begin{cases}  \nx + 2y = 5 \\\\  \n3x + 4y = 11  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 2 & | & 5 \\\\  \n3 & 4 & | & 11  \n\\end{bmatrix}.\n\\]\n\nPivot at (1,1) = 1.\nEliminate below: \\(R_2 \\to R_2 - 3R_1\\).\n\\[\n\\begin{bmatrix}  \n1 & 2 & | & 5 \\\\  \n0 & -2 & | & -4  \n\\end{bmatrix}.\n\\]\nBack substitution: From row 2: \\(-2y = -4 \\implies y = 2\\). Substitute into row 1: \\(x + 2(2) = 5 \\implies x = 1\\).\n\nSolution: \\((x, y) = (1, 2)\\).\n\n\nExample 2: A 3×3 System\nSystem:\n\\[\n\\begin{cases}  \nx + y + z = 6 \\\\  \n2x + 3y + z = 14 \\\\  \nx - y + 2z = 2  \n\\end{cases}\n\\]\nAugmented matrix:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 6 \\\\  \n2 & 3 & 1 & | & 14 \\\\  \n1 & -1 & 2 & | & 2  \n\\end{bmatrix}.\n\\]\nStep 1: Pivot at (1,1). Eliminate below:\n\n\\(R_2 \\to R_2 - 2R_1\\).\n\\(R_3 \\to R_3 - R_1\\).\n\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 6 \\\\  \n0 & 1 & -1 & | & 2 \\\\  \n0 & -2 & 1 & | & -4  \n\\end{bmatrix}.\n\\]\nStep 2: Pivot at (2,2). Eliminate below: \\(R_3 \\to R_3 + 2R_2\\).\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 6 \\\\  \n0 & 1 & -1 & | & 2 \\\\  \n0 & 0 & -1 & | & 0  \n\\end{bmatrix}.\n\\]\nStep 3: Pivot at (3,3). Scale row: \\(R_3 \\to -R_3\\).\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & | & 6 \\\\  \n0 & 1 & -1 & | & 2 \\\\  \n0 & 0 & 1 & | & 0  \n\\end{bmatrix}.\n\\]\nBack substitution:\n\nFrom row 3: \\(z = 0\\).\nFrom row 2: \\(y - z = 2 \\implies y = 2\\).\nFrom row 1: \\(x + y + z = 6 \\implies x = 4\\).\n\nSolution: \\((x, y, z) = (4, 2, 0)\\).\n\n\nWhy Gaussian Elimination Always Works\n\nEach step reduces the number of variables in the lower equations.\nPivoting ensures stability (swap rows to avoid dividing by zero).\nThe algorithm either produces a triangular system (solvable by substitution) or reveals inconsistency (contradictory row).\n\n\n\nGeometric Interpretation\n\nElimination corresponds to progressively restricting the solution set:\n\nFirst equation → a plane in \\(\\mathbb{R}^3\\).\nAdd second equation → intersection becomes a line.\nAdd third equation → intersection becomes a point (unique solution) or vanishes (inconsistent).\n\n\n\n\nWhy It Matters\n\nGaussian elimination is the foundation for solving systems by hand and by computer.\nIt reveals whether a system is consistent and if solutions are unique or infinite.\nIt is the starting point for advanced methods like LU decomposition, QR factorization, and numerical solvers.\nIt shows the interplay between algebra (row operations) and geometry (intersections of subspaces).\n\n\n\nTry It Yourself\n\nSolve the system\n\\[\n\\begin{cases}  \n2x + y = 7 \\\\  \n4x + 3y = 15  \n\\end{cases}\n\\]\nusing Gaussian elimination.\nReduce\n\\[\n\\begin{bmatrix}  \n1 & 2 & -1 & | & 3 \\\\  \n3 & 8 & 1 & | & 12 \\\\  \n2 & 6 & 3 & | & 11  \n\\end{bmatrix}\n\\]\nto REF and solve.\nPractice with a system that has infinitely many solutions:\n\\[\nx + y + z = 4, \\quad 2x + 2y + 2z = 8.\n\\]\nChallenge: Explain why Gaussian elimination always terminates in at most \\(n\\) pivot steps for an \\(n \\times n\\) system.\n\nGaussian elimination transforms the complexity of many equations into an orderly process, making the hidden structure of solutions visible step by step.\n\n\n\n28. Back Substitution and Solution Sets\nOnce Gaussian elimination reduces a system to row-echelon form (REF), the next step is to actually solve for the unknowns. This process is called back substitution: we begin with the bottom equation (which involves the fewest variables) and work our way upward, solving step by step. Back substitution is what converts the structured triangular system into explicit solutions.\n\nThe Structure of Row-Echelon Form\nA system in REF looks like this:\n\\[\n\\begin{bmatrix}  \n- & * & * & * & | & * \\\\  \n0 & * & * & * & | & * \\\\  \n0 & 0 & * & * & | & * \\\\  \n0 & 0 & 0 & * & | & *  \n\\end{bmatrix}\n\\]\n\nEach row corresponds to an equation with fewer variables than the row above.\nThe bottom equation has only one or two variables.\nThis triangular form makes it possible to solve “from the bottom up.”\n\n\n\nStep-by-Step Example: Unique Solution\nSystem after elimination:\n\\[\n\\begin{bmatrix}  \n1 & 2 & -1 & | & 3 \\\\  \n0 & 1 & 2 & | & 4 \\\\  \n0 & 0 & 1 & | & 2  \n\\end{bmatrix}.\n\\]\nThis corresponds to:\n\\[\n\\begin{cases}  \nx + 2y - z = 3 \\\\  \ny + 2z = 4 \\\\  \nz = 2  \n\\end{cases}\n\\]\n\nFrom the last equation: \\(z = 2\\).\nSubstitute into the second: \\(y + 2(2) = 4 \\implies y = 0\\).\nSubstitute into the first: \\(x + 2(0) - 2 = 3 \\implies x = 5\\).\n\nSolution: \\((x, y, z) = (5, 0, 2)\\).\n\n\nInfinite Solutions with Free Variables\nNot all systems reduce to unique solutions. If there are free variables (non-pivot columns), back substitution expresses pivot variables in terms of free ones.\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n0 & 1 & -1 & | & 1 \\\\  \n0 & 0 & 0 & | & 0  \n\\end{bmatrix}.\n\\]\nEquations:\n\\[\n\\begin{cases}  \nx + 2y + z = 4 \\\\  \ny - z = 1  \n\\end{cases}\n\\]\n\nFrom row 2: \\(y = 1 + z\\).\nFrom row 1: \\(x + 2(1 + z) + z = 4 \\implies x = 2 - 3z\\).\n\nSolution set:\n\\[\n(x, y, z) = (2 - 3t, \\; 1 + t, \\; t), \\quad t \\in \\mathbb{R}.\n\\]\nHere \\(z = t\\) is the free variable. The solutions form a line in 3D.\n\n\nGeneral Solution Structure\nFor a consistent system:\n\nUnique solution → every variable is a pivot variable (no free variables).\nInfinitely many solutions → some free variables remain. The solution set is parametrized by these variables and forms a line, plane, or higher-dimensional subspace.\nNo solution → contradiction discovered earlier, so back substitution is impossible.\n\n\n\nGeometric Meaning\n\nUnique solution → a single intersection point of lines/planes.\nInfinite solutions → overlapping subspaces (e.g., two planes intersecting in a line).\nBack substitution describes the exact shape of this intersection.\n\n\n\nExample: Parametric Vector Form\nFor the infinite-solution example above:\n\\[\n(x, y, z) = (2, 1, 0) + t(-3, 1, 1).\n\\]\nThis expresses the solution set as a base point plus a direction vector, making the geometry clear.\n\n\nWhy It Matters\n\nBack substitution turns row-echelon form into concrete answers.\nIt distinguishes unique vs. infinite solutions.\nIt provides a systematic method usable by hand for small systems and forms the basis of computer algorithms for large ones.\nIt reveals the structure of solution sets-whether a point, line, plane, or higher-dimensional object.\n\n\n\nTry It Yourself\n\nSolve by back substitution:\n\n\\[\n\\begin{bmatrix}  \n1 & -1 & 2 & | & 3 \\\\  \n0 & 1 & 3 & | & 5 \\\\  \n0 & 0 & 1 & | & 2  \n\\end{bmatrix}.\n\\]\n\nReduce and solve:\n\n\\[\nx + y + z = 2, \\quad 2x + 2y + 2z = 4.\n\\]\n\nExpress the solution set of the above system in parametric vector form.\nChallenge: For a 4×4 system with two free variables, explain why the solution set forms a plane in \\(\\mathbb{R}^4\\).\n\nBack substitution completes the elimination process, translating triangular structure into explicit solutions, and shows how algebra and geometry meet in the classification of solution sets.\n\n\n\n29. Rank and Its First Meaning\nThe concept of rank lies at the heart of linear algebra. It connects the algebra of solving systems, the geometry of subspaces, and the structure of matrices into one unifying idea. Rank measures the amount of independent information in a matrix: how many rows or columns carry unique directions instead of being repetitions or combinations of others.\n\nDefinition of Rank\nThe rank of a matrix \\(A\\) is the number of pivots in its row-echelon form. Equivalently, it is:\n\nThe dimension of the column space (number of independent columns).\nThe dimension of the row space (number of independent rows).\n\nAll these definitions agree.\n\n\nFirst Encounter with Rank: Pivot Counting\nWhen solving a system with Gaussian elimination:\n\nEvery pivot corresponds to one determined variable.\nThe number of pivots = the rank.\nThe number of free variables = total variables – rank.\n\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 1 & | & 4 \\\\  \n0 & 1 & -1 & | & 2 \\\\  \n0 & 0 & 0 & | & 0  \n\\end{bmatrix}.\n\\]\nHere, there are 2 pivots. So:\n\nRank = 2.\nWith 3 variables total, there is 1 free variable.\n\n\n\nRank in Terms of Independence\nA set of vectors is linearly independent if none can be expressed as a combination of the others.\n\nThe rank of a matrix tells us how many independent rows or columns it has.\nIf some columns are combinations of others, they do not increase the rank.\n\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n2 & 4 & 6 \\\\  \n3 & 6 & 9  \n\\end{bmatrix}.\n\\]\nHere, each row is a multiple of the first. Rank = 1, since only one independent row/column direction exists.\n\n\nRank and Solutions of Systems\nConsider \\(A\\mathbf{x} = \\mathbf{b}\\).\n\nIf \\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}])\\), the system is consistent.\nIf not, inconsistent.\nIf rank = number of variables, the system has a unique solution.\nIf rank &lt; number of variables, there are infinitely many solutions.\n\nThus, rank classifies solution sets.\n\n\nRank and Geometry\nRank tells us the dimension of the subspace spanned by rows or columns.\n\nRank 1: all information lies along a line.\nRank 2: lies in a plane.\nRank 3: fills 3D space.\n\nExample:\n\nIn \\(\\mathbb{R}^3\\), a matrix of rank 2 has columns spanning a plane through the origin.\nA matrix of rank 1 has all columns on a single line.\n\n\n\nRank and Row vs. Column View\nIt is a remarkable fact that the number of independent rows = number of independent columns. This is not obvious at first glance, but it is always true. So we can define rank either by rows or by columns-it makes no difference.\n\n\nWhy It Matters\n\nRank is the bridge between algebra and geometry: pivots ↔︎ dimension.\nIt classifies solutions to systems of equations.\nIt measures redundancy in data (important in statistics, machine learning, signal processing).\nIt prepares the way for advanced concepts like nullity, rank–nullity theorem, and singular value decomposition.\n\n\n\nTry It Yourself\n\nFind the rank of\n\\[\n\\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n2 & 4 & 5 \\\\  \n3 & 6 & 8  \n\\end{bmatrix}.\n\\]\nSolve the system\n\\[\nx + y + z = 2, \\quad 2x + 2y + 2z = 4,\n\\]\nand identify the rank of the coefficient matrix.\nIn \\(\\mathbb{R}^3\\), what is the geometric meaning of a 3×3 matrix of rank 2?\nChallenge: Prove that the row rank always equals the column rank by considering the echelon form of the matrix.\n\nRank is the first truly unifying concept in linear algebra: it tells us how much independent structure a matrix contains and sets the stage for understanding spaces, dimensions, and transformations.\n\n\n\n30. LU Factorization\nGaussian elimination not only solves systems but also reveals a deeper structure: many matrices can be factored into simpler pieces. One of the most useful is the LU factorization, where a matrix \\(A\\) is written as the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\). This factorization captures all the elimination steps in a compact form and allows systems to be solved efficiently.\n\nWhat is LU Factorization?\nIf \\(A\\) is an \\(n \\times n\\) matrix, then\n\\[\nA = LU,\n\\]\nwhere:\n\n\\(L\\) is lower-triangular (entries below diagonal may be nonzero, diagonal entries = 1).\n\\(U\\) is upper-triangular (entries above diagonal may be nonzero).\n\nThis means:\n\n\\(U\\) stores the result of elimination (the triangular system).\n\\(L\\) records the multipliers used during elimination.\n\n\n\nExample: 2×2 Case\nTake\n\\[\nA = \\begin{bmatrix}  \n2 & 3 \\\\  \n4 & 7  \n\\end{bmatrix}.\n\\]\nElimination: \\(R_2 \\to R_2 - 2R_1\\).\n\nMultiplier = 2 (used to eliminate entry 4).\nResulting \\(U\\):\n\\[\nU = \\begin{bmatrix}  \n2 & 3 \\\\  \n0 & 1  \n\\end{bmatrix}.\n\\]\n\\(L\\):\n\\[\nL = \\begin{bmatrix}  \n1 & 0 \\\\  \n2 & 1  \n\\end{bmatrix}.\n\\]\n\nCheck:\n\\[\nLU = \\begin{bmatrix}  \n1 & 0 \\\\  \n2 & 1  \n\\end{bmatrix}  \n\\begin{bmatrix}  \n2 & 3 \\\\  \n0 & 1  \n\\end{bmatrix}  \n= \\begin{bmatrix}  \n2 & 3 \\\\  \n4 & 7  \n\\end{bmatrix} = A.\n\\]\n\n\nExample: 3×3 Case\n\\[\nA = \\begin{bmatrix}  \n2 & 1 & 1 \\\\  \n4 & -6 & 0 \\\\  \n-2 & 7 & 2  \n\\end{bmatrix}.\n\\]\nStep 1: Eliminate below pivot (row 1).\n\nMultiplier \\(m_{21} = 4/2 = 2\\).\nMultiplier \\(m_{31} = -2/2 = -1\\).\n\nStep 2: Eliminate below pivot in column 2.\n\nAfter substitutions, multipliers and pivots are collected.\n\nResult:\n\\[\nL = \\begin{bmatrix}  \n1 & 0 & 0 \\\\  \n2 & 1 & 0 \\\\  \n-1 & -1 & 1  \n\\end{bmatrix}, \\quad  \nU = \\begin{bmatrix}  \n2 & 1 & 1 \\\\  \n0 & -8 & -2 \\\\  \n0 & 0 & 1  \n\\end{bmatrix}.\n\\]\nThus \\(A = LU\\).\n\n\nSolving Systems with LU\nSuppose \\(Ax = b\\). If \\(A = LU\\):\n\nSolve \\(Ly = b\\) by forward substitution (since \\(L\\) is lower-triangular).\nSolve \\(Ux = y\\) by back substitution (since \\(U\\) is upper-triangular).\n\nThis two-step process is much faster than elimination from scratch each time, especially if solving multiple systems with the same \\(A\\) but different \\(b\\).\n\n\nPivoting and Permutations\nSometimes elimination requires row swaps (to avoid division by zero or instability). Then factorization is written as:\n\\[\nPA = LU,\n\\]\nwhere \\(P\\) is a permutation matrix recording the row swaps. This is the practical form used in numerical computing.\n\n\nApplications of LU Factorization\n\nEfficient solving: Multiple right-hand sides \\(Ax = b\\). Compute \\(LU\\) once, reuse for each \\(b\\).\nDeterminants: \\(\\det(A) = \\det(L)\\det(U)\\). Since diagonals of \\(L\\) are 1, this reduces to the product of the diagonal of \\(U\\).\nMatrix inverse: By solving \\(Ax = e_i\\) for each column \\(e_i\\), we can compute \\(A^{-1}\\) efficiently with LU.\nNumerical methods: LU is central in scientific computing, engineering simulations, and optimization.\n\n\n\nGeometric Meaning\nLU decomposition separates the elimination process into:\n\n\\(L\\): shear transformations (adding multiples of rows).\n\\(U\\): scaling and alignment into triangular form.\n\nTogether, they represent the same linear transformation as \\(A\\), but decomposed into simpler building blocks.\n\n\nWhy It Matters\n\nLU factorization compresses elimination into a reusable format.\nIt is a cornerstone of numerical linear algebra and used in almost every solver.\nIt links computation (efficient algorithms) with theory (factorization of transformations).\nIt introduces the broader idea that matrices can be broken into simple, interpretable parts.\n\n\n\nTry It Yourself\n\nFactor\n\\[\nA = \\begin{bmatrix}  \n1 & 2 \\\\  \n3 & 8  \n\\end{bmatrix}\n\\]\ninto \\(LU\\).\nSolve\n\\[\n\\begin{bmatrix}  \n2 & 1 \\\\  \n6 & 3  \n\\end{bmatrix}  \n\\begin{bmatrix}  \nx \\\\ y  \n\\end{bmatrix} =  \n\\begin{bmatrix}  \n5 \\\\ 15  \n\\end{bmatrix}\n\\]\nusing LU decomposition.\nCompute \\(\\det(A)\\) for\n\\[\nA = \\begin{bmatrix}  \n2 & 1 & 1 \\\\  \n4 & -6 & 0 \\\\  \n-2 & 7 & 2  \n\\end{bmatrix}\n\\]\nby using its LU factorization.\nChallenge: Prove that if \\(A\\) is invertible, then it has an LU factorization (possibly after row swaps).\n\nLU factorization organizes elimination into a powerful tool: compact, efficient, and deeply tied to both the theory and practice of linear algebra.\n\n\nClosing\nPaths diverge or merge,\npivots mark the way forward,\ntruth distilled in rows.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-4.-vector-spaces-and-subspaces",
    "href": "books/en-US/book.html#chapter-4.-vector-spaces-and-subspaces",
    "title": "The Book",
    "section": "Chapter 4. Vector spaces and subspaces",
    "text": "Chapter 4. Vector spaces and subspaces\n\nOpening\nEndless skies expand,\nspaces within spaces grow,\nfreedom takes its shape.\n\n\n31. Axioms of Vector Spaces\nUp to now, we have worked with vectors in \\(\\mathbb{R}^2\\), \\(\\mathbb{R}^3\\), and higher-dimensional Euclidean spaces. But the true power of linear algebra comes from abstracting away from coordinates. A vector space is not tied to arrows in physical space-it is any collection of objects that behave like vectors, provided they satisfy certain rules. These rules are called the axioms of vector spaces.\n\nThe Idea of a Vector Space\nA vector space is a set \\(V\\) equipped with two operations:\n\nVector addition: Combine two vectors in \\(V\\) to get another vector in \\(V\\).\nScalar multiplication: Multiply a vector in \\(V\\) by a scalar (a number from a field, usually \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)).\n\nThe magic is that as long as certain rules (axioms) hold, the objects in \\(V\\) can be treated as vectors. They need not be arrows or coordinate lists-they could be polynomials, functions, matrices, or sequences.\n\n\nThe Eight Axioms\nLet \\(u, v, w \\in V\\) (vectors) and \\(a, b \\in \\mathbb{R}\\) (scalars). The axioms are:\n\nClosure under addition: \\(u + v \\in V\\).\nCommutativity of addition: \\(u + v = v + u\\).\nAssociativity of addition: \\((u + v) + w = u + (v + w)\\).\nExistence of additive identity: There exists a zero vector \\(0 \\in V\\) such that \\(v + 0 = v\\).\nExistence of additive inverses: For every \\(v\\), there is \\(-v\\) such that \\(v + (-v) = 0\\).\nClosure under scalar multiplication: \\(a v \\in V\\).\nDistributivity of scalar multiplication over vector addition: \\(a(u + v) = au + av\\).\nDistributivity of scalar multiplication over scalar addition: \\((a + b)v = av + bv\\).\nAssociativity of scalar multiplication: \\(a(bv) = (ab)v\\).\nExistence of multiplicative identity: \\(1 \\cdot v = v\\).\n\n(These are sometimes listed as eight, with some grouped together, but the essence is the same.)\n\n\nExamples of Vector Spaces\n\nEuclidean spaces: \\(\\mathbb{R}^n\\) with standard addition and scalar multiplication.\nPolynomials: The set of all polynomials with real coefficients, \\(\\mathbb{R}[x]\\).\nFunctions: The set of all continuous functions on \\([0,1]\\), with addition of functions and scalar multiplication.\nMatrices: The set of all \\(m \\times n\\) matrices with real entries.\nSequences: The set of all infinite real sequences \\((a_1, a_2, \\dots)\\).\n\nAll of these satisfy the vector space axioms.\n\n\nNon-Examples\n\nThe set of natural numbers \\(\\mathbb{N}\\) is not a vector space (no additive inverses).\nThe set of positive real numbers \\(\\mathbb{R}^+\\) is not a vector space (not closed under scalar multiplication with negative numbers).\nThe set of polynomials of degree exactly 2 is not a vector space (not closed under addition: \\(x^2 + x^2 = 2x^2\\) is still degree 2, but \\(x^2 - x^2 = 0\\), which is degree 0, not allowed).\n\nThese examples show why the axioms are essential: without them, the structure breaks.\n\n\nThe Zero Vector\nEvery vector space must contain a zero vector. This is not optional. It is the “do nothing” element for addition. In \\(\\mathbb{R}^n\\), this is \\((0,0,\\dots,0)\\). In polynomials, it is the zero polynomial. In function spaces, it is the function \\(f(x) = 0\\).\n\n\nAdditive Inverses\nFor every vector \\(v\\), we require \\(-v\\). This ensures that equations like \\(u+v=w\\) can always be rearranged to \\(u=w-v\\). Without additive inverses, solving linear equations would not work.\n\n\nScalars and Fields\nScalars come from a field: usually the real numbers \\(\\mathbb{R}\\) or the complex numbers \\(\\mathbb{C}\\). The choice of scalars matters:\n\nOver \\(\\mathbb{R}\\), a polynomial space is different from over \\(\\mathbb{C}\\).\nOver finite fields (like integers modulo \\(p\\)), vector spaces exist in discrete mathematics and coding theory.\n\n\n\nGeometric Interpretation\n\nThe axioms guarantee that vectors can be added and scaled in predictable ways.\nClosure ensures the space is “self-contained.”\nAdditive inverses ensure symmetry: every direction can be reversed.\nDistributivity ensures consistency between scaling and addition.\n\nTogether, these rules make vector spaces stable and reliable mathematical objects.\n\n\nWhy It Matters\n\nVector spaces unify many areas of math under a single framework.\nThey generalize \\(\\mathbb{R}^n\\) to functions, polynomials, and beyond.\nThe axioms guarantee that all the tools of linear algebra-span, basis, dimension, linear maps-apply.\nRecognizing vector spaces in disguise is a major step in advanced math and physics.\n\n\n\nTry It Yourself\n\nVerify that the set of all 2×2 matrices is a vector space under matrix addition and scalar multiplication.\nShow that the set of polynomials of degree at most 3 is a vector space, but the set of polynomials of degree exactly 3 is not.\nCheck whether the set of all even functions \\(f(-x) = f(x)\\) is a vector space.\nChallenge: Consider the set of all differentiable functions \\(f\\) on \\([0,1]\\). Show that this set forms a vector space under the usual operations.\n\nThe axioms of vector spaces provide the foundation on which the rest of linear algebra is built. Everything that follows-subspaces, independence, basis, dimension-grows naturally from this formal framework.\n\n\n\n32. Subspaces, Column Space, and Null Space\nOnce the idea of a vector space is in place, the next step is to recognize smaller vector spaces that live inside bigger ones. These are called subspaces. Subspaces are central in linear algebra because they reveal the internal structure of matrices and linear systems. Two special subspaces-the column space and the null space-play particularly important roles.\n\nWhat Is a Subspace?\nA subspace \\(W\\) of a vector space \\(V\\) is a subset of \\(V\\) that is itself a vector space under the same operations. To qualify as a subspace, \\(W\\) must satisfy:\n\nThe zero vector \\(0\\) is in \\(W\\).\nIf \\(u, v \\in W\\), then \\(u+v \\in W\\) (closed under addition).\nIf \\(u \\in W\\) and \\(c\\) is a scalar, then \\(cu \\in W\\) (closed under scalar multiplication).\n\nThat’s it-no further checking of all ten vector space axioms is needed, because those are inherited from \\(V\\).\n\n\nSimple Examples of Subspaces\n\nIn \\(\\mathbb{R}^3\\):\n\nA line through the origin is a 1-dimensional subspace.\nA plane through the origin is a 2-dimensional subspace.\nThe whole space itself is a subspace.\nThe trivial subspace \\(\\{0\\}\\) contains only the zero vector.\n\nIn the space of polynomials:\n\nAll polynomials of degree ≤ 3 form a subspace.\nAll polynomials with zero constant term form a subspace.\n\nIn function spaces:\n\nAll continuous functions on \\([0,1]\\) form a subspace of all functions on \\([0,1]\\).\nAll solutions to a linear differential equation form a subspace.\n\n\n\n\nThe Column Space of a Matrix\nGiven a matrix \\(A\\), the column space is the set of all linear combinations of its columns. Formally,\n\\[\nC(A) = \\{ A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{R}^n \\}.\n\\]\n\nThe column space lives inside \\(\\mathbb{R}^m\\) if \\(A\\) is \\(m \\times n\\).\nIt represents all possible outputs of the linear transformation defined by \\(A\\).\nIts dimension is equal to the rank of \\(A\\).\n\nExample:\n\\[\nA = \\begin{bmatrix}  \n1 & 2 \\\\  \n2 & 4 \\\\  \n3 & 6  \n\\end{bmatrix}.\n\\]\nThe second column is just twice the first. So the column space is all multiples of \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}\\), which is a line in \\(\\mathbb{R}^3\\). Rank = 1.\n\n\nThe Null Space of a Matrix\nThe null space (or kernel) of a matrix \\(A\\) is the set of all vectors \\(\\mathbf{x}\\) such that\n\\[\nA\\mathbf{x} = 0.\n\\]\n\nIt lives in \\(\\mathbb{R}^n\\) if \\(A\\) is \\(m \\times n\\).\nIt represents the “invisible” directions that collapse to zero under the transformation.\nIts dimension is the nullity of \\(A\\).\n\nExample:\n\\[\nA = \\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n4 & 5 & 6  \n\\end{bmatrix}.\n\\]\nSolve \\(A\\mathbf{x} = 0\\). This yields a null space spanned by one vector, meaning it is a line through the origin in \\(\\mathbb{R}^3\\).\n\n\nColumn Space vs. Null Space\n\nColumn space: describes outputs (\\(y\\)-values that can be reached).\nNull space: describes hidden inputs (directions that vanish).\n\nTogether, they capture the full behavior of a matrix.\n\n\nGeometric Interpretation\n\nIn \\(\\mathbb{R}^3\\), the column space could be a plane or a line inside 3D space.\nThe null space is orthogonal (in a precise sense) to the row space, which we’ll study later.\nUnderstanding both spaces gives a complete picture of how the matrix transforms vectors.\n\n\n\nWhy It Matters\n\nSubspaces are the natural habitat of linear algebra: almost everything happens inside them.\nThe column space explains what systems \\(Ax=b\\) are solvable.\nThe null space explains why some systems have multiple solutions (free variables).\nThese ideas extend to advanced topics like eigenvectors, SVD, and differential equations.\n\n\n\nTry It Yourself\n\nShow that the set \\(\\{(x,y,0) : x,y \\in \\mathbb{R}\\}\\) is a subspace of \\(\\mathbb{R}^3\\).\nFor\n\\[\nA = \\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n0 & 0 & 0 \\\\  \n1 & 2 & 3  \n\\end{bmatrix},\n\\]\nfind the column space and its dimension.\nFor the same \\(A\\), compute the null space and its dimension.\nChallenge: Prove that the null space of \\(A\\) is always a subspace of \\(\\mathbb{R}^n\\).\n\nSubspaces-especially the column space and null space-are the first glimpse of the hidden geometry inside every matrix, showing us which directions survive and which vanish.\n\n\n\n33. Span and Generating Sets\nThe idea of a span captures the simplest and most powerful way to build new vectors from old ones: by taking linear combinations. A span is not just a set of scattered points but a structured, complete collection of all combinations of a given set of vectors. Understanding span leads directly to the concepts of bases, dimension, and the structure of subspaces.\n\nDefinition of Span\nGiven vectors \\(v_1, v_2, \\dots, v_k \\in V\\), the span of these vectors is\n\\[\n\\text{span}\\{v_1, v_2, \\dots, v_k\\} = \\{a_1 v_1 + a_2 v_2 + \\dots + a_k v_k : a_i \\in \\mathbb{R}\\}.\n\\]\n\nA span is the set of all possible linear combinations of the vectors.\nIt is always a subspace.\nThe given vectors are called a generating set.\n\n\n\nSimple Examples\n\nIn \\(\\mathbb{R}^2\\):\n\nSpan of \\((1,0)\\) = all multiples of the x-axis (a line).\nSpan of \\((1,0)\\) and \\((0,1)\\) = the entire plane \\(\\mathbb{R}^2\\).\nSpan of \\((1,0)\\) and \\((2,0)\\) = still the x-axis, since the second vector is redundant.\n\nIn \\(\\mathbb{R}^3\\):\n\nSpan of a single vector = a line.\nSpan of two independent vectors = a plane through the origin.\nSpan of three independent vectors = the whole space \\(\\mathbb{R}^3\\).\n\n\n\n\nSpan as Coverage\n\nIf you think of vectors as “directions,” the span is everything you can reach by walking in those directions, with any step lengths (scalars) allowed.\nIf you only have one direction, you can walk back and forth on a line.\nWith two independent directions, you can sweep out a plane.\nWith three independent directions in 3D, you can move anywhere.\n\n\n\nGenerating Sets\nA set of vectors is a generating set (or spanning set) for a subspace if their span equals that subspace.\n\nExample: \\(\\{(1,0), (0,1)\\}\\) generates \\(\\mathbb{R}^2\\).\nExample: \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\) generates \\(\\mathbb{R}^3\\).\nExample: The columns of a matrix generate its column space.\n\nDifferent generating sets can span the same space. Some may be redundant, others minimal. Later, the concept of a basis refines this idea.\n\n\nRedundancy in Spanning Sets\n\nIf one vector is a linear combination of others, it does not enlarge the span.\nExample: In \\(\\mathbb{R}^2\\), \\(\\{(1,0), (0,1), (1,1)\\}\\) spans the same space as \\(\\{(1,0), (0,1)\\}\\).\nEliminating redundancy leads to a more efficient generating set.\n\n\n\nSpan and Linear Systems\nConsider the system \\(Ax=b\\).\n\nThe question “Is there a solution?” is equivalent to “Is \\(b\\) in the span of the columns of \\(A\\)?”\nThus, span provides the geometric language for solvability.\n\n\n\nWhy It Matters\n\nSpan is the foundation for defining subspaces generated by vectors.\nIt connects directly to solvability of linear equations.\nIt introduces the notion of redundancy, preparing for bases and independence.\nIt generalizes naturally to function spaces and abstract vector spaces.\n\n\n\nTry It Yourself\n\nFind the span of \\(\\{(1,2), (2,4)\\}\\) in \\(\\mathbb{R}^2\\).\nShow that the vectors \\((1,0,1), (0,1,1), (1,1,2)\\) span only a plane in \\(\\mathbb{R}^3\\).\nDecide whether \\((1,2,3)\\) is in the span of \\((1,0,1)\\) and \\((0,1,2)\\).\nChallenge: Prove that the set of all polynomials \\(\\{1, x, x^2, \\dots\\}\\) spans the space of all polynomials.\n\nThe concept of span transforms our perspective: instead of focusing on single vectors, we see the entire landscape of possibilities they generate.\n\n\n\n34. Linear Independence and Dependence\nHaving introduced span and generating sets, the natural question arises: when are the vectors in a spanning set truly necessary, and when are some redundant? This leads to the idea of linear independence. It is the precise way to distinguish between essential vectors (those that add new directions) and dependent vectors (those that can be expressed in terms of others).\n\nDefinition of Linear Independence\nA set of vectors \\(\\{v_1, v_2, \\dots, v_k\\}\\) is linearly independent if the only solution to\n\\[\na_1 v_1 + a_2 v_2 + \\dots + a_k v_k = 0\n\\]\nis\n\\[\na_1 = a_2 = \\dots = a_k = 0.\n\\]\nIf there exists a nontrivial solution (some \\(a_i \\neq 0\\)), then the vectors are linearly dependent.\n\n\nIntuition\n\nIndependent vectors point in genuinely different directions.\nDependent vectors overlap: at least one can be built from the others.\nIn terms of span: removing a dependent vector does not shrink the span, because it adds no new direction.\n\n\n\nSimple Examples in \\(\\mathbb{R}^2\\)\n\n\\((1,0)\\) and \\((0,1)\\) are independent.\n\nEquation \\(a(1,0) + b(0,1) = (0,0)\\) forces \\(a = b = 0\\).\n\n\\((1,0)\\) and \\((2,0)\\) are dependent.\n\nEquation \\(2(1,0) - (2,0) = (0,0)\\) shows dependence.\n\nAny set of 3 vectors in \\(\\mathbb{R}^2\\) is dependent, since the dimension of the space is 2.\n\n\n\nExamples in \\(\\mathbb{R}^3\\)\n\n\\((1,0,0), (0,1,0), (0,0,1)\\) are independent.\n\\((1,2,3), (2,4,6)\\) are dependent, since the second is just 2× the first.\n\\((1,0,1), (0,1,1), (1,1,2)\\) are dependent: the third is the sum of the first two.\n\n\n\nDetecting Independence with Matrices\nPut the vectors as columns in a matrix. Perform row reduction:\n\nIf every column has a pivot → the set is independent.\nIf some column is free → the set is dependent.\n\nExample:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n0 & 1 & 4 \\\\  \n0 & 0 & 0  \n\\end{bmatrix}.\n\\]\nHere the third column has no pivot → the 3rd vector is dependent on the first two.\n\n\nRelationship with Dimension\n\nIn \\(\\mathbb{R}^n\\), at most \\(n\\) independent vectors exist.\nIf you have more than \\(n\\), dependence is guaranteed.\nA basis of a vector space is simply a maximal independent set that spans the space.\n\n\n\nGeometric Interpretation\n\nIndependent vectors = different directions.\nDependent vectors = one vector lies in the span of others.\nIn 2D: two independent vectors span the plane.\nIn 3D: three independent vectors span the space.\n\n\n\nWhy It Matters\n\nIndependence ensures a generating set is minimal and efficient.\nIt determines whether a system of vectors is a basis.\nIt connects directly to rank: rank = number of independent columns (or rows).\nIt is crucial in geometry, data compression, and machine learning-where redundancy must be identified and removed.\n\n\n\nTry It Yourself\n\nTest whether \\((1,2)\\) and \\((2,4)\\) are independent.\nAre the vectors \\((1,0,0), (0,1,0), (1,1,0)\\) independent in \\(\\mathbb{R}^3\\)?\nPlace the vectors \\((1,0,1), (0,1,1), (1,1,2)\\) into a matrix and row-reduce to check independence.\nChallenge: Prove that any set of \\(n+1\\) vectors in \\(\\mathbb{R}^n\\) is linearly dependent.\n\nLinear independence is the tool that separates essential directions from redundant ones. It is the key to defining bases, counting dimensions, and understanding the structure of all vector spaces.\n\n\n\n35. Basis and Coordinates\nThe concepts of span and linear independence come together in the powerful idea of a basis. A basis gives us the minimal set of building blocks needed to generate an entire vector space, with no redundancy. Once a basis is chosen, every vector in the space can be described uniquely by a list of numbers called its coordinates.\n\nWhat Is a Basis?\nA basis of a vector space \\(V\\) is a set of vectors \\(\\{v_1, v_2, \\dots, v_k\\}\\) that satisfies two properties:\n\nSpanning property: \\(\\text{span}\\{v_1, \\dots, v_k\\} = V\\).\nIndependence property: The vectors are linearly independent.\n\nIn short: a basis is a spanning set with no redundancy.\n\n\nExample: Standard Bases\n\nIn \\(\\mathbb{R}^2\\), the standard basis is \\(\\{(1,0), (0,1)\\}\\).\nIn \\(\\mathbb{R}^3\\), the standard basis is \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\).\nIn \\(\\mathbb{R}^n\\), the standard basis is the collection of unit vectors, each with a 1 in one position and 0 elsewhere.\n\nThese are called standard because they are the default way of describing coordinates.\n\n\nUniqueness of Coordinates\nOne of the most important facts about bases is that they provide unique representations of vectors.\n\nGiven a basis \\(\\{v_1, \\dots, v_k\\}\\), any vector \\(x \\in V\\) can be written uniquely as:\n\\[\nx = a_1 v_1 + a_2 v_2 + \\dots + a_k v_k.\n\\]\nThe coefficients \\((a_1, a_2, \\dots, a_k)\\) are the coordinates of \\(x\\) relative to that basis.\n\nThis uniqueness distinguishes bases from arbitrary spanning sets, where redundancy allows multiple representations.\n\n\nExample in \\(\\mathbb{R}^2\\)\nLet basis = \\(\\{(1,0), (0,1)\\}\\).\n\nVector \\((3,5) = 3(1,0) + 5(0,1)\\).\nCoordinates relative to this basis: \\((3,5)\\).\n\nIf we switch to a different basis, the coordinates change even though the vector itself does not.\n\n\nExample with Non-Standard Basis\nBasis = \\(\\{(1,1), (1,-1)\\}\\) in \\(\\mathbb{R}^2\\). Find coordinates of \\(x = (2,0)\\).\nSolve \\(a(1,1) + b(1,-1) = (2,0)\\). This gives system:\n\\[\na + b = 2, \\quad a - b = 0.\n\\]\nSo \\(a=1, b=1\\). Coordinates relative to this basis: \\((1,1)\\).\nNotice: coordinates depend on basis choice.\n\n\nBasis of Function Spaces\n\nFor polynomials of degree ≤ 2: basis = \\(\\{1, x, x^2\\}\\).\n\nExample: \\(2 + 3x + 5x^2\\) has coordinates \\((2,3,5)\\).\n\nFor continuous functions on \\([0,1]\\), one possible basis is the infinite set \\(\\{1, x, x^2, \\dots\\}\\).\n\nThis shows bases are not restricted to geometric vectors.\n\n\nDimension\nThe number of vectors in a basis is the dimension of the vector space.\n\n\\(\\mathbb{R}^2\\) has dimension 2.\n\\(\\mathbb{R}^3\\) has dimension 3.\nThe space of polynomials of degree ≤ 3 has dimension 4.\n\nDimension tells us how many independent directions exist in the space.\n\n\nChange of Basis\n\nSwitching from one basis to another is like translating between languages.\nThe same vector looks different depending on which “dictionary” (basis) you use.\nChange-of-basis matrices allow systematic translation between coordinate systems.\n\n\n\nGeometric Interpretation\n\nA basis is like setting up coordinate axes in a space.\nIn 2D, two independent vectors define a grid.\nIn 3D, three independent vectors define a full coordinate system.\nDifferent bases = different grids overlaying the same space.\n\n\n\nWhy It Matters\n\nBases provide the simplest possible description of a vector space.\nThey allow us to assign unique coordinates to vectors.\nThey connect the abstract structure of a space with concrete numerical representations.\nThe concept underlies almost all of linear algebra: dimension, transformations, eigenvectors, and more.\n\n\n\nTry It Yourself\n\nShow that \\(\\{(1,2), (3,4)\\}\\) is a basis of \\(\\mathbb{R}^2\\).\nExpress \\((4,5)\\) in terms of basis \\(\\{(1,1), (1,-1)\\}\\).\nProve that no basis of \\(\\mathbb{R}^3\\) can have more than 3 vectors.\nChallenge: Show that the set \\(\\{1, \\cos x, \\sin x\\}\\) is a basis for the space of all linear combinations of \\(1, \\cos x, \\sin x\\).\n\nA basis is the minimal, elegant foundation of a vector space, turning the infinite into the manageable by providing a finite set of independent building blocks.\n\n\n\n36. Dimension\nDimension is one of the most profound and unifying ideas in linear algebra. It gives a single number that captures the “size” or “capacity” of a vector space: how many independent directions it has. Unlike length, width, or height in everyday geometry, dimension in linear algebra applies to spaces of any kind-geometric, algebraic, or even function spaces.\n\nDefinition\nThe dimension of a vector space \\(V\\) is the number of vectors in any basis of \\(V\\).\n\nSince all bases of a vector space have the same number of elements, dimension is well-defined.\nIf \\(\\dim V = n\\), then:\n\nEvery set of more than \\(n\\) vectors in \\(V\\) is dependent.\nEvery set of exactly \\(n\\) independent vectors forms a basis.\n\n\n\n\nExamples in Familiar Spaces\n\n\\(\\dim(\\mathbb{R}^2) = 2\\).\n\nBasis: \\((1,0), (0,1)\\).\nTwo directions cover the whole plane.\n\n\\(\\dim(\\mathbb{R}^3) = 3\\).\n\nBasis: \\((1,0,0), (0,1,0), (0,0,1)\\).\nThree independent directions span 3D space.\n\nThe set of all polynomials of degree ≤ 2 has dimension 3.\n\nBasis: \\(\\{1, x, x^2\\}\\).\n\nThe space of all \\(m \\times n\\) matrices has dimension \\(mn\\).\n\nEach entry is independent, and the standard basis consists of matrices with a single 1 and the rest 0.\n\n\n\n\nFinite vs. Infinite Dimensions\n\nFinite-dimensional spaces: \\(\\mathbb{R}^n\\), polynomials of degree ≤ \\(k\\).\nInfinite-dimensional spaces:\n\nThe space of all polynomials (no degree limit).\nThe space of all continuous functions.\nThese cannot be spanned by a finite set of vectors.\n\n\n\n\nDimension and Subspaces\n\nAny subspace of \\(\\mathbb{R}^n\\) has dimension ≤ \\(n\\).\nA line through the origin in \\(\\mathbb{R}^3\\): dimension 1.\nA plane through the origin in \\(\\mathbb{R}^3\\): dimension 2.\nThe whole space: dimension 3.\nThe trivial subspace \\(\\{0\\}\\): dimension 0.\n\n\n\nDimension and Systems of Equations\nWhen solving \\(A\\mathbf{x} = \\mathbf{b}\\):\n\nThe dimension of the column space = rank = number of independent directions in the outputs.\nThe dimension of the null space = number of free variables.\nBy the rank–nullity theorem:\n\\[\n\\dim(\\text{column space}) + \\dim(\\text{null space}) = \\text{number of variables}.\n\\]\n\n\n\nGeometric Meaning\n\nDimension counts the minimum number of coordinates needed to describe a vector.\nIn \\(\\mathbb{R}^2\\), you need 2 numbers.\nIn \\(\\mathbb{R}^3\\), you need 3 numbers.\nIn the polynomial space of degree ≤ 3, you need 4 coefficients.\n\nThus, dimension = length of coordinate list.\n\n\nChecking Dimension in Practice\n\nPlace candidate vectors as columns of a matrix.\nRow reduce to echelon form.\nCount pivots. That number = dimension of the span of those vectors.\n\n\n\nWhy It Matters\n\nDimension is the most fundamental measure of a vector space.\nIt tells us how “large” or “complex” the space is.\nIt sets absolute limits: in \\(\\mathbb{R}^n\\), no more than \\(n\\) independent vectors exist.\nIt underlies coordinate systems, bases, and transformations.\nIt bridges geometry (lines, planes, volumes) with algebra (solutions, equations, matrices).\n\n\n\nTry It Yourself\n\nWhat is the dimension of the span of \\((1,2,3)\\), \\((2,4,6)\\), \\((0,0,0)\\)?\nFind the dimension of the subspace of \\(\\mathbb{R}^3\\) defined by \\(x+y+z=0\\).\nProve that the set of all \\(2 \\times 2\\) symmetric matrices has dimension 3.\nChallenge: Show that the space of polynomials of degree ≤ \\(k\\) has dimension \\(k+1\\).\n\nDimension is the measuring stick of linear algebra: it tells us how many independent pieces of information are needed to describe the whole space.\n\n\n\n37. Rank–Nullity Theorem\nThe rank–nullity theorem is one of the central results of linear algebra. It gives a precise balance between two fundamental aspects of a matrix: the dimension of its column space (rank) and the dimension of its null space (nullity). It shows that no matter how complicated a matrix looks, the distribution of information between its “visible” outputs and its “hidden” null directions always obeys a strict law.\n\nStatement of the Theorem\nLet \\(A\\) be an \\(m \\times n\\) matrix (mapping \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\)):\n\\[\n\\text{rank}(A) + \\text{nullity}(A) = n\n\\]\nwhere:\n\nrank(A) = dimension of the column space of \\(A\\).\nnullity(A) = dimension of the null space of \\(A\\).\n\\(n\\) = number of columns of \\(A\\), i.e., the number of variables.\n\n\n\nIntuition\nThink of a matrix as a machine that transforms input vectors into outputs:\n\nRank measures how many independent output directions survive.\nNullity measures how many input directions get “lost” (mapped to zero).\nThe theorem says: total inputs = useful directions (rank) + wasted directions (nullity).\n\nThis ensures nothing disappears mysteriously-every input direction is accounted for.\n\n\nExample 1: Full Rank\n\\[\nA = \\begin{bmatrix}  \n1 & 0 \\\\  \n0 & 1 \\\\  \n\\end{bmatrix}.\n\\]\n\nRank = 2 (two independent columns).\nNull space = \\(\\{0\\}\\), so nullity = 0.\nRank + nullity = 2 = number of variables.\n\n\n\nExample 2: Dependent Columns\n\\[\nA = \\begin{bmatrix}  \n1 & 2 \\\\  \n2 & 4 \\\\  \n3 & 6 \\\\  \n\\end{bmatrix}.\n\\]\n\nSecond column is a multiple of the first. Rank = 1.\nNull space contains all vectors \\((x,y)\\) with \\(y = -2x\\). Nullity = 1.\nRank + nullity = 1 + 1 = 2 = number of variables.\n\n\n\nExample 3: Larger System\n\\[\nA = \\begin{bmatrix}  \n1 & 0 & 1 \\\\  \n0 & 1 & 1  \n\\end{bmatrix}.\n\\]\n\nColumns: \\((1,0), (0,1), (1,1)\\).\nOnly two independent columns → Rank = 2.\nNull space: solve \\(x + z = 0, y + z = 0 \\Rightarrow (x,y,z) = (-t,-t,t)\\). Nullity = 1.\nRank + nullity = 2 + 1 = 3 = number of variables.\n\n\n\nProof Sketch (Conceptual)\n\nRow reduce \\(A\\) to echelon form.\nPivots correspond to independent columns → count = rank.\nFree variables correspond to null space directions → count = nullity.\nEach column is either a pivot column or corresponds to a free variable, so:\n\\[\n\\text{rank} + \\text{nullity} = \\text{number of columns}.\n\\]\n\n\n\nGeometric Meaning\n\nIn \\(\\mathbb{R}^3\\), if a transformation collapses all vectors onto a plane (rank = 2), then one direction disappears entirely (nullity = 1).\nIn \\(\\mathbb{R}^4\\), if a matrix has rank 2, then its null space has dimension 2, meaning half the input directions vanish.\n\nThe theorem guarantees the geometry of “surviving” and “vanishing” directions always adds up consistently.\n\n\nApplications\n\nSolving systems \\(Ax = b\\):\n\nRank determines consistency and structure of solutions.\nNullity tells how many free parameters exist in the solution.\n\nData compression: Rank identifies independent features; nullity shows redundancy.\nComputer graphics: Rank–nullity explains how 3D coordinates collapse into 2D images: one dimension of depth is lost.\nMachine learning: Rank signals how much real information a dataset contains; nullity indicates degrees of freedom that add nothing new.\n\n\n\nWhy It Matters\n\nThe rank–nullity theorem connects the abstract ideas of rank and nullity into a single, elegant formula.\nIt ensures conservation of dimension: no information magically appears or disappears.\nIt is essential in understanding solutions of systems, dimensions of subspaces, and the structure of linear transformations.\nIt prepares the ground for deeper results in algebra, topology, and differential equations.\n\n\n\nTry It Yourself\n\nVerify rank–nullity for\n\\[\nA = \\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n4 & 5 & 6  \n\\end{bmatrix}.\n\\]\nFor a \\(4 \\times 5\\) matrix of rank 3, what is its nullity?\nIn \\(\\mathbb{R}^3\\), suppose a matrix maps all of space onto a line. What are its rank and nullity?\nChallenge: Prove rigorously that the row space and null space are orthogonal complements, and use this to derive rank–nullity again.\n\nThe rank–nullity theorem is the law of balance in linear algebra: every input dimension is accounted for, either as a surviving direction (rank) or as one that vanishes (nullity).\n\n\n\n38. Coordinates Relative to a Basis\nOnce a basis for a vector space is chosen, every vector in that space can be described uniquely in terms of the basis. These descriptions are called coordinates. Coordinates transform abstract vectors into concrete lists of numbers, making computation possible. Changing the basis changes the coordinates, but the underlying vector remains the same.\n\nThe Core Idea\nGiven a vector space \\(V\\) and a basis \\(B = \\{v_1, v_2, \\dots, v_n\\}\\), every vector \\(x \\in V\\) can be written uniquely as:\n\\[\nx = a_1 v_1 + a_2 v_2 + \\dots + a_n v_n.\n\\]\nThe coefficients \\((a_1, a_2, \\dots, a_n)\\) are the coordinates of \\(x\\) with respect to the basis \\(B\\).\nThis representation is unique because basis vectors are independent.\n\n\nExample in \\(\\mathbb{R}^2\\)\n\nStandard basis: \\(B = \\{(1,0), (0,1)\\}\\).\n\nVector \\(x = (3,5)\\).\nCoordinates relative to \\(B\\): \\((3,5)\\).\n\nNon-standard basis: \\(B = \\{(1,1), (1,-1)\\}\\).\n\nWrite \\(x = (3,5)\\) as \\(a(1,1) + b(1,-1)\\).\nSolve:\n\\[\na+b = 3, \\quad a-b = 5.\n\\]\nAdding: \\(2a = 8 \\implies a = 4\\). Subtracting: \\(2b = -2 \\implies b = -1\\).\nCoordinates relative to this basis: \\((4, -1)\\).\n\n\nThe same vector looks different depending on the chosen basis.\n\n\nExample in \\(\\mathbb{R}^3\\)\nLet \\(B = \\{(1,0,0), (1,1,0), (1,1,1)\\}\\). Find coordinates of \\(x = (2,3,4)\\).\nSolve \\(a(1,0,0) + b(1,1,0) + c(1,1,1) = (2,3,4)\\). This gives system:\n\\[\na+b+c = 2, \\quad b+c = 3, \\quad c = 4.\n\\]\nFrom \\(c=4\\), we get \\(b+c=3 \\implies b=-1\\). Then \\(a+b+c=2 \\implies a-1+4=2 \\implies a=-1\\). Coordinates: \\((-1, -1, 4)\\).\n\n\nMatrix Formulation\nIf \\(B = \\{v_1, \\dots, v_n\\}\\), form the basis matrix\n\\[\nP = [v_1 \\ v_2 \\ \\dots \\ v_n].\n\\]\nThen for a vector \\(x\\), its coordinate vector \\([x]_B\\) satisfies\n\\[\nP [x]_B = x.\n\\]\nThus,\n\\[\n[x]_B = P^{-1}x.\n\\]\nThis shows coordinate transformation is simply matrix multiplication.\n\n\nChanging Coordinates\nSuppose a vector has coordinates \\([x]_B\\) relative to basis \\(B\\). If we switch to another basis \\(C\\), we use a change-of-basis matrix to convert coordinates:\n\\[\n[x]_C = (P_C^{-1} P_B) [x]_B.\n\\]\nThis process is fundamental in computer graphics, robotics, and data transformations.\n\n\nGeometric Meaning\n\nA basis defines a coordinate system: axes in the space.\nCoordinates are the “addresses” of vectors relative to those axes.\nChanging basis is like rotating or stretching the grid: the address changes, but the point does not.\n\n\n\nWhy It Matters\n\nCoordinates make abstract vectors computable.\nThey allow us to represent functions, polynomials, and geometric objects numerically.\nChanging basis simplifies problems-e.g., diagonalization makes matrices easy to analyze.\nThey connect the abstract (spaces, bases) with the concrete (numbers, matrices).\n\n\n\nTry It Yourself\n\nExpress \\(x=(4,2)\\) relative to basis \\(\\{(1,1),(1,-1)\\}\\).\nFind coordinates of \\(x=(2,1,3)\\) relative to basis \\(\\{(1,0,1),(0,1,1),(1,1,0)\\}\\).\nIf basis \\(B\\) is the standard basis and basis \\(C=\\{(1,1),(1,-1)\\}\\), compute the change-of-basis matrix from \\(B\\) to \\(C\\).\nChallenge: Show that if \\(P\\) is invertible, its columns form a basis, and explain why this guarantees uniqueness of coordinates.\n\nCoordinates relative to a basis are the bridge between geometry and algebra: they turn abstract spaces into numerical systems where computation, reasoning, and transformation become systematic and precise.\n\n\n\n39. Change-of-Basis Matrices\nEvery vector space allows multiple choices of basis, and each basis provides a different way of describing the same vectors. The process of moving from one basis to another is called a change of basis. To perform this change systematically, we use a change-of-basis matrix. This matrix acts as a translator between coordinate systems: it converts the coordinates of a vector relative to one basis into coordinates relative to another.\n\nWhy Change Bases?\n\nSimplicity of computation: Some problems are easier in certain bases. For example, diagonalizing a matrix allows us to raise it to powers more easily.\nGeometry: Different bases can represent rotated or scaled coordinate systems.\nApplications: In physics, computer graphics, robotics, and data science, changing bases is equivalent to switching perspectives or reference frames.\n\n\n\nThe Basic Setup\nLet \\(V\\) be a vector space with two bases:\n\n\\(B = \\{b_1, b_2, \\dots, b_n\\}\\)\n\\(C = \\{c_1, c_2, \\dots, c_n\\}\\)\n\nSuppose a vector \\(x \\in V\\) has coordinates \\([x]_B\\) relative to \\(B\\), and \\([x]_C\\) relative to \\(C\\).\nWe want a matrix \\(P_{B \\to C}\\) such that:\n\\[\n[x]_C = P_{B \\to C} [x]_B.\n\\]\nThis matrix \\(P_{B \\to C}\\) is the change-of-basis matrix from \\(B\\) to \\(C\\).\n\n\nConstructing the Change-of-Basis Matrix\n\nWrite each vector in the basis \\(B\\) in terms of the basis \\(C\\).\nPlace these coordinate vectors as the columns of a matrix.\nThe resulting matrix converts coordinates from \\(B\\) to \\(C\\).\n\nIn matrix form:\n\\[\nP_{B \\to C} = \\big[ [b_1]_C \\ [b_2]_C \\ \\dots \\ [b_n]_C \\big].\n\\]\n\n\nExample in \\(\\mathbb{R}^2\\)\nLet\n\n\\(B = \\{(1,0), (0,1)\\}\\) (standard basis).\n\\(C = \\{(1,1), (1,-1)\\}\\).\n\nTo build \\(P_{B \\to C}\\):\n\nExpress each vector of \\(B\\) in terms of \\(C\\).\n\nSolve:\n\\[\n(1,0) = a(1,1) + b(1,-1).\n\\]\nThis gives system:\n\\[\na+b=1, \\quad a-b=0.\n\\]\nSolution: \\(a=\\tfrac{1}{2}, b=\\tfrac{1}{2}\\). So \\((1,0) = \\tfrac{1}{2}(1,1) + \\tfrac{1}{2}(1,-1)\\).\nNext:\n\\[\n(0,1) = a(1,1) + b(1,-1).\n\\]\nSystem:\n\\[\na+b=0, \\quad a-b=1.\n\\]\nSolution: \\(a=\\tfrac{1}{2}, b=-\\tfrac{1}{2}\\).\nThus:\n\\[\nP_{B \\to C} = \\begin{bmatrix}  \n\\tfrac{1}{2} & \\tfrac{1}{2} \\\\  \n\\tfrac{1}{2} & -\\tfrac{1}{2}  \n\\end{bmatrix}.\n\\]\nSo for any vector \\(x\\),\n\\[\n[x]_C = P_{B \\to C}[x]_B.\n\\]\n\n\nInverse Change of Basis\nIf \\(P_{B \\to C}\\) is the change-of-basis matrix from \\(B\\) to \\(C\\), then its inverse is the change-of-basis matrix in the opposite direction:\n\\[\nP_{C \\to B} = (P_{B \\to C})^{-1}.\n\\]\nThis makes sense: translating back and forth between languages should undo itself.\n\n\nGeneral Formula with Basis Matrices\nLet\n\\[\nP_B = [b_1 \\ b_2 \\ \\dots \\ b_n], \\quad P_C = [c_1 \\ c_2 \\ \\dots \\ c_n],\n\\]\nthe matrices whose columns are basis vectors written in standard coordinates.\nThen the change-of-basis matrix from \\(B\\) to \\(C\\) is:\n\\[\nP_{B \\to C} = P_C^{-1} P_B.\n\\]\nThis formula is extremely useful because it reduces the problem to matrix multiplication.\n\n\nGeometric Interpretation\n\nChanging basis is like rotating or stretching the grid lines of a coordinate system.\nThe vector itself (the point in space) does not move. What changes is its description in terms of the new grid.\nThe change-of-basis matrix is the tool that translates between these descriptions.\n\n\n\nApplications\n\nDiagonalization: Expressing a matrix in a basis of its eigenvectors makes it diagonal, simplifying analysis.\nComputer graphics: Changing camera viewpoints requires change-of-basis matrices.\nRobotics: Coordinate transformations connect robot arms, joints, and workspace frames.\nData science: PCA finds a new basis (principal components) where data is easier to analyze.\n\n\n\nWhy It Matters\n\nProvides a universal method to translate coordinates between bases.\nMakes abstract transformations concrete and computable.\nForms the backbone of diagonalization, Jordan form, and the spectral theorem.\nConnects algebraic manipulations with geometry and real-world reference frames.\n\n\n\nTry It Yourself\n\nCompute the change-of-basis matrix from the standard basis to \\(\\{(2,1),(1,1)\\}\\) in \\(\\mathbb{R}^2\\).\nFind the change-of-basis matrix from basis \\(\\{(1,0,0),(0,1,0),(0,0,1)\\}\\) to \\(\\{(1,1,0),(0,1,1),(1,0,1)\\}\\) in \\(\\mathbb{R}^3\\).\nShow that applying \\(P_{B \\to C}\\) then \\(P_{C \\to B}\\) returns the original coordinates.\nChallenge: Derive the formula \\(P_{B \\to C} = P_C^{-1} P_B\\) starting from the definition of coordinates.\n\nChange-of-basis matrices give us the precise mechanism for switching perspectives. They ensure that although bases change, vectors remain invariant, and computations remain consistent.\n\n\n\n40. Affine Subspaces\nSo far, vector spaces and subspaces have always passed through the origin. But in many real-world situations, we deal with shifted versions of these spaces: planes not passing through the origin, lines offset from the zero vector, or solution sets to linear equations with nonzero constants. These structures are called affine subspaces. They extend the idea of subspaces by allowing “translation away from the origin.”\n\nDefinition\nAn affine subspace of a vector space \\(V\\) is a set of the form\n\\[\nx_0 + W = \\{x_0 + w : w \\in W\\},\n\\]\nwhere:\n\n\\(x_0 \\in V\\) is a fixed vector (the “base point” or “anchor”),\n\\(W \\subseteq V\\) is a linear subspace.\n\nThus, an affine subspace is simply a subspace shifted by a vector.\n\n\nExamples in \\(\\mathbb{R}^2\\)\n\nA line through the origin: \\(\\text{span}\\{(1,2)\\}\\). This is a subspace.\nA line not through the origin: \\((3,1) + \\text{span}\\{(1,2)\\}\\). This is an affine subspace.\nThe entire plane: \\(\\mathbb{R}^2\\), which is both a subspace and an affine subspace.\n\n\n\nExamples in \\(\\mathbb{R}^3\\)\n\nPlane through the origin: \\(\\text{span}\\{(1,0,0),(0,1,0)\\}\\).\nPlane not through the origin: \\((2,3,4) + \\text{span}\\{(1,0,0),(0,1,0)\\}\\).\nLine parallel to the z-axis but passing through \\((1,1,5)\\): \\((1,1,5) + \\text{span}\\{(0,0,1)\\}\\).\n\n\n\nRelation to Linear Systems\nAffine subspaces naturally arise as solution sets of linear equations.\n\nHomogeneous system: \\(Ax = 0\\).\n\nSolution set is a subspace (the null space).\n\nNon-homogeneous system: \\(Ax = b\\) with \\(b \\neq 0\\).\n\nSolution set is affine.\nIf \\(x_p\\) is one particular solution, then the general solution is:\n\\[\nx = x_p + N(A),\n\\]\nwhere \\(N(A)\\) is the null space.\n\n\nThus, the geometry of solving equations leads naturally to affine subspaces.\n\n\nAffine Dimension\nThe dimension of an affine subspace is defined as the dimension of its direction subspace \\(W\\).\n\nA point: affine subspace of dimension 0.\nA line: dimension 1.\nA plane: dimension 2.\nHigher analogs continue in \\(\\mathbb{R}^n\\).\n\n\n\nDifference Between Subspaces and Affine Subspaces\n\nSubspaces always contain the origin.\nAffine subspaces may or may not pass through the origin.\nEvery subspace is an affine subspace (with base point \\(x_0 = 0\\)).\n\n\n\nGeometric Intuition\nThink of affine subspaces as “flat sheets” floating in space:\n\nA line through the origin is a rope tied at the center.\nA line parallel to it but offset is the same rope moved to the side.\nAffine subspaces preserve shape and direction, but not position.\n\n\n\nApplications\n\nLinear equations: General solutions are affine subspaces.\nOptimization: Feasible regions in linear programming are affine subspaces (intersected with inequalities).\nComputer graphics: Affine transformations map affine subspaces to affine subspaces, preserving straightness and parallelism.\nMachine learning: Affine decision boundaries (like hyperplanes) separate data into classes.\n\n\n\nWhy It Matters\n\nAffine subspaces generalize subspaces, making linear algebra more flexible.\nThey allow us to describe solution sets that don’t include the origin.\nThey provide the geometric foundation for affine geometry, computer graphics, and optimization.\nThey serve as the bridge from pure linear algebra to applied modeling.\n\n\n\nTry It Yourself\n\nShow that the set of solutions to\n\\[\nx+y+z=1\n\\]\nis an affine subspace of \\(\\mathbb{R}^3\\). Identify its dimension.\nFind the general solution to\n\\[\nx+2y=3\n\\]\nand describe it as an affine subspace.\nProve that the intersection of two affine subspaces is either empty or another affine subspace.\nChallenge: Show that every affine subspace can be written uniquely as \\(x_0 + W\\) with \\(W\\) a subspace.\n\nAffine subspaces are the natural setting for most real-world linear problems: they combine the strict structure of subspaces with the freedom of translation, capturing both direction and position.\n\n\nClosing\nEach basis a song,\ndimension counts melodies,\nthe space breathes its form.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-5.-linear-transformation-and-structure",
    "href": "books/en-US/book.html#chapter-5.-linear-transformation-and-structure",
    "title": "The Book",
    "section": "Chapter 5. Linear Transformation and Structure",
    "text": "Chapter 5. Linear Transformation and Structure\n\nOpening\nMaps preserve the line,\nreflections ripple outward,\nmotion kept in frame.\n\n\n41. Linear Transformations\nA linear transformation is the heart of linear algebra. It is the rule that connects two vector spaces in a way that respects their linear structure: addition and scalar multiplication. Instead of thinking of vectors as static objects, linear transformations let us study how vectors move, stretch, rotate, project, or reflect. They give linear algebra its dynamic power and are the bridge between abstract theory and concrete applications.\n\nDefinition\nA function \\(T: V \\to W\\) between vector spaces is called a linear transformation if for all \\(u, v \\in V\\) and scalars \\(a, b \\in \\mathbb{R}\\) (or another field),\n\\[\nT(au + bv) = aT(u) + bT(v).\n\\]\nThis single condition encodes two rules:\n\nAdditivity: \\(T(u+v) = T(u) + T(v)\\).\nHomogeneity: \\(T(av) = aT(v)\\).\n\nIf both are satisfied, the transformation is linear.\n\n\nExamples of Linear Transformations\n\nScaling: \\(T(x) = 3x\\) in \\(\\mathbb{R}\\). Every number is stretched threefold.\nRotation in the plane: \\(T(x,y) = (x\\cos\\theta - y\\sin\\theta, \\, x\\sin\\theta + y\\cos\\theta)\\).\nProjection: Projecting \\((x,y,z)\\) onto the \\(xy\\)-plane: \\(T(x,y,z) = (x,y,0)\\).\nDifferentiation: On the space of polynomials, \\(T(p(x)) = p'(x)\\).\nIntegration: On continuous functions, \\(T(f)(x) = \\int_0^x f(t) \\, dt\\).\n\nAll these are linear because they preserve addition and scaling.\n\n\nNon-Examples\n\n\\(T(x) = x^2\\) is not linear, because \\((x+y)^2 \\neq x^2 + y^2\\).\n\\(T(x,y) = (x+1, y)\\) is not linear, because it fails homogeneity: scaling doesn’t preserve the “+1.”\n\nNonlinear rules break the structure of vector spaces.\n\n\nMatrix Representation\nEvery linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) can be represented by a matrix.\nIf \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\), then there exists an \\(m \\times n\\) matrix \\(A\\) such that:\n\\[\nT(x) = Ax.\n\\]\nThe columns of \\(A\\) are simply \\(T(e_1), T(e_2), \\dots, T(e_n)\\), where \\(e_i\\) are the standard basis vectors.\nExample: Let \\(T(x,y) = (2x+y, x-y)\\).\n\n\\(T(e_1) = T(1,0) = (2,1)\\).\n\\(T(e_2) = T(0,1) = (1,-1)\\). So\n\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & -1 \\end{bmatrix}.\n\\]\nThen \\(T(x,y) = A \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\).\n\n\nProperties of Linear Transformations\n\nThe image of the zero vector is always zero: \\(T(0) = 0\\).\nThe image of a line through the origin is again a line (or collapsed to a point).\nComposition of linear transformations is linear.\nEvery linear transformation preserves the structure of subspaces.\n\n\n\nKernel and Image (Preview)\nFor \\(T: V \\to W\\):\n\nThe kernel (or null space) is all vectors mapped to zero: \\(\\ker T = \\{v \\in V : T(v) = 0\\}\\).\nThe image (or range) is all outputs that can be achieved: \\(\\text{im}(T) = \\{T(v) : v \\in V\\}\\). The rank–nullity theorem applies here:\n\n\\[\n\\dim(\\ker T) + \\dim(\\text{im}(T)) = \\dim(V).\n\\]\n\n\nGeometric Interpretation\nLinear transformations reshape space:\n\nScaling stretches space uniformly in one direction.\nRotation spins space while preserving lengths.\nProjection flattens space onto lower dimensions.\nReflection flips space across a line or plane.\n\nThe key feature: straight lines remain straight, and the origin stays fixed.\n\n\nApplications\n\nComputer graphics: Scaling, rotating, projecting 3D objects onto 2D screens.\nRobotics: Transformations between joint coordinates and workspace positions.\nData science: Linear mappings represent dimensionality reduction and feature extraction.\nDifferential equations: Solutions often involve linear operators acting on function spaces.\nMachine learning: Weight matrices in neural networks are stacked linear transformations, interspersed with nonlinearities.\n\n\n\nWhy It Matters\n\nLinear transformations generalize matrices to any vector space.\nThey unify geometry, algebra, and applications under one concept.\nThey provide the natural framework for studying eigenvalues, eigenvectors, and decompositions.\nThey model countless real-world processes: physical, computational, and abstract.\n\n\n\nTry It Yourself\n\nProve that \\(T(x,y,z) = (x+2y, z, x-y+z)\\) is linear.\nFind the matrix representation of the transformation that reflects vectors in \\(\\mathbb{R}^2\\) across the line \\(y=x\\).\nShow why \\(T(x,y) = (x^2,y)\\) is not linear.\nChallenge: For the differentiation operator \\(D: P_3 \\to P_2\\) on polynomials of degree ≤ 3, find its matrix relative to the basis \\(\\{1,x,x^2,x^3\\}\\) in the domain and \\(\\{1,x,x^2\\}\\) in the codomain.\n\nLinear transformations are the language of linear algebra. They capture the essence of symmetry, motion, and structure in spaces of any kind, making them indispensable for both theory and practice.\n\n\n\n42. Matrix Representation of a Linear Map\nEvery linear transformation can be expressed concretely as a matrix. This is one of the most powerful bridges in mathematics: it translates abstract functional rules into arrays of numbers that can be calculated, manipulated, and visualized.\n\nFrom Abstract Rule to Concrete Numbers\nSuppose \\(T: V \\to W\\) is a linear transformation between two finite-dimensional vector spaces. To represent \\(T\\) as a matrix, we first select bases:\n\n\\(B = \\{v_1, v_2, \\dots, v_n\\}\\) for the domain \\(V\\).\n\\(C = \\{w_1, w_2, \\dots, w_m\\}\\) for the codomain \\(W\\).\n\nFor each basis vector \\(v_j\\), compute \\(T(v_j)\\). Each image \\(T(v_j)\\) is a vector in \\(W\\), so it can be written as a combination of the basis \\(C\\):\n\\[\nT(v_j) = a_{1j}w_1 + a_{2j}w_2 + \\dots + a_{mj}w_m.\n\\]\nThe coefficients \\((a_{1j}, a_{2j}, \\dots, a_{mj})\\) become the j-th column of the matrix representing \\(T\\).\nThus, the matrix of \\(T\\) relative to bases \\(B\\) and \\(C\\) is\n\\[\n[T]_{B \\to C} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}.\n\\]\nThis guarantees that for any vector \\(x\\) in coordinates relative to \\(B\\),\n\\[\n[T(x)]_C = [T]_{B \\to C}[x]_B.\n\\]\n\n\nStandard Basis Case\nWhen both \\(B\\) and \\(C\\) are the standard bases, the process simplifies:\n\nTake \\(T(e_1), T(e_2), \\dots, T(e_n)\\).\nPlace them as columns in a matrix.\n\nThat matrix directly represents \\(T\\).\nExample: Let \\(T(x,y) = (2x+y, x-y)\\).\n\n\\(T(e_1) = (2,1)\\).\n\\(T(e_2) = (1,-1)\\).\n\nSo the standard matrix is\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & -1 \\end{bmatrix}.\n\\]\nFor any vector \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\),\n\\[\nT(x,y) = A \\begin{bmatrix} x \\\\ y \\end{bmatrix}.\n\\]\n\n\nMultiple Perspectives\n\nColumns-as-images: Each column shows where a basis vector goes.\nRow view: Each row encodes how to compute one coordinate of the output.\nOperator view: The matrix acts like a machine: input vector → multiply → output vector.\n\n\n\nGeometric Insight\nMatrices reshape space. In \\(\\mathbb{R}^2\\):\n\nThe first column shows where the x-axis goes.\nThe second column shows where the y-axis goes. The entire grid is determined by these two images.\n\nIn \\(\\mathbb{R}^3\\), the three columns are the images of the unit coordinate directions, defining how the whole space twists, rotates, or compresses.\n\n\nApplications\n\nComputer graphics: Rotations, scaling, and projections are represented by small matrices.\nRobotics: Coordinate changes between joints and workspaces rely on transformation matrices.\nData science: Linear maps such as PCA are implemented with matrices that project data into lower dimensions.\nPhysics: Linear operators like rotations, boosts, and stress tensors are matrix representations.\n\n\n\nWhy It Matters\n\nMatrices are computational tools: we can add, multiply, invert them.\nThey let us use algorithms like Gaussian elimination, LU/QR/SVD to study transformations.\nThey link abstract vector space theory to hands-on numerical calculation.\nThey reveal the structure of transformations at a glance, just by inspecting columns and rows.\n\n\n\nTry It Yourself\n\nFind the matrix for the transformation \\(T(x,y,z) = (x+2y, y+z, x+z)\\) in the standard basis.\nCompute the matrix of \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), where \\(T(x,y) = (x-y, x+y)\\).\nUsing the basis \\(B=\\{(1,1), (1,-1)\\}\\) for \\(\\mathbb{R}^2\\), find the matrix of \\(T(x,y) = (2x, y)\\) relative to \\(B\\).\nChallenge: Show that matrix multiplication corresponds to composition of transformations, i.e. \\([S \\circ T] = [S][T]\\).\n\nMatrix representations are the practical form of linear transformations, turning elegant definitions into something we can compute, visualize, and apply across science and engineering.\n\n\n\n43. Kernel and Image\nEvery linear transformation hides two essential structures: the set of vectors that collapse to zero, and the set of all possible outputs. These are called the kernel and the image. They are the DNA of a linear map, revealing its internal structure, its strengths, and its limitations.\n\nThe Kernel\nThe kernel (or null space) of a linear transformation \\(T: V \\to W\\) is defined as:\n\\[\n\\ker(T) = \\{ v \\in V : T(v) = 0 \\}.\n\\]\n\nIt is the set of all vectors that the transformation sends to the zero vector.\nIt measures how much information is “lost” under the transformation.\nThe kernel is always a subspace of the domain \\(V\\).\n\nExamples:\n\nFor \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(T(x,y) = (x,0)\\).\n\nKernel: all vectors of the form \\((0,y)\\). This is the y-axis.\n\nFor \\(T: \\mathbb{R}^3 \\to \\mathbb{R}^2\\), \\(T(x,y,z) = (x,y)\\).\n\nKernel: all vectors of the form \\((0,0,z)\\). This is the z-axis.\n\n\nThe kernel tells us which directions in the domain vanish under \\(T\\).\n\n\nThe Image\nThe image (or range) of a linear transformation is defined as:\n\\[\n\\text{im}(T) = \\{ T(v) : v \\in V \\}.\n\\]\n\nIt is the set of all vectors that can actually be reached by applying \\(T\\).\nIt describes the “output space” of the transformation.\nThe image is always a subspace of the codomain \\(W\\).\n\nExamples:\n\nFor \\(T(x,y) = (x,0)\\):\n\nImage: all vectors of the form \\((a,0)\\). This is the x-axis.\n\nFor \\(T(x,y,z) = (x+y, y+z)\\):\n\nImage: all of \\(\\mathbb{R}^2\\). Any vector \\((u,v)\\) can be achieved by solving equations for \\((x,y,z)\\).\n\n\n\n\nKernel and Image Together\nThese two subspaces reflect two aspects of \\(T\\):\n\nThe kernel measures the collapse in dimension.\nThe image measures the preserved and transmitted directions.\n\nA central result is the Rank–Nullity Theorem:\n\\[\n\\dim(\\ker T) + \\dim(\\text{im }T) = \\dim(V).\n\\]\n\n\\(\\dim(\\ker T)\\) is the nullity.\n\\(\\dim(\\text{im }T)\\) is the rank.\n\nThis theorem guarantees a perfect balance: the domain splits into lost directions (kernel) and active directions (image).\n\n\nMatrix View\nFor a matrix \\(A\\), the linear map is \\(T(x) = Ax\\).\n\nThe kernel is the solution set of \\(Ax = 0\\).\nThe image is the column space of \\(A\\).\n\nExample:\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 1 \\end{bmatrix}.\n\\]\n\nImage: span of the columns\n\n\\[\n\\text{im}(A) = \\text{span}\\{ (1,0), (2,1), (3,1) \\}.\n\\]\n\nKernel: solve\n\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n\\]\nThis leads to solutions like \\(x=-y-2z\\). So the kernel is 1-dimensional, the image is 2-dimensional, and the domain (3D) splits as \\(1+2=3\\).\n\n\nGeometric Intuition\n\nThe kernel is the set of invisible directions, like shadows disappearing in projection.\nThe image is the set of all shadows that can appear.\nTogether they describe projection, flattening, stretching, or collapsing.\n\nExample: Projecting \\(\\mathbb{R}^3\\) onto the xy-plane:\n\nKernel: the z-axis (all points collapsed to zero height).\nImage: the entire xy-plane (all possible shadows).\n\n\n\nApplications\n\nSolving equations: Kernel describes all solutions to \\(Ax=0\\). Image describes what right-hand sides \\(b\\) make \\(Ax=b\\) solvable.\nData science: Nullity corresponds to redundant features; rank corresponds to useful independent features.\nPhysics: In mechanics, symmetries often form the kernel of a transformation, while observable quantities form the image.\nControl theory: The kernel and image determine controllability and observability of systems.\n\n\n\nWhy It Matters\n\nKernel and image classify transformations into invertible or not.\nThey give a precise language to describe dimension changes.\nThey are the foundation of rank, nullity, and invertibility.\nThey generalize far beyond matrices: to polynomials, functions, operators, and differential equations.\n\n\n\nTry It Yourself\n\nCompute the kernel and image of \\(T(x,y,z) = (x+y, y+z)\\).\nFor the projection \\(T(x,y,z) = (x,y,0)\\), identify kernel and image.\nShow that if the kernel is trivial (\\(\\{0\\}\\)), then the transformation is injective.\nChallenge: Prove the rank–nullity theorem for a \\(3\\times 3\\) matrix by working through examples.\n\nThe kernel and image are the twin lenses through which linear transformations are understood. One tells us what disappears, the other what remains. Together, they give the clearest picture of a transformation’s essence.\n\n\n\n44. Invertibility and Isomorphisms\nLinear transformations come in many forms: some collapse space into lower dimensions, others stretch it, and a special group preserves all information perfectly. These special transformations are invertible, meaning they can be reversed exactly. When two vector spaces are related by such a transformation, we say they are isomorphic-structurally identical, even if they look different on the surface.\n\nInvertibility of Linear Transformations\nA linear transformation \\(T: V \\to W\\) is invertible if there exists another linear transformation \\(S: W \\to V\\) such that:\n\\[\nS \\circ T = I_V \\quad \\text{and} \\quad T \\circ S = I_W,\n\\]\nwhere \\(I_V\\) and \\(I_W\\) are identity maps on \\(V\\) and \\(W\\).\n\n\\(S\\) is called the inverse of \\(T\\).\nIf such an inverse exists, \\(T\\) is a bijection: both one-to-one (injective) and onto (surjective).\nIn finite-dimensional spaces, this is equivalent to saying that \\(T\\) is represented by an invertible matrix.\n\n\n\nInvertible Matrices\nAn \\(n \\times n\\) matrix \\(A\\) is invertible if there exists another \\(n \\times n\\) matrix \\(A^{-1}\\) such that:\n\\[\nAA^{-1} = A^{-1}A = I.\n\\]\nCharacterizations of Invertibility:\n\n\\(A\\) is invertible ⇔ \\(\\det(A) \\neq 0\\).\n⇔ Columns of \\(A\\) are linearly independent.\n⇔ Columns of \\(A\\) span \\(\\mathbb{R}^n\\).\n⇔ Rank of \\(A\\) is \\(n\\).\n⇔ The system \\(Ax=b\\) has exactly one solution for every \\(b\\).\n\nAll these properties tie together: invertibility means no information is lost when transforming vectors.\n\n\nIsomorphisms of Vector Spaces\nTwo vector spaces \\(V\\) and \\(W\\) are isomorphic if there exists a bijective linear transformation \\(T: V \\to W\\).\n\nThis means \\(V\\) and \\(W\\) are “the same” in structure, though they may look different.\nFor finite-dimensional spaces:\n\\[\nV \\cong W \\quad \\text{if and only if} \\quad \\dim(V) = \\dim(W).\n\\]\nExample: \\(\\mathbb{R}^2\\) and the set of all polynomials of degree ≤ 1 are isomorphic, because both have dimension 2.\n\n\n\nExamples of Invertibility\n\nRotation in the plane: Every rotation matrix has an inverse (rotation by the opposite angle).\n\\[\nR(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}, \\quad R(\\theta)^{-1} = R(-\\theta).\n\\]\nScaling by nonzero factor: \\(T(x) = ax\\) with \\(a \\neq 0\\). Inverse is \\(T^{-1}(x) = \\tfrac{1}{a}x\\).\nProjection onto a line: Not invertible, because depth is lost. The kernel is nontrivial.\nDifferentiation on polynomials of degree ≤ n: Not invertible, since constant terms vanish in the kernel.\nDifferentiation on exponential functions: Invertible: the inverse is integration (up to constants).\n\n\n\nGeometric Interpretation\n\nInvertible transformations preserve dimension: no flattening or collapsing occurs.\nThey may rotate, shear, stretch, or reflect, but every input vector can be uniquely recovered.\nThe determinant tells the “volume scaling” of the transformation: invertibility requires this volume not to collapse to zero.\n\n\n\nApplications\n\nComputer graphics: Invertible matrices allow smooth transformations where no information is lost. Non-invertible maps (like projections) create 2D renderings from 3D worlds.\nCryptography: Encryption systems rely on invertible linear maps for encoding/decoding.\nRobotics: Transformations between joint and workspace coordinates must often be invertible for precise control.\nData science: PCA often reduces dimension (non-invertible), but whitening transformations are invertible within the chosen subspace.\nPhysics: Coordinate changes (e.g., Galilean or Lorentz transformations) are invertible, ensuring that physical laws remain consistent.\n\n\n\nWhy It Matters\n\nInvertible maps preserve the entire structure of a vector space.\nThey classify vector spaces: if two have the same dimension, they are fundamentally the same via isomorphism.\nThey allow reversible modeling, essential in physics, cryptography, and computation.\nThey highlight the delicate balance between lossless transformations (invertible) and lossy ones (non-invertible).\n\n\n\nTry It Yourself\n\nProve that the matrix \\(\\begin{bmatrix} 2 & 1 \\\\ 3 & 2 \\end{bmatrix}\\) is invertible by computing its determinant and its inverse.\nShow that projection onto the x-axis in \\(\\mathbb{R}^2\\) is not invertible. Identify its kernel.\nConstruct an explicit isomorphism between \\(\\mathbb{R}^3\\) and the space of polynomials of degree ≤ 2.\nChallenge: Prove that if \\(T\\) is an isomorphism, then it maps bases to bases.\n\nInvertibility and isomorphism are the gateways from “linear rules” to the grand idea of equivalence. They allow us to say, with mathematical precision, when two spaces are truly the same in structure-different clothes, same skeleton.\n\n\n\n45. Composition, Powers, and Iteration\nLinear transformations are not isolated operations-they can be combined, repeated, and layered to build more complex effects. This leads us to the ideas of composition, powers of transformations, and iteration. These concepts form the backbone of linear dynamics, algorithms, and many real-world systems where repeated actions accumulate into surprising results.\n\nComposition of Linear Transformations\nIf \\(T: U \\to V\\) and \\(S: V \\to W\\) are linear transformations, then their composition is another transformation\n\\[\nS \\circ T : U \\to W, \\quad (S \\circ T)(u) = S(T(u)).\n\\]\n\nComposition is associative: \\((R \\circ S) \\circ T = R \\circ (S \\circ T)\\).\nComposition is linear: the result of composing two linear maps is still linear.\nIn terms of matrices, if \\(T(x) = Ax\\) and \\(S(x) = Bx\\), then\n\\[\n(S \\circ T)(x) = B(Ax) = (BA)x.\n\\]\nNotice that the order matters: composition corresponds to matrix multiplication.\n\nExample:\n\n\\(T(x,y) = (x+2y, y)\\).\n\\(S(x,y) = (2x, x-y)\\). Then \\((S \\circ T)(x,y) = S(x+2y,y) = (2(x+2y), (x+2y)-y) = (2x+4y, x+y)\\). Matrix multiplication confirms the same result.\n\n\n\nPowers of Transformations\nIf \\(T: V \\to V\\), we can apply it repeatedly:\n\\[\nT^2 = T \\circ T, \\quad T^3 = T \\circ T \\circ T, \\quad \\dots\n\\]\n\nThese are called powers of \\(T\\).\nIf \\(T(x) = Ax\\), then \\(T^k(x) = A^k x\\).\nPowers of transformations capture repeated processes, like compounding interest, population growth, or iterative algorithms.\n\nExample: Let \\(T(x,y) = (2x, 3y)\\). Then\n\\[\nT^n(x,y) = (2^n x, 3^n y).\n\\]\nEach iteration amplifies the scaling along different directions.\n\n\nIteration and Dynamical Systems\nIteration means applying the same transformation repeatedly to study long-term behavior:\n\\[\nx_{k+1} = T(x_k), \\quad x_0 \\text{ given}.\n\\]\n\nThis creates a discrete dynamical system.\nDepending on \\(T\\), vectors may grow, shrink, oscillate, or stabilize.\n\nExample 1 (Markov Chains): If \\(T\\) is a stochastic matrix, iteration describes probability evolution over time. Eventually, the system may converge to a steady-state distribution.\nExample 2 (Population Models): If \\(T\\) describes how sub-populations interact, iteration simulates generations. Eigenvalues dictate whether populations explode, stabilize, or vanish.\nExample 3 (Computer Graphics): Repeated affine transformations create fractals like the Sierpinski triangle.\n\n\nStability and Eigenvalues\nThe behavior of \\(T^n(x)\\) depends heavily on eigenvalues of the transformation.\n\nIf \\(|\\lambda| &lt; 1\\), repeated application shrinks vectors in that direction to zero.\nIf \\(|\\lambda| &gt; 1\\), repeated application causes exponential growth.\nIf \\(|\\lambda| = 1\\), vectors rotate or oscillate without changing length.\n\nThis link between powers and eigenvalues underpins many algorithms in numerical analysis and physics.\n\n\nGeometric Interpretation\n\nComposition = chaining geometric actions (rotate then reflect, scale then shear).\nPowers = applying the same action repeatedly (rotating 90° four times = identity).\nIteration = exploring the “orbit” of a vector under repeated transformations.\n\n\n\nApplications\n\nSearch engines: PageRank is computed by iterating a linear transformation until it stabilizes.\nEconomics: Input–output models iterate to predict long-term equilibrium of industries.\nPhysics: Time evolution of quantum states is modeled by repeated application of unitary operators.\nNumerical methods: Iterative solvers (like power iteration) approximate eigenvectors.\nComputer graphics: Iterated function systems generate self-similar fractals.\n\n\n\nWhy It Matters\n\nComposition unifies matrix multiplication and transformation chaining.\nPowers reveal exponential growth, decay, and oscillation.\nIteration is the core of modeling dynamic processes in mathematics, science, and engineering.\nThe link to eigenvalues makes these ideas the foundation of stability analysis.\n\n\n\nTry It Yourself\n\nLet \\(T(x,y) = (x+y, y)\\). Compute \\(T^2(x,y)\\) and \\(T^3(x,y)\\). What happens as \\(n \\to \\infty\\)?\nConsider rotation by 90° in \\(\\mathbb{R}^2\\). Show that \\(T^4 = I\\).\nFor matrix \\(A = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}\\), iterate \\(A^n\\). What happens to arbitrary vectors?\nChallenge: Prove that if \\(A\\) is diagonalizable as \\(A = PDP^{-1}\\), then \\(A^n = PD^nP^{-1}\\). Use this to analyze long-term behavior.\n\nComposition, powers, and iteration take linear algebra beyond static equations into the world of processes over time. They explain how small, repeated steps shape long-term outcomes-whether stabilizing systems, amplifying signals, or creating infinite complexity.\n\n\n\n46. Similarity and Conjugation\nIn linear algebra, different matrices can represent the same underlying transformation when written in different coordinate systems. This relationship is captured by the idea of similarity. Two matrices are similar if one is obtained from the other by a conjugation with an invertible change-of-basis matrix. This concept is central to understanding canonical forms, eigenvalue decompositions, and the deep structure of linear operators.\n\nDefinition of Similarity\nTwo \\(n \\times n\\) matrices \\(A\\) and \\(B\\) are called similar if there exists an invertible matrix \\(P\\) such that:\n\\[\nB = P^{-1}AP.\n\\]\n\nHere, \\(P\\) represents a change of basis.\n\\(A\\) and \\(B\\) describe the same linear transformation, but expressed relative to different bases.\n\n\n\nConjugation as Change of Basis\nSuppose \\(T: V \\to V\\) is a linear transformation and \\(A\\) is its matrix in basis \\(B\\). If we switch to a new basis \\(C\\), the matrix becomes \\(B\\). The conversion is:\n\\[\nB = P^{-1}AP,\n\\]\nwhere \\(P\\) is the change-of-basis matrix from basis \\(B\\) to basis \\(C\\).\nThis shows that similarity is not just algebraic coincidence-it’s geometric: the operator is the same, but our perspective (basis) has changed.\n\n\nProperties Preserved Under Similarity\nIf \\(A\\) and \\(B\\) are similar, they share many key properties:\n\nDeterminant: \\(\\det(A) = \\det(B)\\).\nTrace: \\(\\text{tr}(A) = \\text{tr}(B)\\).\nRank: \\(\\text{rank}(A) = \\text{rank}(B)\\).\nEigenvalues: Same set of eigenvalues (with multiplicity).\nCharacteristic polynomial: Identical.\nMinimal polynomial: Identical.\n\nThese invariants define the “skeleton” of a linear operator, unaffected by coordinate changes.\n\n\nExamples\n\nRotation in the plane: The matrix for rotation by 90° is\n\\[\nA = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\nIn another basis, the rotation might be represented by a more complicated-looking matrix, but all such matrices are similar to \\(A\\).\nDiagonalization: A matrix \\(A\\) is diagonalizable if it is similar to a diagonal matrix \\(D\\). That is,\n\\[\nA = PDP^{-1}.\n\\]\nHere, similarity reduces \\(A\\) to its simplest form.\nShear transformation: A shear matrix \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\) is not diagonalizable, but it may be similar to a Jordan block.\n\n\n\nGeometric Interpretation\n\nSimilarity says: two matrices may look different, but they are “the same” transformation seen from different coordinate systems.\nConjugation is the mathematical act of relabeling coordinates.\nThink of shifting your camera angle: the scene hasn’t changed, only the perspective has.\n\n\n\nApplications\n\nDiagonalization: Reducing a matrix to diagonal form (when possible) uses similarity. This simplifies powers, exponentials, and iterative analysis.\nJordan canonical form: Every square matrix is similar to a Jordan form, giving a complete structural classification.\nQuantum mechanics: Operators on state spaces often change representation, but similarity guarantees invariance of spectra.\nControl theory: Canonical forms simplify analysis of system stability and controllability.\nNumerical methods: Eigenvalue algorithms rely on repeated similarity transformations (e.g., QR algorithm).\n\n\n\nWhy It Matters\n\nSimilarity reveals the true identity of a linear operator, independent of coordinates.\nIt allows simplification: many problems become easier in the right basis.\nIt preserves invariants, giving us tools to classify and compare operators.\nIt connects abstract algebra with concrete computations in geometry, physics, and engineering.\n\n\n\nTry It Yourself\n\nShow that \\(\\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\\) is similar to \\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\\). Why or why not?\nCompute \\(P^{-1}AP\\) for \\(A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\\) and \\(P = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\). Interpret the result.\nProve that if two matrices are similar, they must have the same trace.\nChallenge: Show that if \\(A\\) and \\(B\\) are similar, then \\(A^k\\) and \\(B^k\\) are also similar for all integers \\(k \\geq 0\\).\n\nSimilarity and conjugation elevate linear algebra from mere calculation to structural understanding. They tell us when two seemingly different matrices are just different “faces” of the same underlying transformation.\n\n\n\n47. Projections and Reflections\nAmong the many transformations in linear algebra, two stand out for their geometric clarity and practical importance: projections and reflections. These operations reshape vectors in simple but powerful ways, and they form the building blocks of algorithms in statistics, optimization, graphics, and physics.\n\nProjection: Flattening onto a Subspace\nA projection is a linear transformation that takes a vector and drops it onto a subspace, like casting a shadow.\nFormally, if \\(W\\) is a subspace of \\(V\\), the projection of a vector \\(v\\) onto \\(W\\) is the unique vector \\(w \\in W\\) that is closest to \\(v\\).\nIn \\(\\mathbb{R}^2\\): projecting onto the x-axis takes \\((x,y)\\) and produces \\((x,0)\\).\n\nOrthogonal Projection Formula\nSuppose \\(u\\) is a nonzero vector. The projection of \\(v\\) onto the line spanned by \\(u\\) is:\n\\[\n\\text{proj}_u(v) = \\frac{v \\cdot u}{u \\cdot u} u.\n\\]\nThis formula works in any dimension. It uses the dot product to measure how much of \\(v\\) points in the direction of \\(u\\).\nExample: Project \\((2,3)\\) onto \\(u=(1,1)\\):\n\\[\n\\text{proj}_u(2,3) = \\frac{(2,3)\\cdot(1,1)}{(1,1)\\cdot(1,1)} (1,1) = \\frac{5}{2}(1,1) = (2.5,2.5).\n\\]\nThe vector \\((2,3)\\) splits into \\((2.5,2.5)\\) along the line plus \\((-0.5,0.5)\\) orthogonal to it.\n\n\nProjection Matrices\nFor unit vector \\(u\\):\n\\[\nP = uu^T\n\\]\nis the projection matrix onto the span of \\(u\\).\nFor a general subspace with orthonormal basis columns in matrix \\(Q\\):\n\\[\nP = QQ^T\n\\]\nprojects any vector onto that subspace.\nProperties:\n\n\\(P^2 = P\\) (idempotent).\n\\(P^T = P\\) (symmetric, for orthogonal projections).\n\n\n\n\nReflection: Flipping Across a Subspace\nA reflection takes a vector and flips it across a line or plane. Geometrically, it’s like a mirror.\nReflection across a line spanned by unit vector \\(u\\):\n\\[\nR(v) = 2\\text{proj}_u(v) - v.\n\\]\nMatrix form:\n\\[\nR = 2uu^T - I.\n\\]\nExample: Reflect \\((2,3)\\) across the line \\(y=x\\). With \\(u=(1/\\sqrt{2},1/\\sqrt{2})\\):\n\\[\nR = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\nSo reflection swaps coordinates: \\((2,3) \\mapsto (3,2)\\).\n\n\nGeometric Insight\n\nProjection shortens vectors by removing components orthogonal to the subspace.\nReflection preserves length but flips orientation relative to the subspace.\nProjection is about approximation (“closest point”), reflection is about symmetry.\n\n\n\nApplications\n\nStatistics & Machine Learning: Least-squares regression is projection of data onto the span of predictor variables.\nComputer Graphics: Projection transforms 3D scenes into 2D screen images. Reflections simulate mirrors and shiny surfaces.\nOptimization: Projections enforce constraints by bringing guesses back into feasible regions.\nPhysics: Reflections describe wave behavior, optics, and particle interactions.\nNumerical Methods: Projection operators are key to iterative algorithms (like Krylov subspace methods).\n\n\n\nWhy It Matters\n\nProjection captures the essence of approximation: keeping what fits, discarding what doesn’t.\nReflection embodies symmetry and invariance, key to geometry and physics.\nBoth are linear, with elegant matrix representations.\nThey combine easily with other transformations, making them versatile in computation.\n\n\n\nTry It Yourself\n\nFind the projection matrix onto the line spanned by \\((3,4)\\). Verify it is idempotent.\nCompute the reflection of \\((1,2)\\) across the x-axis.\nShow that reflection matrices are orthogonal (\\(R^T R = I\\)).\nChallenge: For subspace \\(W\\) with orthonormal basis \\(Q\\), derive the reflection matrix \\(R = 2QQ^T - I\\).\n\nProjections and reflections are two of the purest examples of how linear transformations embody geometric ideas. One approximates, the other symmetrizes-but both expose the deep structure of space through the lens of linear algebra.\n\n\n\n48. Rotations and Shear\nLinear transformations can twist, turn, and distort space in strikingly different ways. Two of the most fundamental examples are rotations-which preserve lengths and angles while turning vectors-and shears-which slide one part of space relative to another, distorting shape while often preserving area. These two transformations form the geometric heart of linear algebra, and they are indispensable in graphics, physics, and engineering.\n\nRotations in the Plane\nA rotation in \\(\\mathbb{R}^2\\) by an angle \\(\\theta\\) is defined as:\n\\[\nR_\\theta = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}.\n\\]\nFor any vector \\((x,y)\\):\n\\[\nR_\\theta \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x\\cos\\theta - y\\sin\\theta \\\\ x\\sin\\theta + y\\cos\\theta \\end{bmatrix}.\n\\]\nProperties:\n\nPreserves lengths: \\(\\|R_\\theta v\\| = \\|v\\|\\).\nPreserves angles: the dot product is unchanged.\nDeterminant = \\(+1\\), so it preserves orientation and area.\nInverse: \\(R_\\theta^{-1} = R_{-\\theta}\\).\n\nExample: A 90° rotation:\n\\[\nR_{90^\\circ} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, \\quad (1,0) \\mapsto (0,1).\n\\]\n\n\nRotations in Three Dimensions\nRotations in \\(\\mathbb{R}^3\\) occur around an axis. For example, rotation by angle \\(\\theta\\) around the z-axis:\n\\[\nR_z(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n\\]\n\nLeaves the z-axis fixed.\nRotates the xy-plane like a 2D rotation.\n\nGeneral rotations in 3D are described by orthogonal matrices with determinant +1, forming the group \\(SO(3)\\).\n\n\nShear Transformations\nA shear slides one coordinate direction while keeping another fixed, distorting shapes.\nIn \\(\\mathbb{R}^2\\):\n\\[\nS = \\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} 1 & 0 \\\\ k & 1 \\end{bmatrix}.\n\\]\n\nThe first form “slides” x-coordinates depending on y.\nThe second form slides y-coordinates depending on x.\n\nExample:\n\\[\nS = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad (x,y) \\mapsto (x+y, y).\n\\]\n\nSquares become parallelograms.\nAreas are preserved if \\(\\det(S) = 1\\).\n\nIn \\(\\mathbb{R}^3\\): shears distort volumes while preserving parallelism of faces.\n\n\nGeometric Comparison\n\nRotation: Preserves size and shape exactly, only changes orientation. Circles remain circles.\nShear: Distorts shape but often preserves area (in 2D) or volume (in 3D). Circles become ellipses or slanted figures.\n\nTogether, rotations and shears can generate a vast variety of linear distortions.\n\n\nApplications\n\nComputer Graphics: Rotations orient objects; shears simulate perspective.\nEngineering: Shear stresses deform materials; rotations model rigid-body motion.\nRobotics: Rotations define arm orientation; shears approximate local deformations.\nPhysics: Rotations are symmetries of space; shears appear in fluid flows and elasticity.\nData Science: Shears represent changes of variables that preserve volume but distort distributions.\n\n\n\nWhy It Matters\n\nRotations model pure symmetry-no distortion, just reorientation.\nShears show how geometry can be distorted while preserving volume or area.\nBoth are building blocks: any invertible matrix in \\(\\mathbb{R}^2\\) can be factored into rotations, shears, and scalings.\nThey bridge algebra and geometry, giving visual meaning to abstract matrices.\n\n\n\nTry It Yourself\n\nRotate \\((1,0)\\) by 60° and compute the result explicitly.\nApply the shear \\(S=\\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\\) to the square with vertices \\((0,0),(1,0),(0,1),(1,1)\\). What shape results?\nShow that rotation matrices are orthogonal (\\(R^TR=I\\)).\nChallenge: Prove that any area-preserving \\(2\\times2\\) matrix with determinant 1 can be decomposed into a product of rotations and shears.\n\nRotations and shears highlight two complementary sides of linear algebra: symmetry versus distortion. Together, they show how transformations can either preserve the essence of space or bend it into new shapes while keeping its structure intact.\n\n\n\n49. Rank and Operator Viewpoint\nThe rank of a linear transformation or matrix is one of the most important measures of its power. It captures how many independent directions a transformation preserves, how much information it carries from input to output, and how “full” its action on space is. Thinking of rank not just as a number, but as a description of an operator, gives us a clearer picture of what transformations really do.\n\nDefinition of Rank\nFor a matrix \\(A\\) representing a linear transformation \\(T: V \\to W\\):\n\\[\n\\text{rank}(A) = \\dim(\\text{im}(A)) = \\dim(\\text{im}(T)).\n\\]\nThat is, the rank is the dimension of the image (or column space). It counts the maximum number of linearly independent columns.\n\n\nBasic Properties\n\n\\(\\text{rank}(A) \\leq \\min(m,n)\\) for an \\(m \\times n\\) matrix.\n\\(\\text{rank}(A) = \\text{rank}(A^T)\\).\nRank is equal to the number of pivot columns in row-reduced form.\nRank links directly with nullity via the rank–nullity theorem:\n\\[\n\\text{rank}(A) + \\text{nullity}(A) = n.\n\\]\n\n\n\nOperator Perspective\nInstead of focusing on rows and columns, imagine rank as a measure of how much of the domain is transmitted faithfully to the codomain.\n\nIf rank = full (\\(n\\)), the transformation is injective: nothing collapses.\nIf rank = dimension of codomain (\\(m\\)), the transformation is surjective: every target vector can be reached.\nIf rank is smaller, the transformation compresses space: parts of the domain are “invisible” and collapse into the kernel.\n\nExample 1 (Projection): Projection from \\(\\mathbb{R}^3\\) onto the xy-plane has rank 2. It annihilates the z-direction but preserves two independent directions.\nExample 2 (Rotation): Rotation in \\(\\mathbb{R}^2\\) has rank 2. No directions are lost.\nExample 3 (Zero map): The transformation sending everything to zero has rank 0.\n\n\nGeometric Meaning\n\nRank = number of independent directions preserved.\nA rank-1 transformation maps all of space onto a single line.\nRank-2 in \\(\\mathbb{R}^3\\) maps space onto a plane.\nRank-full maps space onto its entire dimension without collapse.\n\nVisually: rank describes the “dimensional thickness” of the image.\n\n\nRank and Matrix Factorizations\nRank reveals hidden structure:\n\nLU factorization: Rank determines the number of nonzero pivots.\nQR factorization: Rank controls the number of orthogonal directions.\nSVD (Singular Value Decomposition): The number of nonzero singular values equals the rank.\n\nSVD in particular gives a geometric operator view: each nonzero singular value corresponds to a preserved dimension, while zeros indicate collapsed directions.\n\n\nRank in Applications\n\nData compression: Low-rank approximations reduce storage (e.g., image compression with SVD).\nStatistics: Rank of the design matrix determines identifiability of regression coefficients.\nMachine learning: Rank of weight matrices controls expressive power of models.\nControl theory: Rank conditions ensure controllability and observability of systems.\nNetwork analysis: Rank of adjacency or Laplacian matrices reflects connectivity of graphs.\n\n\n\nRank Deficiency\nIf a transformation has less than full rank, it is rank-deficient. This means:\n\nSome directions are lost (kernel nontrivial).\nSome outputs are unreachable (image smaller than codomain).\nEquations \\(Ax=b\\) may be inconsistent or underdetermined.\n\nDetecting and handling rank deficiency is crucial in numerical linear algebra, where ill-conditioning can hide in nearly dependent columns.\n\n\nWhy It Matters\n\nRank measures the true dimensional effect of a transformation.\nIt distinguishes between full-strength operators and those that collapse information.\nIt connects row space, column space, image, and kernel under one number.\nIt underpins algorithms for regression, decomposition, and dimensionality reduction.\n\n\n\nTry It Yourself\n\nFind the rank of \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\\). Why is it less than 2?\nDescribe geometrically the image of a rank-1 transformation in \\(\\mathbb{R}^3\\).\nFor a \\(5 \\times 5\\) diagonal matrix with diagonal entries \\((2,0,3,0,5)\\), compute rank and nullity.\nChallenge: Show that for any matrix \\(A\\), the rank equals the number of nonzero singular values of \\(A\\).\n\nRank tells us not just how many independent vectors survive a transformation, but also how much structure the operator truly preserves. It is the bridge between abstract linear maps and their practical power.\n\n\n\n50. Block Matrices and Block Maps\nAs problems grow in size, matrices become large and difficult to manage element by element. A powerful strategy is to organize matrices into blocks-submatrices grouped together like tiles in a mosaic. This allows us to treat large transformations as compositions of smaller, more understandable ones. Block matrices preserve structure, simplify computations, and reveal deep insights into how transformations act on subspaces.\n\nWhat Are Block Matrices?\nA block matrix partitions a matrix into rectangular submatrices. Each block is itself a matrix, and the entire matrix can be manipulated using block rules.\nExample: a \\(4 \\times 4\\) matrix divided into four \\(2 \\times 2\\) blocks:\n\\[\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix},\n\\]\nwhere each \\(A_{ij}\\) is \\(2 \\times 2\\).\nInstead of thinking in terms of 16 entries, we work with 4 blocks.\n\n\nBlock Maps as Linear Transformations\nSuppose \\(V = V_1 \\oplus V_2\\) is decomposed into two subspaces. A linear map \\(T: V \\to V\\) can be described in terms of how it acts on each component. Relative to this decomposition, the matrix of \\(T\\) has block form:\n\\[\n[T] = \\begin{bmatrix}\nT_{11} & T_{12} \\\\\nT_{21} & T_{22}\n\\end{bmatrix}.\n\\]\n\n\\(T_{11}\\): how \\(V_1\\) maps into itself.\n\\(T_{12}\\): how \\(V_2\\) contributes to \\(V_1\\).\n\\(T_{21}\\): how \\(V_1\\) contributes to \\(V_2\\).\n\\(T_{22}\\): how \\(V_2\\) maps into itself.\n\nThis decomposition highlights how subspaces interact under the transformation.\n\n\nBlock Matrix Operations\nBlock matrices obey the same rules as normal matrices, but operations are done block by block.\nAddition:\n\\[\n\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} +\n\\begin{bmatrix} E & F \\\\ G & H \\end{bmatrix} =\n\\begin{bmatrix} A+E & B+F \\\\ C+G & D+H \\end{bmatrix}.\n\\]\nMultiplication:\n\\[\n\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}\n\\begin{bmatrix} E & F \\\\ G & H \\end{bmatrix} =\n\\begin{bmatrix} AE+BG & AF+BH \\\\ CE+DG & CF+DH \\end{bmatrix}.\n\\]\nThe formulas look like ordinary multiplication, but each term is itself a product of submatrices.\n\n\nSpecial Block Structures\n\nBlock Diagonal Matrices:\n\\[\n\\begin{bmatrix} A & 0 \\\\ 0 & D \\end{bmatrix}.\n\\]\nIndependent actions on subspaces-no mixing between them.\nBlock Upper Triangular:\n\\[\n\\begin{bmatrix} A & B \\\\ 0 & D \\end{bmatrix}.\n\\]\nSubspace \\(V_1\\) influences \\(V_2\\), but not vice versa.\nBlock Symmetric: If overall matrix is symmetric, so are certain block relationships: \\(A^T=A, D^T=D, B^T=C\\).\n\nThese structures appear naturally in decomposition and iterative algorithms.\n\n\nBlock Matrix Inverses\nSome block matrices can be inverted using special formulas. For\n\\[\nM = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix},\n\\]\nif \\(A\\) is invertible, the inverse can be expressed using the Schur complement:\n\\[\nM^{-1} = \\begin{bmatrix}\nA^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}\n\\end{bmatrix}.\n\\]\nThis formula is central in statistics, optimization, and numerical analysis.\n\n\nGeometric Interpretation\n\nA block diagonal matrix acts like two independent transformations operating side by side.\nA block triangular matrix shows a “hierarchy”: one subspace influences the other but not the reverse.\nThis decomposition mirrors how systems can be separated into smaller interacting parts.\n\n\n\nApplications\n\nNumerical Linear Algebra: Block operations optimize computation on large sparse matrices.\nControl Theory: State-space models are naturally expressed in block form.\nStatistics: Partitioned covariance matrices rely on block inversion formulas.\nMachine Learning: Neural networks layer transformations, often structured into blocks for efficiency.\nParallel Computing: Block decomposition distributes large matrix problems across processors.\n\n\n\nWhy It Matters\n\nBlock matrices turn big problems into manageable smaller ones.\nThey reflect natural decompositions of systems into interacting parts.\nThey make explicit the geometry of subspace interactions.\nThey provide efficient algorithms, especially for large-scale scientific computing.\n\n\n\nTry It Yourself\n\nMultiply two \\(4 \\times 4\\) matrices written as \\(2 \\times 2\\) block matrices and confirm the block multiplication rule.\nWrite the projection matrix onto a 2D subspace in \\(\\mathbb{R}^4\\) using block form.\nCompute the Schur complement of\n\\[\n\\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}.\n\\]\nChallenge: Show that the determinant of a block triangular matrix equals the product of the determinants of its diagonal blocks.\n\nBlock matrices and block maps show how complexity can be organized. Instead of drowning in thousands of entries, we see structure, interaction, and hierarchy-revealing how large systems can be built from simple linear pieces.\n\n\nClosing\nShadows twist and turn,\nkernels hide and images flow,\nform remains within.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-6.-determinants-and-volume",
    "href": "books/en-US/book.html#chapter-6.-determinants-and-volume",
    "title": "The Book",
    "section": "Chapter 6. Determinants and volume",
    "text": "Chapter 6. Determinants and volume\n\nOpening\nAreas unfold,\nparallels stretch into waves,\nscale whispers in signs.\n\n\n51. Areas, Volumes, and Signed Scale Factors\nDeterminants often feel like an abstract formula until we see their geometric meaning: they measure area in 2D, volume in 3D, and, in higher dimensions, the general “size” of a transformed shape. Even more, determinants encode whether orientation is preserved or flipped, giving them a “signed” interpretation. This perspective transforms determinants from algebraic curiosities into geometric tools.\n\nTransformations and Scaling of Space\nConsider a linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) represented by a square matrix \\(A\\). When \\(A\\) acts on vectors, it reshapes space: it stretches, compresses, rotates, reflects, or shears regions.\n\nIf you apply \\(A\\) to a unit square in \\(\\mathbb{R}^2\\), the image is a parallelogram.\nIf you apply \\(A\\) to a unit cube in \\(\\mathbb{R}^3\\), the image is a parallelepiped.\nIn general, the determinant of \\(A\\) tells us how the measure (area, volume, hyper-volume) of the shape has changed.\n\n\n\nDeterminant as Signed Scale Factor\n\n\\(|\\det(A)|\\) = the scale factor for areas (2D), volumes (3D), or n-dimensional content.\nIf \\(\\det(A) = 0\\), the transformation collapses space into a lower dimension, flattening all volume away.\nIf \\(\\det(A) &gt; 0\\), the orientation of space is preserved.\nIf \\(\\det(A) &lt; 0\\), the orientation is flipped (like a reflection in a mirror).\n\nThus, determinants are not just numbers-they carry both magnitude and sign, telling us about size and handedness.\n\n\n2D Case: Area of Parallelogram\nTake two column vectors \\(u,v \\in \\mathbb{R}^2\\). Place them as columns in a matrix:\n\\[\nA = \\begin{bmatrix} u & v \\end{bmatrix}.\n\\]\nThe absolute value of the determinant gives the area of the parallelogram spanned by \\(u\\) and \\(v\\):\n\\[\n\\text{Area} = |\\det(A)|.\n\\]\nExample:\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}.\n\\]\nThen \\(\\det(A) = (2)(3) - (1)(1) = 5\\). The unit square maps to a parallelogram of area 5.\n\n\n3D Case: Volume of Parallelepiped\nFor three vectors \\(u,v,w \\in \\mathbb{R}^3\\), form a matrix\n\\[\nA = \\begin{bmatrix} u & v & w \\end{bmatrix}.\n\\]\nThen the absolute determinant gives the volume of the parallelepiped:\n\\[\n\\text{Volume} = |\\det(A)|.\n\\]\nGeometrically, this is the scalar triple product:\n\\[\n\\det(A) = u \\cdot (v \\times w).\n\\]\nExample:\n\\[\nA = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}, \\quad \\det(A) = 6.\n\\]\nSo the unit cube is stretched into a box with volume 6.\n\n\nOrientation and Signed Measure\nDeterminants do more than measure size-they also detect orientation:\n\nIn 2D, flipping x and y axes changes the sign of the determinant.\nIn 3D, swapping two vectors changes the “handedness” (right-hand rule becomes left-hand rule).\n\nThis explains why determinants can be negative: they mark transformations that reverse orientation.\n\n\nHigher Dimensions\nIn \\(\\mathbb{R}^n\\), determinants extend the same idea. A unit hypercube (side length 1) is transformed into an n-dimensional parallelotope, whose volume is given by \\(|\\det(A)|\\).\nThough we cannot visualize beyond 3D, the concept generalizes smoothly: determinants encode how much an n-dimensional object is stretched or collapsed.\n\n\nApplications\n\nGeometry: Computing areas, volumes, and orientation directly from vectors.\nComputer Graphics: Determinants detect whether a transformation preserves or flips orientation, useful in rendering.\nPhysics: Determinants describe Jacobians for coordinate changes in integrals, adjusting volume elements.\nEngineering: Determinants quantify deformation and stress in materials (strain tensors).\nData Science: Determinants of covariance matrices encode “volume” of uncertainty ellipsoids.\n\n\n\nWhy It Matters\n\nDeterminants connect algebra (formulas) to geometry (shapes).\nThey explain why some transformations lose information: \\(\\det=0\\).\nThey preserve orientation, key for consistent physical laws and geometry.\nThey prepare us for advanced tools like Jacobians, eigenvalues, and volume-preserving maps.\n\n\n\nTry It Yourself\n\nCompute the area of the parallelogram spanned by \\((1,2)\\) and \\((3,1)\\).\nFind the volume of the parallelepiped defined by vectors \\((1,0,0),(0,1,0),(1,1,1)\\).\nShow that swapping two columns of a matrix flips the sign of the determinant but keeps absolute value unchanged.\nChallenge: Explain why \\(\\det(A)\\) gives the scaling factor for integrals under change of variables.\n\nDeterminants begin as algebraic formulas, but their real meaning lies in geometry: they measure how linear transformations scale, compress, or flip space itself.\n\n\n\n52. Determinant via Linear Rules\nThe determinant is not just a mysterious formula-it is a function built from a few simple rules that uniquely determine its behavior. These rules, often called determinant axioms, allow us to see the determinant as the only measure of “signed volume” compatible with linear algebra. Understanding these rules gives clarity: instead of memorizing expansion formulas, we see why determinants behave as they do.\n\nThe Setup\nTake a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\). Think of \\(A\\) as a list of \\(n\\) column vectors:\n\\[\nA = \\begin{bmatrix} a_1 & a_2 & \\cdots & a_n \\end{bmatrix}.\n\\]\nThe determinant is a function \\(\\det: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}\\) that assigns a single number to \\(A\\). Geometrically, it gives the signed volume of the parallelotope spanned by \\((a_1, \\dots, a_n)\\). Algebraically, it follows three key rules.\n\n\nRule 1: Linearity in Each Column\nIf you scale one column by a scalar \\(c\\), the determinant scales by \\(c\\).\n\\[\n\\det(a_1, \\dots, c a_j, \\dots, a_n) = c \\cdot \\det(a_1, \\dots, a_j, \\dots, a_n).\n\\]\nIf you replace a column with a sum, the determinant splits:\n\\[\n\\det(a_1, \\dots, (b+c), \\dots, a_n) = \\det(a_1, \\dots, b, \\dots, a_n) + \\det(a_1, \\dots, c, \\dots, a_n).\n\\]\nThis linearity means determinants behave predictably with respect to scaling and addition.\n\n\nRule 2: Alternating Property\nIf two columns are the same, the determinant is zero:\n\\[\n\\det(\\dots, a_i, \\dots, a_i, \\dots) = 0.\n\\]\nThis makes sense geometrically: if two spanning vectors are identical, they collapse the volume to zero.\nEquivalently: if you swap two columns, the determinant flips sign:\n\\[\n\\det(\\dots, a_i, \\dots, a_j, \\dots) = -\\det(\\dots, a_j, \\dots, a_i, \\dots).\n\\]\n\n\nRule 3: Normalization\nThe determinant of the identity matrix is 1:\n\\[\n\\det(I_n) = 1.\n\\]\nThis anchors the function: the unit cube has volume 1, with positive orientation.\n\n\nConsequence: Uniqueness\nThese three rules (linearity, alternating, normalization) uniquely define the determinant. Any function satisfying them must be the determinant. This makes it less of an arbitrary formula and more of a natural consequence of linear structure.\n\n\nSmall Cases: Explicit Formulas\n\n2×2 matrices:\n\\[\n\\det \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc.\n\\]\nThis formula arises directly from the rules: linearity in columns and alternating sign when swapping them.\n3×3 matrices: Expansion formula:\n\\[\n\\det \\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n= aei + bfg + cdh - ceg - bdi - afh.\n\\]\n\nThis looks complicated, but it comes from systematically applying the rules to break down the volume.\n\n\nGeometric Interpretation of the Rules\n\nLinearity: Stretching one side of a parallelogram or parallelepiped scales the area or volume.\nAlternating: If two sides collapse into the same direction, the area/volume vanishes. Swapping sides flips orientation.\nNormalization: The unit cube has size 1 by definition.\n\nTogether, these mirror geometric intuition exactly.\n\n\nHigher-Dimensional Generalization\nIn \\(\\mathbb{R}^n\\), determinants measure oriented hyper-volume. For example, in 4D, determinants give the “4-volume” of a parallelotope. Though impossible to picture, the same rules apply.\n\n\nApplications\n\nDefining area and volume: Determinants provide a universal formula for computing geometric sizes from coordinates.\nJacobian determinants: Used in calculus when changing variables in multiple integrals.\nOrientation detection: Whether transformations preserve handedness in geometry or physics.\nComputer graphics: Ensuring consistent orientation of polygons and meshes.\n\n\n\nWhy It Matters\nDeterminants are not arbitrary. They arise naturally once we demand a function that is linear in columns, alternating, and normalized. This explains why so many different formulas and properties agree: they are all shadows of the same underlying definition.\n\n\nTry It Yourself\n\nShow that scaling one column by 3 multiplies the determinant by 3.\nCompute the determinant of \\(\\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\\) and explain why it is zero.\nSwap two columns in \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and confirm the determinant changes sign.\nChallenge: Use only the three rules to derive the \\(2 \\times 2\\) determinant formula.\n\nThe determinant is the unique bridge between algebra and geometry, born from a handful of simple but powerful rules.\n\n\n\n53. Determinant and Row Operations\nOne of the most practical ways to compute determinants is by using row operations, the same tools used in Gaussian elimination. Determinants interact with these operations in very structured ways. By understanding the rules, we can compute determinants systematically without resorting to long expansion formulas.\n\nRow Operations Recap\nThere are three elementary row operations:\n\nRow Swap (R\\(_i \\leftrightarrow\\) R\\(_j\\)) – exchange two rows.\nRow Scaling (c·R\\(_i\\)) – multiply a row by a scalar \\(c\\).\nRow Replacement (R\\(_i\\) + c·R\\(_j\\)) – replace one row with itself plus a multiple of another row.\n\nSince the determinant is defined in terms of linearity and alternation of rows (or columns), each operation has a clear effect.\n\n\nRule 1: Row Swap Changes Sign\nIf you swap two rows, the determinant changes sign:\n\\[\n\\det(A \\text{ with } R_i \\leftrightarrow R_j) = -\\det(A).\n\\]\nReason: Swapping two spanning vectors flips orientation. In 2D, swapping basis vectors flips a parallelogram across the diagonal, reversing handedness.\n\n\nRule 2: Row Scaling Multiplies Determinant\nIf you multiply a row by a scalar \\(c\\):\n\\[\n\\det(A \\text{ with } cR_i) = c \\cdot \\det(A).\n\\]\nReason: Scaling one side of a parallelogram multiplies its area; scaling one dimension of a cube multiplies its volume.\n\n\nRule 3: Row Replacement Leaves Determinant Unchanged\nIf you replace one row with itself plus a multiple of another row:\n\\[\n\\det(A \\text{ with } R_i \\to R_i + cR_j) = \\det(A).\n\\]\nReason: Adding a multiple of one spanning vector to another doesn’t change the spanned volume. The parallelogram or parallelepiped is sheared, but its area or volume remains the same.\n\n\nWhy These Rules Work Together\nThese three rules align perfectly with the determinant axioms:\n\nAlternating → row swaps flip sign.\nLinearity → scaling multiplies by scalar.\nNormalization → row replacement preserves measure.\n\nThus, row operations provide a complete framework for computing determinants.\n\n\nComputing Determinants with Elimination\nTo compute \\(\\det(A)\\):\n\nPerform Gaussian elimination to reduce \\(A\\) to an upper triangular matrix \\(U\\).\nTrack how row swaps and scalings affect the determinant.\nUse the fact that the determinant of a triangular matrix is the product of its diagonal entries.\n\nExample:\n\\[\nA = \\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 1 & 7 \\\\ -2 & 5 & 1 \\end{bmatrix}.\n\\]\n\nStep 1: \\(R_2 \\to R_2 - 2R_1\\), \\(R_3 \\to R_3 + R_1\\). No determinant change.\nStep 2: Upper triangular form emerges:\n\\[\nU = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & -1 & 1 \\\\ 0 & 0 & -5 \\end{bmatrix}.\n\\]\nStep 3: Determinant is product of diagonals: \\(\\det(A) = 2 \\cdot (-1) \\cdot (-5) = 10.\\)\n\nEfficient, clear, and no messy cofactor expansions.\n\n\nGeometric View\n\nRow swap: Flips orientation of the volume.\nRow scaling: Stretches or compresses one dimension of the volume.\nRow replacement: Slides faces of the volume without changing its size.\n\nThis geometric reasoning reinforces why the rules are natural.\n\n\nApplications\n\nEfficient computation: Algorithms for large determinants (LU decomposition) are based on row operations.\nNumerical analysis: Determinant rules help detect stability and singularity.\nGeometry: Orientation tests for polygons rely on row swap rules.\nTheoretical results: Many determinant identities are derived directly from row operation behavior.\n\n\n\nWhy It Matters\n\nDeterminants link algebra to geometry, but computation requires efficient methods.\nRow operations give a hands-on toolkit: they’re the backbone of practical determinant computation.\nUnderstanding these rules explains why algorithms like LU factorization work so well.\n\n\n\nTry It Yourself\n\nCompute the determinant of \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) using elimination.\nVerify that replacing \\(R_2 \\to R_2 + 3R_1\\) does not change the determinant.\nCheck how many sign flips occur if you reorder rows into strictly increasing order.\nChallenge: Prove that elimination combined with these rules always leads to the triangular product formula.\n\nDeterminants are not meant to be expanded by brute force; row operations transform the problem into a clear sequence of steps, connecting algebraic efficiency with geometric intuition.\n\n\n\n54. Triangular Matrices and Product of Diagonals\nAmong all types of matrices, triangular matrices stand out for their simplicity. These are matrices where every entry either above or below the main diagonal is zero. What makes them especially important is that their determinants can be computed almost instantly: the determinant of a triangular matrix is simply the product of its diagonal entries. This property is not only computationally convenient, it also reveals deep connections between determinants, row operations, and structure in linear algebra.\n\nTriangular Matrices Defined\nA square matrix is called upper triangular if all entries below the main diagonal are zero, and lower triangular if all entries above the diagonal are zero.\n\nUpper triangular example:\n\\[\nU = \\begin{bmatrix}\n2 & 5 & -1 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 7\n\\end{bmatrix}.\n\\]\nLower triangular example:\n\\[\nL = \\begin{bmatrix}\n4 & 0 & 0 \\\\\n-2 & 5 & 0 \\\\\n1 & 3 & 6\n\\end{bmatrix}.\n\\]\n\nBoth share the key feature: “everything off one side of the diagonal vanishes.”\n\n\nDeterminant Rule\nFor any triangular matrix,\n\\[\n\\det(T) = \\prod_{i=1}^n t_{ii},\n\\]\nwhere \\(t_{ii}\\) are the diagonal entries.\nSo for the upper triangular \\(U\\) above,\n\\[\n\\det(U) = 2 \\times 3 \\times 7 = 42.\n\\]\n\n\nWhy This Works\nThe determinant is multilinear and alternating. When you expand it (e.g., via cofactor expansion), only one product of entries survives in the expansion: the one that picks exactly the diagonal terms.\n\nIf you try to pick an off-diagonal entry in a row, you eventually get stuck with a zero entry because of the triangular shape.\nThe only surviving term is the product of the diagonals, with sign \\(+1\\).\n\nThis elegant reasoning explains why the rule holds universally.\n\n\nConnection to Row Operations\nRecall: elimination reduces any square matrix to an upper triangular form. Once triangular, the determinant is simply the product of the diagonals, adjusted for row swaps and scalings.\nThus, triangular matrices are not just simple-they are the end goal of elimination algorithms for determinant computation.\n\n\nGeometric Meaning\nIn geometric terms:\n\nA triangular matrix represents a transformation where each coordinate direction depends only on itself and earlier coordinates.\nThe determinant equals the product of scaling along each axis.\nExample: In 3D, scaling x by 2, y by 3, and z by 7 gives a volume scaling of \\(2 \\cdot 3 \\cdot 7 = 42\\).\n\nEven if shear is present in the upper entries, the determinant ignores it-it only cares about the pure diagonal scaling.\n\n\nApplications\n\nEfficient computation: LU decomposition reduces determinants to diagonal product form.\nTheoretical proofs: Many determinant identities reduce to triangular cases.\nNumerical stability: Triangular matrices are well-behaved in computation, crucial for algorithms in numerical linear algebra.\nEigenvalues: For triangular matrices, eigenvalues are exactly the diagonal entries; thus determinant = product of eigenvalues.\nComputer graphics: Triangular forms simplify geometric transformations.\n\n\n\nWhy It Matters\n\nProvides the fastest way to compute determinants in special cases.\nServes as the computational foundation for general determinant algorithms.\nConnects determinants directly to eigenvalues and scaling factors.\nIllustrates how elimination transforms complexity into simplicity.\n\n\n\nTry It Yourself\n\nCompute the determinant of\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix}.\n\\]\n(Check: it should equal \\(1 \\cdot 4 \\cdot 6\\)).\nVerify that a lower triangular matrix with diagonal entries \\((2, -1, 5)\\) has determinant \\(-10\\).\nExplain why an upper triangular matrix with a zero on the diagonal must have determinant 0.\nChallenge: Prove that every square matrix can be reduced to triangular form with determinant tracked by elimination steps.\n\nThe triangular case reveals the heart of determinants: a product of diagonal scalings, stripped of all extra noise. It is the simplest lens through which determinants become transparent.\n\n\n\n55. The Multiplicative Property of Determinants: \\(\\det(AB) = \\det(A)\\det(B)\\)\nOne of the most remarkable and useful facts about determinants is that they multiply across matrix products. For two square matrices of the same size,\n\\[\n\\det(AB) = \\det(A) \\cdot \\det(B).\n\\]\nThis property is fundamental: it connects algebra (matrix multiplication) with geometry (scaling volumes) and is essential for proofs, computations, and applications across mathematics, physics, and engineering.\n\nThe Statement in Words\n\nIf you first apply a linear transformation \\(B\\), and then apply \\(A\\), the total scaling of space is the product of their individual scalings.\nDeterminants track exactly this: the signed volume change under linear transformations.\n\n\n\nGeometric Intuition\nThink of \\(\\det(A)\\) as the signed scale factor by which \\(A\\) changes volume.\n\nApply \\(B\\): a unit cube becomes some parallelepiped with volume \\(|\\det(B)|\\).\nApply \\(A\\): the new parallelepiped scales again by \\(|\\det(A)|\\).\nTotal effect: volume scales by \\(|\\det(A)| \\times |\\det(B)|\\).\n\nThe orientation flips are also consistent: if both flip (negative determinants), the total orientation is preserved (positive product).\n\n\nAlgebraic Reasoning\nThe proof can be approached in multiple ways:\n\nRow Operations and Elimination:\n\n\\(A\\) and \\(B\\) can be factored into elementary matrices (row swaps, scalings, replacements).\nDeterminants behave predictably for each operation.\nSince both sides agree for elementary operations and determinant is multiplicative, the identity holds in general.\n\nAbstract Characterization:\n\nDeterminants are the unique multilinear alternating functions normalized at the identity.\nComposition of linear maps preserves this property, so multiplicativity follows.\n\n\n\n\nSmall Cases\n\n2×2 matrices:\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix}.\n\\]\nCompute \\(AB\\), then \\(\\det(AB)\\). After expansion, you find: \\(\\det(AB) = (ad - bc)(eh - fg) = \\det(A)\\det(B).\\)\n3×3 matrices: A direct computation is messy, but the property still holds and is consistent with elimination proofs.\n\n\n\nKey Consequences\n\nDeterminant of a Power:\n\\[\n\\det(A^k) = (\\det(A))^k.\n\\]\nGeometric meaning: applying the same transformation \\(k\\) times multiplies volume scale repeatedly.\nInverse Matrix: If \\(A\\) is invertible,\n\\[\n\\det(A^{-1}) = \\frac{1}{\\det(A)}.\n\\]\nEigenvalues: Since \\(\\det(A)\\) is the product of eigenvalues, this property matches the fact that eigenvalues multiply under matrix multiplication (when considered via characteristic polynomials).\n\n\n\nGeometric Meaning in Higher Dimensions\n\nIf \\(B\\) scales space by 3 and flips it (det = –3), and \\(A\\) scales by 2 without flipping (det = 2), then \\(AB\\) scales by –6, consistent with the rule.\nDeterminants encapsulate both magnitude (volume scaling) and sign (orientation). Multiplicativity ensures these combine correctly.\n\n\n\nApplications\n\nChange of Variables in Calculus: The Jacobian determinant follows this multiplicative rule, ensuring transformations compose consistently.\nGroup Theory: \\(\\det\\) defines a group homomorphism from the general linear group \\(GL_n\\) to the nonzero reals under multiplication.\nNumerical Analysis: Determinant multiplicativity underlies LU decomposition methods.\nPhysics: In mechanics and relativity, volume elements transform consistently under successive transformations.\nCryptography and Coding Theory: Determinants in modular arithmetic rely on this multiplicative property to preserve structure.\n\n\n\nWhy It Matters\n\nGuarantees consistency: determinants match our intuition about scaling.\nSimplifies computation: determinants of factorizations can be obtained by multiplying smaller pieces.\nProvides theoretical structure: \\(\\det\\) is a homomorphism, embedding linear algebra into the algebra of scalars.\n\n\n\nTry It Yourself\n\nVerify \\(\\det(AB) = \\det(A)\\det(B)\\) for\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 3 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 1 & 4 \\\\ 0 & -2 \\end{bmatrix}.\n\\]\nProve that \\(\\det(A^{-1}) = 1/\\det(A)\\) using the multiplicative rule.\nShow that if \\(\\det(A)=0\\), then \\(\\det(AB)=0\\) for any \\(B\\). Explain why this makes sense geometrically.\nChallenge: Using row operations, show explicitly how multiplicativity emerges from properties of elementary matrices.\n\nThe rule \\(\\det(AB) = \\det(A)\\det(B)\\) transforms determinants from a mysterious calculation into a natural and consistent measure of how linear transformations combine.\n\n\n\n56. Invertibility and Zero Determinant\nThe determinant is more than a geometric scale factor-it is the ultimate test of whether a matrix is invertible. A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) has an inverse if and only if its determinant is nonzero. When the determinant vanishes, the matrix collapses space into a lower dimension, losing information that no transformation can undo.\n\nThe Criterion\n\\[\nA \\text{ invertible } \\iff \\det(A) \\neq 0.\n\\]\n\nIf \\(\\det(A) \\neq 0\\), the transformation stretches or shrinks space but never flattens it. Every output corresponds to exactly one input, so \\(A^{-1}\\) exists.\nIf \\(\\det(A) = 0\\), some directions are squashed into lower dimensions. Information is destroyed, so no inverse exists.\n\n\n\nGeometric Meaning\n\nIn 2D:\n\nA nonzero determinant means the unit square is sent to a parallelogram with nonzero area.\nA zero determinant means the square collapses into a line segment or a point.\n\nIn 3D:\n\nNonzero determinant → unit cube becomes a 3D parallelepiped with volume.\nZero determinant → cube flattens into a sheet or a line; 3D volume is lost.\n\nIn Higher Dimensions:\n\nNonzero determinant preserves n-dimensional volume.\nZero determinant collapses dimension, destroying invertibility.\n\n\n\n\nAlgebraic Meaning\n\nThe determinant is the product of eigenvalues:\n\\[\n\\det(A) = \\lambda_1 \\lambda_2 \\cdots \\lambda_n.\n\\]\nIf any eigenvalue is zero, then \\(\\det(A) = 0\\) and the matrix is singular (not invertible).\nEquivalently, a zero determinant means the matrix has linearly dependent columns or rows. This dependence implies redundancy: not all directions are independent, so the mapping cannot be one-to-one.\n\n\n\nConnection with Linear Systems\n\nIf \\(\\det(A) \\neq 0\\):\n\nThe system \\(Ax = b\\) has a unique solution for every \\(b\\).\nThe inverse matrix \\(A^{-1}\\) exists and satisfies \\(x = A^{-1}b\\).\n\nIf \\(\\det(A) = 0\\):\n\nEither no solutions (inconsistent system) or infinitely many solutions (dependent equations).\nThe mapping \\(x \\mapsto Ax\\) cannot be reversed.\n\n\n\n\nExample: Invertible vs. Singular\n\n\n\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad \\det(A) = 5 \\neq 0.\n\\]\nInvertible.\n\n\n\n\\[\nB = \\begin{bmatrix} 2 & 4 \\\\ 1 & 2 \\end{bmatrix}, \\quad \\det(B) = 0.\n\\]\nNot invertible, since the second column is just twice the first.\n\n\nApplications\n\nSolving Systems: Inverse-based methods rely on nonzero determinants.\nNumerical Methods: Detecting near-singularity warns of unstable solutions.\nGeometry: A singular matrix corresponds to degenerate shapes (flattened, collapsed).\nPhysics: In mechanics and relativity, invertibility ensures that transformations can be reversed.\nComputer Graphics: Non-invertible transformations crush dimensions, breaking rendering pipelines.\n\n\n\nWhy It Matters\n\nDeterminants provide a single scalar test for invertibility.\nThis connects geometry (volume collapse), algebra (linear dependence), and analysis (solvability of systems).\nThe zero/nonzero divide is one of the sharpest and most important in all of linear algebra.\n\n\n\nTry It Yourself\n\nDetermine whether\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\n\\]\nis invertible. Explain both geometrically and algebraically.\nFor\n\\[\n\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix},\n\\]\ncompute the determinant and describe the geometric transformation.\nChallenge: Show that if \\(\\det(A)=0\\), the rows (or columns) of \\(A\\) are linearly dependent.\n\nThe determinant acts as the ultimate yes-or-no test: nonzero means full-dimensional, reversible transformation; zero means collapse and irreversibility.\n\n\n\n57. Cofactor Expansion\nWhile elimination gives a practical way to compute determinants, the cofactor expansion (also called Laplace expansion) offers a recursive definition that works for all square matrices. It expresses the determinant of an \\(n \\times n\\) matrix in terms of determinants of smaller \\((n-1) \\times (n-1)\\) matrices. This method reveals the internal structure of determinants and serves as a bridge between theory and computation.\n\nMinors and Cofactors\n\nThe minor \\(M_{ij}\\) of an entry \\(a_{ij}\\) is the determinant of the submatrix obtained by deleting the \\(i\\)-th row and \\(j\\)-th column from \\(A\\).\nThe cofactor \\(C_{ij}\\) adds a sign factor:\n\\[\nC_{ij} = (-1)^{i+j} M_{ij}.\n\\]\n\nThus each entry contributes to the determinant through its cofactor, with alternating signs arranged in a checkerboard pattern:\n\\[\n\\begin{bmatrix}\n+ & - & + & - & \\cdots \\\\\n- & + & - & + & \\cdots \\\\\n+ & - & + & - & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}.\n\\]\n\n\nThe Expansion Formula\nFor any row \\(i\\):\n\\[\n\\det(A) = \\sum_{j=1}^n a_{ij} C_{ij}.\n\\]\nOr for any column \\(j\\):\n\\[\n\\det(A) = \\sum_{i=1}^n a_{ij} C_{ij}.\n\\]\nThat is, the determinant can be computed by expanding along any row or column.\n\n\nExample: 3×3 Case\nLet\n\\[\nA = \\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}.\n\\]\nExpanding along the first row:\n\\[\n\\det(A) = a \\cdot \\det \\begin{bmatrix} e & f \\\\ h & i \\end{bmatrix}\n- b \\cdot \\det \\begin{bmatrix} d & f \\\\ g & i \\end{bmatrix}\n+ c \\cdot \\det \\begin{bmatrix} d & e \\\\ g & h \\end{bmatrix}.\n\\]\nSimplify each 2×2 determinant:\n\\[\n= a(ei - fh) - b(di - fg) + c(dh - eg).\n\\]\nThis matches the familiar expansion formula for 3×3 determinants.\n\n\nWhy It Works\nCofactor expansion follows directly from the multilinearity and alternating rules of determinants:\n\nOnly one element per row and per column contributes to each term.\nSigns alternate because swapping rows/columns reverses orientation.\nRecursive expansion reduces the problem size until reaching 2×2 determinants, where the formula is simple.\n\n\n\nComputational Complexity\n\nFor \\(n=2\\), expansion is immediate.\nFor \\(n=3\\), expansion is manageable.\nFor large \\(n\\), expansion is very inefficient: computing \\(\\det(A)\\) via cofactors requires \\(O(n!)\\) operations.\n\nThat’s why in practice, elimination or LU decomposition is preferred. Cofactor expansion is best for theory, proofs, and small matrices.\n\n\nGeometric Interpretation\nEach cofactor corresponds to excluding one direction (row/column), measuring the volume of the remaining sub-parallelotope. The alternating sign keeps track of orientation. Thus the determinant is a weighted combination of contributions from all entries along a chosen row or column.\n\n\nApplications\n\nTheoretical proofs: Cofactor expansion underlies many determinant identities.\nAdjugate matrix: Cofactors form the adjugate used in the explicit formula for matrix inverses.\nEigenvalues: Characteristic polynomials use cofactor expansion.\nGeometry: Cofactors describe signed volumes of faces of higher-dimensional shapes.\n\n\n\nWhy It Matters\n\nCofactor expansion connects determinants across dimensions.\nIt provides a universal definition independent of row operations.\nIt explains why determinants behave consistently with volume, orientation, and algebraic rules.\n\n\n\nTry It Yourself\n\nExpand the determinant of\n\\[\n\\begin{bmatrix}\n2 & 1 & 3 \\\\\n0 & -1 & 4 \\\\\n1 & 2 & 0\n\\end{bmatrix}\n\\]\nalong the first row.\nCompute the same determinant by expanding along the second column. Verify the result matches.\nShow that expanding along two different rows gives the same determinant.\nChallenge: Prove by induction that cofactor expansion works for all \\(n \\times n\\) matrices.\n\nCofactor expansion is not the fastest method, but it reveals the recursive structure of determinants and explains why they hold their rich algebraic and geometric meaning.\n\n\n\n58. Permutations and the Sign of the Determinant\nBehind every determinant formula lies a hidden structure: permutations. Determinants can be expressed as a weighted sum over all possible ways of selecting one entry from each row and each column of a matrix. The weight for each selection is determined by the sign of the permutation used. This viewpoint reveals why determinants encode orientation and why their formulas alternate between positive and negative terms.\n\nThe Permutation Definition\nLet \\(S_n\\) denote the set of all permutations of \\(n\\) elements. Each permutation \\(\\sigma \\in S_n\\) rearranges the numbers \\(\\{1, 2, \\ldots, n\\}\\).\nThe determinant of an \\(n \\times n\\) matrix \\(A = [a_{ij}]\\) is defined as:\n\\[\n\\det(A) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{i, \\sigma(i)}.\n\\]\n\nEach product \\(\\prod_{i=1}^n a_{i, \\sigma(i)}\\) picks one entry from each row and each column, according to \\(\\sigma\\).\nThe factor \\(\\text{sgn}(\\sigma)\\) is \\(+1\\) if \\(\\sigma\\) is an even permutation (achieved by an even number of swaps), and \\(-1\\) if it is odd.\n\n\n\nWhy Permutations Appear\nA determinant requires:\n\nLinearity in each row.\nAlternating property (row swaps flip the sign).\nNormalization (\\(\\det(I)=1\\)).\n\nWhen you expand by multilinearity, all possible combinations of choosing one entry per row and column arise. The alternating rule enforces that terms with repeated columns vanish, leaving only permutations. The sign of each permutation enforces the orientation flip.\n\n\nExample: 2×2 Case\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\]\nThere are two permutations in \\(S_2\\):\n\nIdentity \\((1,2)\\): sign \\(+1\\), contributes \\(a \\cdot d\\).\nSwap \\((2,1)\\): sign \\(-1\\), contributes \\(-bc\\).\n\nSo,\n\\[\n\\det(A) = ad - bc.\n\\]\n\n\nExample: 3×3 Case\n\\[\nA = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}.\n\\]\nThere are \\(3! = 6\\) permutations:\n\n\\((1,2,3)\\): even, \\(+aei\\).\n\\((1,3,2)\\): odd, \\(-afh\\).\n\\((2,1,3)\\): odd, \\(-bdi\\).\n\\((2,3,1)\\): even, \\(+bfg\\).\n\\((3,1,2)\\): even, \\(+cdh\\).\n\\((3,2,1)\\): odd, \\(-ceg\\).\n\nSo,\n\\[\n\\det(A) = aei + bfg + cdh - ceg - bdi - afh.\n\\]\nThis is exactly the cofactor expansion result, but now explained as a permutation sum.\n\n\nGeometric Meaning of Signs\n\nEven permutations correspond to consistent orientation of basis vectors.\nOdd permutations correspond to flipped orientation.\nThe determinant alternates signs because flipping axes reverses handedness.\n\n\n\nCounting Growth\n\nFor \\(n=4\\), there are \\(4! = 24\\) terms.\nFor \\(n=5\\), \\(5! = 120\\) terms.\nIn general, \\(n!\\) terms make this formula impractical for large matrices.\nStill, it gives the deepest definition of determinants, from which all other rules follow.\n\n\n\nApplications\n\nAbstract algebra: Determinant definition via permutations works over any field.\nCombinatorics: Determinants encode signed sums over permutations, connecting to permanents.\nTheoretical proofs: Many determinant properties, like multiplicativity, emerge cleanly from the permutation definition.\nLeibniz formula: Explicit but impractical formula for computation.\nAdvanced math: Determinants generalize to alternating multilinear forms in linear algebra and differential geometry.\n\n\n\nWhy It Matters\n\nProvides the most fundamental definition of determinants.\nExplains alternating signs in formulas naturally.\nBridges algebra, geometry, and combinatorics.\nShows how orientation emerges from row/column arrangements.\n\n\n\nTry It Yourself\n\nWrite out all 6 terms in the 3×3 determinant expansion and verify the sign of each permutation.\nCompute the determinant of \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) using the permutation definition.\nShow that if two columns are equal, all permutation terms cancel, giving \\(\\det(A)=0\\).\nChallenge: Prove that swapping two rows changes the sign of every permutation term, flipping the total determinant.\n\nDeterminants may look like algebraic puzzles, but the permutation formula reveals their true nature: a grand sum over all possible ways of matching rows to columns, with signs recording whether orientation is preserved or reversed.\n\n\n\n59. Cramer’s Rule\nCramer’s Rule is a classical method for solving systems of linear equations using determinants. While rarely used in large-scale computation due to inefficiency, it offers deep theoretical insights into the relationship between determinants, invertibility, and linear systems. It shows how the determinant of a matrix encodes not only volume scaling but also the exact solution to equations.\n\nThe Setup\nConsider a system of \\(n\\) linear equations with \\(n\\) unknowns:\n\\[\nAx = b,\n\\]\nwhere \\(A\\) is an invertible \\(n \\times n\\) matrix, \\(x\\) is the vector of unknowns, and \\(b\\) is the right-hand side vector.\nCramer’s Rule states:\n\\[\nx_i = \\frac{\\det(A_i)}{\\det(A)},\n\\]\nwhere \\(A_i\\) is the matrix \\(A\\) with its \\(i\\)-th column replaced by \\(b\\).\n\n\nExample: 2×2 Case\nSolve:\n\\[\n\\begin{cases}\n2x + y = 5 \\\\\nx + 3y = 7\n\\end{cases}\n\\]\nMatrix form:\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}.\n\\]\nDeterminant of \\(A\\):\n\\[\n\\det(A) = 2\\cdot 3 - 1\\cdot 1 = 5.\n\\]\n\nFor \\(x_1\\): replace first column with \\(b\\):\n\n\\[\nA_1 = \\begin{bmatrix} 5 & 1 \\\\ 7 & 3 \\end{bmatrix}, \\quad \\det(A_1) = 15 - 7 = 8.\n\\]\nSo \\(x_1 = 8/5\\).\n\nFor \\(x_2\\): replace second column with \\(b\\):\n\n\\[\nA_2 = \\begin{bmatrix} 2 & 5 \\\\ 1 & 7 \\end{bmatrix}, \\quad \\det(A_2) = 14 - 5 = 9.\n\\]\nSo \\(x_2 = 9/5\\).\nSolution: \\((x,y) = (8/5, 9/5)\\).\n\n\nWhy It Works\nSince \\(A\\) is invertible,\n\\[\nx = A^{-1}b.\n\\]\nBut recall the formula for the inverse:\n\\[\nA^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A),\n\\]\nwhere \\(\\text{adj}(A)\\) is the adjugate (transpose of the cofactor matrix). When we multiply \\(\\text{adj}(A)b\\), each component naturally becomes a determinant with one column replaced by \\(b\\). This is exactly Cramer’s Rule.\n\n\nGeometric Interpretation\n\nThe denominator \\(\\det(A)\\) represents the volume of the parallelotope spanned by the columns of \\(A\\).\nThe numerator \\(\\det(A_i)\\) represents the volume when the \\(i\\)-th column is replaced by \\(b\\).\nThe ratio tells how much of the volume contribution is aligned with the \\(i\\)-th direction, giving the solution coordinate.\n\n\n\nEfficiency and Limitations\n\nGood for small \\(n\\): Elegant for 2×2 or 3×3 systems.\nInefficient for large \\(n\\): Requires computing \\(n+1\\) determinants, each with factorial complexity if done by cofactor expansion.\nNumerical instability: Determinants can be sensitive to rounding errors.\nIn practice, Gaussian elimination or LU decomposition is far superior.\n\n\n\nApplications\n\nTheoretical proofs: Establishes uniqueness of solutions for small systems.\nGeometry: Connects solutions to ratios of volumes of parallelotopes.\nSymbolic algebra: Useful for deriving closed-form expressions.\nControl theory: Sometimes applied in proofs of controllability/observability.\n\n\n\nWhy It Matters\n\nProvides a clear formula linking determinants and solutions of linear systems.\nDemonstrates the power of determinants as more than just volume measures.\nActs as a conceptual bridge between algebraic solutions and geometric interpretations.\n\n\n\nTry It Yourself\n\nSolve \\(\\begin{cases} x + 2y = 3 \\\\ 4x + 5y = 6 \\end{cases}\\) using Cramer’s Rule.\nFor the 3×3 system with matrix \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 4 \\\\ 5 & 6 & 0 \\end{bmatrix}\\), compute \\(x_1\\) using Cramer’s Rule.\nVerify that when \\(\\det(A)=0\\), Cramer’s Rule breaks down, matching the fact that the system is either inconsistent or has infinitely many solutions.\nChallenge: Derive Cramer’s Rule from the adjugate matrix formula.\n\nCramer’s Rule is not a computational workhorse, but it elegantly ties together determinants, invertibility, and the solution of linear systems-showing how geometry, algebra, and computation meet in one neat formula.\n\n\n\n60. Computing Determinants in Practice\nDeterminants carry deep meaning, but when it comes to actual computation, the method you choose makes all the difference. For small matrices, formulas like cofactor expansion or Cramer’s Rule are manageable. For larger systems, however, these direct approaches quickly become inefficient. Practical computation relies on systematic algorithms that exploit structure-especially elimination and matrix factorizations.\n\nSmall Matrices (n ≤ 3)\n\n2×2 case:\n\\[\n\\det \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc.\n\\]\n3×3 case: Either expand by cofactors or use the “rule of Sarrus”:\n\\[\n\\det \\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix} = aei + bfg + cdh - ceg - bdi - afh.\n\\]\n\nThese formulas are compact, but do not generalize well beyond \\(3 \\times 3\\).\n\n\nLarge Matrices: Elimination and LU Decomposition\nFor \\(n &gt; 3\\), practical methods revolve around Gaussian elimination.\n\nRow Reduction:\n\nReduce \\(A\\) to an upper triangular matrix \\(U\\) using row operations.\nKeep track of operations:\n\nRow swaps → flip sign of determinant.\nRow scaling → multiply determinant by the scaling factor.\nRow replacements → no effect.\n\nOnce triangular, compute determinant as the product of diagonal entries.\n\nLU Factorization:\n\nExpress \\(A = LU\\), where \\(L\\) is lower triangular and \\(U\\) is upper triangular.\nThen \\(\\det(A) = \\det(L)\\det(U)\\).\nSince \\(L\\) has 1s on its diagonal, \\(\\det(L)=1\\), so the determinant is just the product of diagonals of \\(U\\).\n\n\nThis approach reduces the complexity to \\(O(n^3)\\), far more efficient than the factorial growth of cofactor expansion.\n\n\nNumerical Considerations\n\nFloating-Point Stability: Determinants can be very large or very small, leading to overflow or underflow in computers.\nPivoting: In practice, partial pivoting ensures stability during elimination.\nCondition Number: If a matrix is nearly singular (\\(\\det(A)\\) close to 0), computed determinants may be highly inaccurate.\n\nFor these reasons, in numerical linear algebra, determinants are rarely computed directly; instead, properties of LU or QR factorizations are used.\n\n\nDeterminant via Eigenvalues\nSince the determinant equals the product of eigenvalues,\n\\[\n\\det(A) = \\lambda_1 \\lambda_2 \\cdots \\lambda_n,\n\\]\nit can be computed by finding eigenvalues (numerically via QR iteration or other methods). This is useful when eigenvalues are already needed, but computing them just for the determinant is often more expensive than elimination.\n\n\nSpecial Matrices\n\nDiagonal or triangular matrices: Determinant is product of diagonals-fastest case.\nBlock diagonal matrices: Determinant is the product of determinants of blocks.\nSparse matrices: Exploit structure-only nonzero patterns matter.\nOrthogonal matrices: Determinant is always \\(+1\\) or \\(-1\\).\n\n\n\nApplications\n\nSystem solving: Determinants test invertibility, but actual solving uses elimination.\nComputer graphics: Determinants detect orientation flips (useful for rendering).\nOptimization: Determinants of Hessians signal curvature and stability.\nStatistics: Determinants of covariance matrices measure uncertainty volumes.\nPhysics: Determinants appear in Jacobians for change of variables in integrals.\n\n\n\nWhy It Matters\n\nDeterminants provide a global property of matrices, but computation must be efficient.\nDirect expansion is elegant but impractical.\nElimination-based methods balance theory, speed, and reliability, forming the backbone of modern computational linear algebra.\n\n\n\nTry It Yourself\n\nCompute the determinant of \\(\\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 1 & 7 \\\\ -2 & 5 & 1 \\end{bmatrix}\\) using elimination, confirming the diagonal product method.\nFor a diagonal matrix with entries \\((2, 3, -1, 5)\\), verify that the determinant is simply their product.\nUse LU decomposition to compute the determinant of a \\(3 \\times 3\\) matrix of your choice.\nChallenge: Show that determinant computation by LU requires only \\(O(n^3)\\) operations, while cofactor expansion requires \\(O(n!)\\).\n\nDeterminants are central, but in practice they are best approached with systematic algorithms, where triangular forms and factorizations reveal the answer quickly and reliably.\n\n\nClosing\nFlatness or fullness,\ndeterminants quietly weigh\ndepth in every move.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-7.-eigenvalues-eigenvectors-and-dynamics",
    "href": "books/en-US/book.html#chapter-7.-eigenvalues-eigenvectors-and-dynamics",
    "title": "The Book",
    "section": "Chapter 7. Eigenvalues, eigenvectors, and dynamics",
    "text": "Chapter 7. Eigenvalues, eigenvectors, and dynamics\n\nOpening\nStillness in motion,\ndirections that never fade,\ntime reveals its core.\n\n\n61. Eigenvalues and Eigenvectors\nAmong all the concepts in linear algebra, few are as central and powerful as eigenvalues and eigenvectors. They reveal the hidden “axes of action” of a linear transformation-directions in space where the transformation behaves in the simplest possible way. Instead of mixing and rotating everything, an eigenvector is left unchanged in direction, scaled only by its corresponding eigenvalue.\n\nThe Core Idea\nLet \\(A\\) be an \\(n \\times n\\) matrix. A nonzero vector \\(v \\in \\mathbb{R}^n\\) is called an eigenvector of \\(A\\) if\n\\[\nAv = \\lambda v,\n\\]\nfor some scalar \\(\\lambda \\in \\mathbb{R}\\) (or \\(\\mathbb{C}\\)). The scalar \\(\\lambda\\) is the eigenvalue corresponding to \\(v\\).\n\nEigenvector: A special direction that is preserved by the transformation.\nEigenvalue: The factor by which the eigenvector is stretched or compressed.\n\nIf \\(\\lambda &gt; 1\\), the eigenvector is stretched. If \\(0 &lt; \\lambda &lt; 1\\), it is compressed. If \\(\\lambda &lt; 0\\), it is flipped in direction and scaled. If \\(\\lambda = 0\\), the vector is flattened to zero.\n\n\nWhy They Matter\nEigenvalues and eigenvectors describe the intrinsic structure of a transformation:\n\nThey give preferred directions in which the action of the matrix is simplest.\nThey summarize long-term behavior of repeated applications (e.g., powers of \\(A\\)).\nThey connect algebra, geometry, and applications in physics, data science, and engineering.\n\n\n\nExample: A Simple 2D Case\nLet\n\\[\nA = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}.\n\\]\n\nApplying \\(A\\) to \\((1,0)\\):\n\\[\nA \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} = 2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n\\]\nSo \\((1,0)\\) is an eigenvector with eigenvalue \\(2\\).\nApplying \\(A\\) to \\((0,1)\\):\n\\[\nA \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix} = 3 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\]\nSo \\((0,1)\\) is an eigenvector with eigenvalue \\(3\\).\n\nHere the eigenvectors align with the coordinate axes, and the eigenvalues are the diagonal entries.\n\n\nGeneral Case: The Eigenvalue Equation\nTo find eigenvalues, we solve\n\\[\nAv = \\lambda v \\quad \\Leftrightarrow \\quad (A - \\lambda I)v = 0.\n\\]\nFor nontrivial \\(v\\), the matrix \\((A - \\lambda I)\\) must be singular:\n\\[\n\\det(A - \\lambda I) = 0.\n\\]\nThis determinant expands to the characteristic polynomial, whose roots are the eigenvalues. Eigenvectors come from solving the corresponding null spaces.\n\n\nGeometric Interpretation\n\nEigenvectors are invariant directions. When you apply \\(A\\), the vector may stretch or flip, but it does not rotate off its line.\nEigenvalues are scaling factors. They describe how much stretching, shrinking, or flipping happens along that invariant direction.\n\nFor example:\n\nIn 2D, an eigenvector might be a line through the origin where the transformation acts as a stretch.\nIn 3D, planes of shear often have eigenvectors along axes of invariance.\n\n\n\nDynamics and Repeated Applications\nOne reason eigenvalues are so important is that they describe repeated transformations:\n\\[\nA^k v = \\lambda^k v.\n\\]\nIf you apply \\(A\\) repeatedly to an eigenvector, the result is predictable: just multiply by \\(\\lambda^k\\). This explains stability in dynamical systems, growth in population models, and convergence in Markov chains.\n\nIf \\(|\\lambda| &lt; 1\\), repeated applications shrink the vector to zero.\nIf \\(|\\lambda| &gt; 1\\), the vector grows without bound.\nIf \\(\\lambda = 1\\), the vector stays the same length (though direction may flip if \\(\\lambda=-1\\)).\n\n\n\nApplications\n\nPhysics: Vibrations of molecules, quantum energy levels, and resonance all rely on eigenvalues/eigenvectors.\nData Science: Principal Component Analysis (PCA) finds eigenvectors of covariance matrices to detect key directions of variance.\nMarkov Chains: Steady-state probabilities correspond to eigenvectors with eigenvalue 1.\nDifferential Equations: Eigenvalues simplify systems of linear ODEs.\nComputer Graphics: Transformations like rotations and scalings can be analyzed with eigen-decompositions.\n\n\n\nWhy It Matters\n\nEigenvalues and eigenvectors reduce complex transformations to their simplest components.\nThey unify algebra (roots of characteristic polynomials), geometry (invariant directions), and applications (stability, resonance, variance).\nThey are the foundation for diagonalization, SVD, and spectral analysis, which dominate modern applied mathematics.\n\n\n\nTry It Yourself\n\nCompute the eigenvalues and eigenvectors of \\(\\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\\).\nFor \\(A = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\), find its eigenvalues. (Hint: they are complex.)\nTake a random 2×2 matrix and check if its eigenvectors align with coordinate axes.\nChallenge: Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent.\n\nEigenvalues and eigenvectors are the “fingerprints” of a matrix: they capture the essential behavior of a transformation, guiding us to understand stability, dynamics, and structure across countless disciplines.\n\n\n\n62. The Characteristic Polynomial\nTo uncover the eigenvalues of a matrix, we use a central tool: the characteristic polynomial. This polynomial encodes the relationship between a matrix and its eigenvalues. The roots of the polynomial are precisely the eigenvalues, making it the algebraic gateway to spectral analysis.\n\nDefinition\nFor a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), the characteristic polynomial is defined as\n\\[\np_A(\\lambda) = \\det(A - \\lambda I).\n\\]\n\n\\(I\\) is the identity matrix of the same size as \\(A\\).\nThe polynomial \\(p_A(\\lambda)\\) has degree \\(n\\).\nThe eigenvalues of \\(A\\) are exactly the roots of \\(p_A(\\lambda)\\).\n\n\n\nWhy This Works\nThe eigenvalue equation is\n\\[\nAv = \\lambda v \\quad \\iff \\quad (A - \\lambda I)v = 0.\n\\]\nFor nontrivial \\(v\\), the matrix \\(A - \\lambda I\\) must be singular:\n\\[\n\\det(A - \\lambda I) = 0.\n\\]\nThus, eigenvalues are precisely the scalars \\(\\lambda\\) for which the determinant vanishes.\n\n\nExample: 2×2 Case\nLet\n\\[\nA = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}.\n\\]\nCompute:\n\\[\np_A(\\lambda) = \\det \\begin{bmatrix} 4-\\lambda & 2 \\\\ 1 & 3-\\lambda \\end{bmatrix}.\n\\]\nExpanding:\n\\[\np_A(\\lambda) = (4-\\lambda)(3-\\lambda) - 2.\n\\]\n\\[\n= \\lambda^2 - 7\\lambda + 10.\n\\]\nThe roots are \\(\\lambda = 5\\) and \\(\\lambda = 2\\). These are the eigenvalues of \\(A\\).\n\n\nExample: 3×3 Case\nFor\n\\[\nB = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix},\n\\]\n\\[\np_B(\\lambda) = \\det \\begin{bmatrix} 2-\\lambda & 0 & 0 \\\\ 0 & 3-\\lambda & 4 \\\\ 0 & 4 & 9-\\lambda \\end{bmatrix}.\n\\]\nExpand:\n\\[\np_B(\\lambda) = (2-\\lambda)\\big[(3-\\lambda)(9-\\lambda) - 16\\big].\n\\]\n\\[\n= (2-\\lambda)(\\lambda^2 - 12\\lambda + 11).\n\\]\nRoots: \\(\\lambda = 2, 1, 11\\).\n\n\nProperties of the Characteristic Polynomial\n\nDegree: Always degree \\(n\\).\nLeading term: \\((-1)^n \\lambda^n\\).\nConstant term: \\(\\det(A)\\).\nCoefficient of \\(\\lambda^{n-1}\\): \\(-\\text{tr}(A)\\), where \\(\\text{tr}(A)\\) is the trace (sum of diagonal entries).\n\nSo:\n\\[\np_A(\\lambda) = (-1)^n \\lambda^n + (\\text{tr}(A))(-1)^{n-1}\\lambda^{n-1} + \\cdots + \\det(A).\n\\]\nThis ties together trace, determinant, and eigenvalues in one polynomial.\n\n\nGeometric Meaning\n\nThe roots of the characteristic polynomial tell us scaling factors along invariant directions.\nIn 2D: the polynomial encodes area scaling (\\(\\det(A)\\)) and total stretching (\\(\\text{tr}(A)\\)).\nIn higher dimensions: it condenses the complexity of \\(A\\) into a single equation whose solutions reveal the spectrum.\n\n\n\nApplications\n\nEigenvalue computation: Foundation for diagonalization and spectral theory.\nControl theory: Stability of systems depends on eigenvalues (roots of the characteristic polynomial).\nDifferential equations: Characteristic polynomials describe natural frequencies and modes of oscillation.\nGraph theory: The characteristic polynomial of an adjacency matrix encodes structural properties of the graph.\nQuantum mechanics: Energy levels of quantum systems come from solving characteristic polynomials of operators.\n\n\n\nWhy It Matters\n\nProvides a systematic, algebraic way to find eigenvalues.\nConnects trace and determinant to deeper spectral properties.\nBridges linear algebra, polynomial theory, and geometry.\nForms the foundation for modern computational methods like QR iteration.\n\n\n\nTry It Yourself\n\nCompute the characteristic polynomial of \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 2 \\end{bmatrix}\\). Find its eigenvalues.\nVerify that the product of eigenvalues equals the determinant.\nVerify that the sum of eigenvalues equals the trace.\nChallenge: Prove that \\(p_{AB}(\\lambda) = p_{BA}(\\lambda)\\) for any \\(A, B\\) of the same size.\n\nThe characteristic polynomial distills a matrix into a single algebraic object whose roots reveal the essential dynamics of the transformation.\n\n\n\n63. Algebraic vs. Geometric Multiplicity\nWhen studying eigenvalues, it’s not enough to just find the roots of the characteristic polynomial. Each eigenvalue can appear multiple times, and this “multiplicity” can be understood in two distinct but related ways: algebraic multiplicity (how many times it appears as a root) and geometric multiplicity (the dimension of its eigenspace). These two multiplicities capture both the algebraic and geometric richness of eigenvalues.\n\nAlgebraic Multiplicity\nThe algebraic multiplicity (AM) of an eigenvalue \\(\\lambda\\) is the number of times it appears as a root of the characteristic polynomial \\(p_A(\\lambda)\\).\n\nIf \\((\\lambda - \\lambda_0)^k\\) divides \\(p_A(\\lambda)\\), then the algebraic multiplicity of \\(\\lambda_0\\) is \\(k\\).\nThe sum of all algebraic multiplicities equals the size of the matrix (\\(n\\)).\n\nExample: If\n\\[\np_A(\\lambda) = (\\lambda-2)^3(\\lambda+1)^2,\n\\]\nthen eigenvalue \\(\\lambda=2\\) has AM = 3, and \\(\\lambda=-1\\) has AM = 2.\n\n\nGeometric Multiplicity\nThe geometric multiplicity (GM) of an eigenvalue \\(\\lambda\\) is the dimension of the eigenspace corresponding to \\(\\lambda\\):\n\\[\n\\text{GM}(\\lambda) = \\dim(\\ker(A - \\lambda I)).\n\\]\n\nThis counts how many linearly independent eigenvectors correspond to \\(\\lambda\\).\nAlways satisfies:\n\\[\n1 \\leq \\text{GM}(\\lambda) \\leq \\text{AM}(\\lambda).\n\\]\n\nExample: If\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix},\n\\]\nthen \\(p_A(\\lambda) = (\\lambda-2)^2\\).\n\nAM of \\(\\lambda=2\\) is 2.\nSolve \\((A-2I)v=0\\):\n\\[\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} v = 0 \\quad \\Rightarrow \\quad v = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n\\]\nOnly 1 independent eigenvector.\nGM of \\(\\lambda=2\\) is 1.\n\n\n\nRelationship Between the Two\n\nAlways: \\(\\text{GM}(\\lambda) \\leq \\text{AM}(\\lambda)\\).\nIf they are equal for all eigenvalues, the matrix is diagonalizable.\nIf GM &lt; AM for some eigenvalue, the matrix is defective, meaning it cannot be diagonalized, though it may still have a Jordan canonical form.\n\n\n\nGeometric Meaning\n\nAM measures how strongly the eigenvalue is “encoded” in the polynomial.\nGM measures how much geometric freedom the eigenvalue’s eigenspace provides.\nIf AM &gt; GM, the eigenvalue “wants” more independent directions than the space allows.\n\nThink of AM as the theoretical demand for eigenvectors, and GM as the actual supply.\n\n\nExample: Diagonalizable vs. Defective\n\nDiagonalizable case:\n\\[\nB = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\n\n\\(p_B(\\lambda) = (\\lambda-2)^2\\).\nAM = 2 for eigenvalue 2.\nGM = 2, since the eigenspace is all of \\(\\mathbb{R}^2\\).\nEnough eigenvectors to diagonalize.\n\nDefective case: The earlier example\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nhad AM = 2, GM = 1.\n\nNot enough eigenvectors.\nCannot be diagonalized.\n\n\n\n\nApplications\n\nDiagonalization: Only possible when GM = AM for all eigenvalues.\nJordan form: Defective matrices require Jordan blocks, governed by the gap between AM and GM.\nDifferential equations: The solution form depends on multiplicity; repeated eigenvalues with fewer eigenvectors require generalized solutions.\nStability analysis: Multiplicities reveal degeneracies in dynamical systems.\nQuantum mechanics: Degeneracy of eigenvalues (AM vs. GM) encodes physical symmetry.\n\n\n\nWhy It Matters\n\nMultiplicities separate algebraic roots from geometric structure.\nThey decide whether diagonalization is possible.\nThey reveal hidden constraints in systems with repeated eigenvalues.\nThey form the basis for advanced concepts like Jordan canonical form and generalized eigenvectors.\n\n\n\nTry It Yourself\n\nFind AM and GM for \\(\\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}\\).\nFind AM and GM for \\(\\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix}\\). Compare with the first case.\nShow that AM always equals the multiplicity of a root of the characteristic polynomial.\nChallenge: Prove that for any eigenvalue, GM ≥ 1.\n\nAlgebraic and geometric multiplicity together tell the full story: the algebra tells us how many times an eigenvalue appears, while the geometry tells us how much room it really occupies in the vector space.\n\n\n\n64. Diagonalization\nDiagonalization is one of the most powerful ideas in linear algebra. It takes a complicated matrix and, when possible, rewrites it in a simple form where its action is completely transparent. A diagonal matrix is easy to understand: it just stretches or compresses each coordinate axis by a fixed factor. If we can transform a matrix into a diagonal one, many calculations-like computing powers or exponentials-become almost trivial.\n\nThe Core Concept\nA square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is diagonalizable if there exists an invertible matrix \\(P\\) and a diagonal matrix \\(D\\) such that\n\\[\nA = P D P^{-1}.\n\\]\n\nThe diagonal entries of \\(D\\) are the eigenvalues of \\(A\\).\nThe columns of \\(P\\) are the corresponding eigenvectors.\n\nIn words: \\(A\\) can be “rewritten” in a coordinate system made of its eigenvectors, where its action reduces to simple scaling along independent directions.\n\n\nWhy Diagonalization Matters\n\nSimplifies Computations:\n\nComputing powers:\n\\[\nA^k = P D^k P^{-1}, \\quad D^k \\text{ is trivial to compute}.\n\\]\nMatrix exponential:\n\\[\ne^A = P e^D P^{-1}.\n\\]\nCritical in solving differential equations.\n\nClarifies Dynamics:\n\nLong-term behavior of iterative processes depends directly on eigenvalues.\nStable vs. unstable systems can be read off from \\(D\\).\n\nReveals Structure:\n\nTells us whether the system can be understood through independent modes.\nConnects algebraic structure with geometry.\n\n\n\n\nConditions for Diagonalization\nA matrix \\(A\\) is diagonalizable if and only if it has enough linearly independent eigenvectors to form a basis for \\(\\mathbb{R}^n\\).\n\nEquivalently: For each eigenvalue, geometric multiplicity = algebraic multiplicity.\nDistinct eigenvalues guarantee diagonalizability, since their eigenvectors are linearly independent.\n\n\n\nExample: Diagonalizable Case\nLet\n\\[\nA = \\begin{bmatrix} 4 & 0 \\\\ 1 & 3 \\end{bmatrix}.\n\\]\n\nCharacteristic polynomial:\n\\[\np_A(\\lambda) = (4-\\lambda)(3-\\lambda).\n\\]\nEigenvalues: \\(\\lambda_1=4, \\lambda_2=3\\).\nEigenvectors:\n\nFor \\(\\lambda=4\\): \\((1,1)^T\\).\nFor \\(\\lambda=3\\): \\((0,1)^T\\).\n\nBuild \\(P = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\), \\(D = \\begin{bmatrix} 4 & 0 \\\\ 0 & 3 \\end{bmatrix}\\).\nThen \\(A = P D P^{-1}\\).\n\nNow, computing \\(A^{10}\\) is easy: just compute \\(D^{10}\\) and conjugate.\n\n\nExample: Defective (Non-Diagonalizable) Case\n\\[\nB = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\n\nCharacteristic polynomial: \\((\\lambda - 2)^2\\).\nAM of eigenvalue 2 is 2, but GM = 1 (only one eigenvector).\nNot diagonalizable. Needs Jordan form instead.\n\n\n\nGeometric Meaning\nDiagonalization means we can rotate into a basis of eigenvectors where the transformation acts simply: scale each axis by its eigenvalue.\n\nThink of a room where the floor stretches more in one direction than another. In the right coordinate system (aligned with eigenvectors), the stretch is purely along axes.\nWithout diagonalization, stretching mixes directions and is harder to describe.\n\n\n\nApplications\n\nDifferential Equations: Solving systems of linear ODEs relies on diagonalization or Jordan form.\nMarkov Chains: Transition matrices are analyzed through diagonalization to study steady states.\nQuantum Mechanics: Operators are diagonalized to reveal measurable states.\nPCA (Principal Component Analysis): A covariance matrix is diagonalized to extract independent variance directions.\nComputer Graphics: Diagonalization simplifies rotation-scaling transformations.\n\n\n\nWhy It Matters\nDiagonalization transforms complexity into simplicity. It exposes the fundamental action of a matrix: scaling along preferred axes. Without it, understanding or computing repeated transformations would be intractable.\n\n\nTry It Yourself\n\nDiagonalize\n\\[\nC = \\begin{bmatrix} 1 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\nCompute \\(C^5\\) using \\(P D^5 P^{-1}\\).\nShow why\n\\[\n\\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\ncannot be diagonalized.\nChallenge: Prove that any symmetric real matrix is diagonalizable with an orthogonal basis.\n\nDiagonalization is like finding the natural “language” of a matrix: once we listen in its native basis, everything becomes clear, elegant, and simple.\n\n\n\n65. Powers of a Matrix\nOnce we know about diagonalization, one of its most powerful consequences is the ability to compute powers of a matrix efficiently. Normally, multiplying a matrix by itself repeatedly is expensive and messy. But if a matrix can be diagonalized, its powers become almost trivial to calculate. This is crucial in understanding long-term behavior of dynamical systems, Markov chains, and iterative algorithms.\n\nThe General Principle\nIf a matrix \\(A\\) is diagonalizable, then\n\\[\nA = P D P^{-1},\n\\]\nwhere \\(D\\) is diagonal and \\(P\\) is invertible.\nThen for any positive integer \\(k\\):\n\\[\nA^k = (P D P^{-1})^k = P D^k P^{-1}.\n\\]\nBecause \\(P^{-1}P = I\\), the middle terms cancel out in the product.\n\nComputing \\(D^k\\) is simple: just raise each diagonal entry to the \\(k\\)-th power.\nThus, eigenvalues control the growth or decay of powers of the matrix.\n\n\n\nExample: A Simple Diagonal Case\nLet\n\\[\nD = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}.\n\\]\nThen\n\\[\nD^k = \\begin{bmatrix} 2^k & 0 \\\\ 0 & 3^k \\end{bmatrix}.\n\\]\nEach eigenvalue is raised independently to the \\(k\\)-th power.\n\n\nExample: Using Diagonalization\nConsider\n\\[\nA = \\begin{bmatrix} 4 & 0 \\\\ 1 & 3 \\end{bmatrix}.\n\\]\nFrom before, we know it diagonalizes as\n\\[\nA = P D P^{-1}, \\quad D = \\begin{bmatrix} 4 & 0 \\\\ 0 & 3 \\end{bmatrix}.\n\\]\nSo,\n\\[\nA^k = P \\begin{bmatrix} 4^k & 0 \\\\ 0 & 3^k \\end{bmatrix} P^{-1}.\n\\]\nInstead of multiplying \\(A\\) by itself \\(k\\) times, we just exponentiate the eigenvalues.\n\n\nLong-Term Behavior\nEigenvalues reveal exactly what happens as \\(k \\to \\infty\\).\n\nIf all eigenvalues satisfy \\(|\\lambda| &lt; 1\\), then \\(A^k \\to 0\\).\nIf some eigenvalues have \\(|\\lambda| &gt; 1\\), then \\(A^k\\) diverges along those eigenvector directions.\nIf \\(|\\lambda| = 1\\), the behavior depends on the specific structure: it may oscillate, stabilize, or remain bounded.\n\nThis explains stability in recursive systems and iterative algorithms.\n\n\nSpecial Case: Markov Chains\nIn probability, the transition matrix of a Markov chain has eigenvalues less than or equal to 1.\n\nThe largest eigenvalue is always \\(\\lambda = 1\\).\nAs powers of the transition matrix grow, the chain converges to the eigenvector associated with \\(\\lambda = 1\\), representing the stationary distribution.\n\nThus, \\(A^k\\) describes the long-run behavior of the chain.\n\n\nNon-Diagonalizable Matrices\nIf a matrix is not diagonalizable, things become more complicated. Such matrices require the Jordan canonical form, where blocks can lead to terms like \\(k \\lambda^{k-1}\\).\nExample:\n\\[\nB = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\nThen\n\\[\nB^k = \\begin{bmatrix} 2^k & k 2^{k-1} \\\\ 0 & 2^k \\end{bmatrix}.\n\\]\nThe presence of the off-diagonal entry introduces linear growth in \\(k\\), in addition to exponential scaling.\n\n\nGeometric Meaning\n\nPowers of \\(A\\) correspond to repeated application of the linear transformation.\nEigenvalues dictate whether directions expand, shrink, or remain steady.\nThe eigenvectors mark the axes along which the repeated action is simplest to describe.\n\nThink of stretching a rubber sheet: after each stretch, the sheet aligns more and more strongly with the dominant eigenvector.\n\n\nApplications\n\nDynamical Systems: Population models, economic growth, and iterative algorithms all rely on powers of a matrix.\nMarkov Chains: Powers reveal equilibrium behavior and mixing rates.\nDifferential Equations: Discrete-time models use matrix powers to describe state evolution.\nComputer Graphics: Repeated transformations can be analyzed via eigenvalues.\nMachine Learning: Convergence of iterative solvers (like gradient descent with linear updates) depends on spectral radius.\n\n\n\nWhy It Matters\nMatrix powers are the foundation of stability analysis, asymptotic behavior, and convergence. Diagonalization turns this from a brute-force multiplication into a deep, structured understanding.\n\n\nTry It Yourself\n\nCompute \\(A^5\\) for \\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\\).\nFor \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\), compute \\(A^k\\). What happens as \\(k \\to \\infty\\)?\nExplore what happens to \\(A^k\\) when the largest eigenvalue has absolute value &lt; 1, = 1, and &gt; 1.\nChallenge: Show that if a diagonalizable matrix has eigenvalues \\(|\\lambda_i| &lt; 1\\), then \\(\\lim_{k \\to \\infty} A^k = 0\\).\n\nPowers of a matrix reveal the story of repetition: how a transformation evolves when applied again and again. They connect linear algebra to time, growth, and stability in every system that unfolds step by step.\n\n\n\n66. Real vs. Complex Spectra\nNot all eigenvalues are real numbers. Even when working with real matrices, eigenvalues can emerge as complex numbers. Understanding when eigenvalues are real, when they are complex, and what this means geometrically is critical for grasping the full behavior of linear transformations.\n\nEigenvalues Over the Complex Numbers\nEvery square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) has at least one eigenvalue in the complex numbers. This is guaranteed by the Fundamental Theorem of Algebra, which says every polynomial (like the characteristic polynomial) has roots in \\(\\mathbb{C}\\).\n\nIf \\(p_A(\\lambda)\\) has only real roots, all eigenvalues are real.\nIf \\(p_A(\\lambda)\\) has quadratic factors with no real roots, then eigenvalues appear as complex conjugate pairs.\n\n\n\nWhy Complex Numbers Appear\nConsider a 2D rotation matrix:\n\\[\nR_\\theta = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}.\n\\]\nThe characteristic polynomial is\n\\[\np(\\lambda) = \\lambda^2 - 2\\cos\\theta \\lambda + 1.\n\\]\nThe eigenvalues are\n\\[\n\\lambda = \\cos\\theta \\pm i \\sin\\theta = e^{\\pm i\\theta}.\n\\]\n\nUnless \\(\\theta = 0, \\pi\\), these eigenvalues are not real.\nGeometrically, this makes sense: pure rotation has no invariant real direction. Instead, the eigenvalues are complex numbers of unit magnitude, encoding the rotation angle.\n\n\n\nReal vs. Complex Scenarios\n\nSymmetric Real Matrices:\n\nAll eigenvalues are real.\nEigenvectors form an orthogonal basis.\nExample: \\(\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) has eigenvalues \\(3, 1\\).\n\nGeneral Real Matrices:\n\nEigenvalues may be complex.\nIf complex, they always come in conjugate pairs: if \\(\\lambda = a+bi\\), then \\(\\overline{\\lambda} = a-bi\\) is also an eigenvalue.\n\nSkew-Symmetric Matrices:\n\nPurely imaginary eigenvalues.\nExample: \\(\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\) has eigenvalues \\(\\pm i\\).\n\n\n\n\nGeometric Meaning of Complex Eigenvalues\n\nIf eigenvalues are real, the transformation scales along real directions.\nIf eigenvalues are complex, the transformation involves a combination of rotation and scaling.\n\nFor \\(\\lambda = re^{i\\theta}\\):\n\n\\(r = |\\lambda|\\) controls expansion or contraction.\n\\(\\theta\\) controls rotation.\n\nSo a complex eigenvalue represents a spiral: stretching or shrinking while rotating.\n\n\nExample: Spiral Dynamics\nMatrix\n\\[\nA = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n\\]\nrotates vectors by 90°.\n\nEigenvalues: \\(\\pm i\\).\nMagnitude = 1, angle = \\(\\pi/2\\).\nInterpretation: every step is a rotation of 90°, with no scaling.\n\nIf we change to\n\\[\nB = \\begin{bmatrix} 0.8 & -0.6 \\\\ 0.6 & 0.8 \\end{bmatrix},\n\\]\nthe eigenvalues are complex with modulus &lt; 1.\n\nInterpretation: rotation combined with shrinking → spiraling toward the origin.\n\n\n\nApplications\n\nDifferential Equations: Complex eigenvalues produce oscillatory solutions with sine and cosine terms.\nPhysics: Vibrations and wave phenomena rely on complex eigenvalues to model periodic behavior.\nControl Systems: Stability requires checking magnitudes of eigenvalues in the complex plane.\nComputer Graphics: Rotations and spiral motions are naturally described by complex spectra.\nSignal Processing: Fourier transforms rely on complex eigenstructures of convolution operators.\n\n\n\nWhy It Matters\n\nReal eigenvalues describe pure stretching or compression.\nComplex eigenvalues describe combined rotation and scaling.\nTogether, they provide a complete picture of matrix behavior in both real and complex spaces.\nWithout considering complex eigenvalues, we miss entire classes of transformations, like rotation and oscillation.\n\n\n\nTry It Yourself\n\nFind eigenvalues of \\(\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\). Interpret geometrically.\nFor rotation by 45°, find eigenvalues of \\(\\begin{bmatrix} \\cos\\frac{\\pi}{4} & -\\sin\\frac{\\pi}{4} \\\\ \\sin\\frac{\\pi}{4} & \\cos\\frac{\\pi}{4} \\end{bmatrix}\\). Show that they are \\(e^{\\pm i\\pi/4}\\).\nCheck eigenvalues of \\(\\begin{bmatrix} 2 & -5 \\\\ 1 & -2 \\end{bmatrix}\\). Are they real or complex?\nChallenge: Prove that real polynomials of odd degree always have at least one real root. Connect this to eigenvalues of odd-dimensional real matrices.\n\nComplex spectra extend our understanding of linear algebra into the full richness of oscillations, rotations, and spirals, where numbers alone are not enough-geometry and complex analysis merge to reveal the truth.\n\n\n\n67. Defective Matrices and Jordan Form (a Glimpse)\nNot every matrix can be simplified all the way into a diagonal form. Some matrices, while having repeated eigenvalues, do not have enough independent eigenvectors to span the entire space. These are called defective matrices. Understanding them requires introducing the Jordan canonical form, a generalization of diagonalization that handles these tricky cases.\n\nDefective Matrices\nA square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is called defective if:\n\nIt has an eigenvalue \\(\\lambda\\) with algebraic multiplicity (AM) strictly larger than its geometric multiplicity (GM).\nEquivalently, \\(A\\) does not have enough linearly independent eigenvectors to form a full basis of \\(\\mathbb{R}^n\\).\n\nExample:\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\n\nCharacteristic polynomial: \\((\\lambda - 2)^2\\), so AM = 2.\nSolving \\((A - 2I)v = 0\\):\n\\[\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}v = 0 \\quad \\Rightarrow \\quad v = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n\\]\nOnly one independent eigenvector → GM = 1.\nSince GM &lt; AM, this matrix is defective.\n\nDefective matrices cannot be diagonalized.\n\n\nWhy Defective Matrices Exist\nDiagonalization requires one independent eigenvector per eigenvalue copy. But sometimes the matrix “collapses” those directions together, producing fewer eigenvectors than expected.\n\nThink of it like having multiple musical notes written in the score (AM), but fewer instruments available to play them (GM).\nThe matrix “wants” more independent directions, but the geometry of its null spaces prevents that.\n\n\n\nJordan Canonical Form (Intuition)\nWhile defective matrices cannot be diagonalized, they can still be put into a nearly diagonal form called the Jordan canonical form (JCF):\n\\[\nJ = P^{-1} A P,\n\\]\nwhere \\(J\\) consists of Jordan blocks:\n\\[\nJ_k(\\lambda) = \\begin{bmatrix}\n\\lambda & 1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda & 1 & \\cdots & 0 \\\\\n0 & 0 & \\lambda & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & 1 \\\\\n0 & 0 & 0 & \\cdots & \\lambda\n\\end{bmatrix}.\n\\]\nEach block corresponds to one eigenvalue \\(\\lambda\\), with 1s on the superdiagonal indicating the lack of independent eigenvectors.\n\nIf every block is \\(1 \\times 1\\), the matrix is diagonalizable.\nIf larger blocks appear, the matrix is defective.\n\n\n\nExample: Jordan Block of Size 2\nThe earlier defective example\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nhas Jordan form\n\\[\nJ = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\nNotice it is already in Jordan form: one block of size 2 for eigenvalue 2.\n\n\nPowers of Jordan Blocks\nA key property is how powers behave. For\n\\[\nJ = \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix},\n\\]\n\\[\nJ^k = \\begin{bmatrix} \\lambda^k & k\\lambda^{k-1} \\\\ 0 & \\lambda^k \\end{bmatrix}.\n\\]\n\nUnlike diagonal matrices, extra polynomial terms in \\(k\\) appear.\nThis explains why defective matrices produce behavior like growth proportional to \\(k \\lambda^{k-1}\\).\n\n\n\nGeometric Meaning\n\nEigenvectors describe invariant lines.\nWhen there aren’t enough eigenvectors, Jordan form encodes chains of generalized eigenvectors.\nEach chain captures how the matrix transforms vectors slightly off the invariant line, nudging them along directions linked together by the Jordan block.\n\nSo while a diagonalizable matrix decomposes space into neat independent directions, a defective matrix entangles some directions together, forcing them into chains.\n\n\nApplications\n\nDifferential Equations: Jordan blocks determine the appearance of extra polynomial factors (like \\(t e^{\\lambda t}\\)) in solutions.\nMarkov Chains: Non-diagonalizable transition matrices produce slower convergence to steady states.\nNumerical Analysis: Algorithms may fail or slow down if the system matrix is defective.\nControl Theory: Stability depends not just on eigenvalues, but on whether the matrix is diagonalizable.\nQuantum Mechanics: Degenerate eigenvalues require Jordan analysis to fully describe states.\n\n\n\nWhy It Matters\n\nDiagonalization is not always possible, and defective matrices are the exceptions.\nJordan form is the universal fallback: every square matrix has one, and it generalizes diagonalization.\nIt introduces generalized eigenvectors, which extend the reach of spectral theory.\n\n\n\nTry It Yourself\n\nVerify that \\(\\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}\\) is defective. Find its Jordan form.\nShow that for a Jordan block of size 3,\n\\[\nJ^k = \\lambda^k I + k \\lambda^{k-1} N + \\frac{k(k-1)}{2}\\lambda^{k-2} N^2,\n\\]\nwhere \\(N\\) is the nilpotent part (matrix with 1s above diagonal).\nCompare the behavior of \\(A^k\\) for a diagonalizable vs. a defective matrix with the same eigenvalues.\nChallenge: Prove that every square matrix has a Jordan form over the complex numbers.\n\nDefective matrices and Jordan form show us that even when eigenvectors are “insufficient,” we can still impose structure, capturing how linear transformations behave in their most fundamental building blocks.\n\n\n\n68. Stability and Spectral Radius\nWhen a matrix is applied repeatedly-through iteration, recursion, or dynamical systems-its long-term behavior is governed not by individual entries, but by its eigenvalues. The key measure here is the spectral radius, which tells us whether repeated applications lead to convergence, oscillation, or divergence.\n\nThe Spectral Radius\nThe spectral radius of a matrix \\(A\\) is defined as\n\\[\n\\rho(A) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}.\n\\]\n\nIt is the largest absolute value among all eigenvalues.\nIf \\(|\\lambda| &gt; 1\\), the eigenvalue leads to exponential growth along its eigenvector.\nIf \\(|\\lambda| &lt; 1\\), it leads to exponential decay.\nIf \\(|\\lambda| = 1\\), behavior depends on whether the eigenvalue is simple or defective.\n\n\n\nStability in Iterative Systems\nConsider a recursive process:\n\\[\nx_{k+1} = A x_k.\n\\]\n\nIf \\(\\rho(A) &lt; 1\\), then \\(A^k \\to 0\\) as \\(k \\to \\infty\\). All trajectories converge to the origin.\nIf \\(\\rho(A) &gt; 1\\), then \\(A^k\\) grows without bound along the dominant eigenvector.\nIf \\(\\rho(A) = 1\\), trajectories neither vanish nor diverge but may oscillate or stagnate.\n\n\n\nExample: Convergence with Small Spectral Radius\n\\[\nA = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.8 \\end{bmatrix}.\n\\]\n\nEigenvalues: \\(0.5, 0.8\\).\n\\(\\rho(A) = 0.8 &lt; 1\\).\nPowers \\(A^k\\) shrink vectors to zero → stable system.\n\n\n\nExample: Divergence with Large Spectral Radius\n\\[\nB = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0.5 \\end{bmatrix}.\n\\]\n\nEigenvalues: \\(2, 0.5\\).\n\\(\\rho(B) = 2 &gt; 1\\).\nPowers \\(B^k\\) explode along the eigenvector \\((1,0)\\).\n\n\n\nExample: Oscillation with Complex Eigenvalues\n\\[\nC = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\n\nEigenvalues: \\(\\pm i\\), both with modulus 1.\n\\(\\rho(C) = 1\\).\nSystem is neutrally stable: vectors rotate forever without shrinking or growing.\n\n\n\nBeyond Simple Stability: Defective Cases\nIf a matrix has eigenvalues with \\(|\\lambda|=1\\) and is defective, extra polynomial terms in \\(k\\) appear in \\(A^k\\), leading to slow divergence even though \\(\\rho(A)=1\\).\nExample:\n\\[\nD = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}.\n\\]\n\nEigenvalue: \\(\\lambda=1\\) (AM=2, GM=1).\n\\(\\rho(D)=1\\).\nPowers grow linearly with \\(k\\):\n\\[\nD^k = \\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix}.\n\\]\nSystem is unstable, despite spectral radius equal to 1.\n\n\n\nGeometric Meaning\nThe spectral radius measures the dominant mode of a transformation:\n\nImagine stretching and rotating a rubber sheet. After many repetitions, the sheet aligns with the direction corresponding to the largest eigenvalue.\nIf the stretching is less than 1, everything shrinks.\nIf greater than 1, everything expands.\nIf exactly 1, the system is balanced on the edge of stability.\n\n\n\nApplications\n\nNumerical Methods: Convergence of iterative solvers (e.g., Jacobi, Gauss–Seidel) depends on spectral radius &lt; 1.\nMarkov Chains: Long-term distributions exist if the largest eigenvalue = 1 and others &lt; 1 in magnitude.\nControl Theory: System stability is judged by eigenvalues inside the unit circle (\\(|\\lambda| &lt; 1\\)).\nEconomics: Input-output models remain bounded only if spectral radius &lt; 1.\nEpidemiology: Basic reproduction number \\(R_0\\) is essentially the spectral radius of a next-generation matrix.\n\n\n\nWhy It Matters\n\nThe spectral radius condenses the entire spectrum of a matrix into a single stability criterion.\nIt predicts the fate of iterative processes, from financial growth to disease spread.\nIt draws a sharp boundary between decay, balance, and explosion in linear systems.\n\n\n\nTry It Yourself\n\nCompute the spectral radius of \\(\\begin{bmatrix} 0.6 & 0.3 \\\\ 0.1 & 0.8 \\end{bmatrix}\\). Does the system converge?\nShow that for any matrix norm \\(\\|\\cdot\\|\\),\n\\[\n\\rho(A) \\leq \\|A\\|.\n\\]\n(Hint: use Gelfand’s formula.)\nFor \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\), explain why it diverges even though \\(\\rho=1\\).\nChallenge: Prove Gelfand’s formula:\n\\[\n\\rho(A) = \\lim_{k\\to\\infty} \\|A^k\\|^{1/k}.\n\\]\n\nThe spectral radius is the compass of linear dynamics: it points to stability, oscillation, or divergence, guiding us across disciplines wherever repeated transformations shape the future.\n\n\n\n69. Markov Chains and Steady States\nMarkov chains are one of the most direct and beautiful applications of eigenvalues in probability and statistics. They describe systems that evolve step by step, where the next state depends only on the current one, not on the past. The mathematics of steady states-the long-term behavior of such chains-rests firmly on eigenvalues and eigenvectors of the transition matrix.\n\nTransition Matrices\nA Markov chain is defined by a transition matrix \\(P \\in \\mathbb{R}^{n \\times n}\\) with the following properties:\n\nAll entries are nonnegative: \\(p_{ij} \\geq 0\\).\nEach row sums to 1: \\(\\sum_j p_{ij} = 1\\).\n\nIf the chain is in state \\(i\\) at time \\(k\\), then \\(p_{ij}\\) is the probability of moving to state \\(j\\) at time \\(k+1\\).\n\n\nEvolution of States\nIf the probability distribution at time \\(k\\) is a row vector \\(\\pi^{(k)}\\), then\n\\[\n\\pi^{(k+1)} = \\pi^{(k)} P.\n\\]\nAfter \\(k\\) steps:\n\\[\n\\pi^{(k)} = \\pi^{(0)} P^k.\n\\]\nSo understanding the long-term behavior requires analyzing \\(P^k\\).\n\n\nEigenvalue Structure of Transition Matrices\n\nEvery transition matrix \\(P\\) has eigenvalue \\(\\lambda = 1\\).\nAll other eigenvalues satisfy \\(|\\lambda| \\leq 1\\).\nIf the chain is irreducible (all states communicate) and aperiodic (no cyclic locking), then:\n\n\\(\\lambda=1\\) is a simple eigenvalue (AM=GM=1).\nAll other eigenvalues have magnitude strictly less than 1.\n\n\nThis ensures convergence to a unique steady state.\n\n\nSteady States as Eigenvectors\nA steady state distribution \\(\\pi\\) satisfies:\n\\[\n\\pi = \\pi P.\n\\]\nThis is equivalent to:\n\\[\n\\pi^T \\text{ is a right eigenvector of } P^T \\text{ with eigenvalue } 1.\n\\]\n\nThe steady state vector lies in the eigenspace of eigenvalue 1.\nSince probabilities must sum to 1, normalization gives a unique steady state.\n\n\n\nExample: A 2-State Markov Chain\n\\[\nP = \\begin{bmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{bmatrix}.\n\\]\n\nEigenvalues: solve \\(\\det(P-\\lambda I) = 0\\).\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 0.3.\n\\]\nThe steady state is found from \\(\\pi = \\pi P\\):\n\\[\n\\pi = \\bigg(\\frac{4}{7}, \\frac{3}{7}\\bigg).\n\\]\nAs \\(k \\to \\infty\\), any initial distribution \\(\\pi^{(0)}\\) converges to this steady state.\n\n\n\nExample: Random Walk on a Graph\nTake a simple graph: 3 nodes in a line, where each node passes to neighbors equally.\nTransition matrix:\n\\[\nP = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0.5 & 0 & 0.5 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n\\]\n\nEigenvalues: \\(\\{1, 0, -1\\}\\).\nThe steady state corresponds to eigenvalue 1.\nAfter many steps, the distribution converges to \\((0.25, 0.5, 0.25)\\).\n\n\n\nGeometric Meaning\n\nEigenvalue 1: the fixed “direction” of probabilities that does not change under transitions.\nEigenvalues &lt; 1 in magnitude: transient modes that vanish as \\(k \\to \\infty\\).\nThe dominant eigenvector (steady state) is like the “center of gravity” of the system.\n\nSo powers of \\(P\\) filter out all but the eigenvector of eigenvalue 1.\n\n\nApplications\n\nGoogle PageRank: Steady state eigenvectors rank webpages.\nEconomics: Input-output models evolve like Markov chains.\nEpidemiology: Spread of diseases can be modeled as Markov processes.\nMachine Learning: Hidden Markov models (HMMs) underpin speech recognition and bioinformatics.\nQueuing Theory: Customer arrivals and service evolve according to Markov dynamics.\n\n\n\nWhy It Matters\n\nThe concept of steady states shows how randomness can lead to predictability.\nEigenvalues explain why convergence happens, and at what rate.\nThe link between linear algebra and probability provides one of the clearest real-world uses of eigenvectors.\n\n\n\nTry It Yourself\n\nFor\n\\[\nP = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{bmatrix},\n\\]\ncompute its eigenvalues and steady state.\nShow that for any transition matrix, the largest eigenvalue is always 1.\nProve that if a chain is irreducible and aperiodic, the steady state is unique.\nChallenge: Construct a 3-state transition matrix with a cycle (periodic) and show why it doesn’t converge to a steady distribution until perturbed.\n\nMarkov chains and steady states are the meeting point of probability and linear algebra: randomness, when multiplied many times, is tamed by the calm persistence of eigenvalue 1.\n\n\n\n70. Linear Differential Systems\nMany natural and engineered processes evolve continuously over time. When these processes can be expressed as linear relationships, they lead to systems of linear differential equations. The analysis of such systems relies almost entirely on eigenvalues and eigenvectors, which determine the behavior of solutions: whether they oscillate, decay, grow, or stabilize.\n\nThe General Setup\nConsider a system of first-order linear differential equations:\n\\[\n\\frac{d}{dt}x(t) = A x(t),\n\\]\nwhere:\n\n\\(x(t) \\in \\mathbb{R}^n\\) is the state vector at time \\(t\\).\n\\(A \\in \\mathbb{R}^{n \\times n}\\) is a constant coefficient matrix.\n\nThe task is to solve for \\(x(t)\\), given an initial state \\(x(0)\\).\n\n\nThe Matrix Exponential\nThe formal solution is:\n\\[\nx(t) = e^{At} x(0),\n\\]\nwhere \\(e^{At}\\) is the matrix exponential defined as:\n\\[\ne^{At} = I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots.\n\\]\nBut how do we compute \\(e^{At}\\) in practice? The answer comes from diagonalization and Jordan form.\n\n\nCase 1: Diagonalizable Matrices\nIf \\(A\\) is diagonalizable:\n\\[\nA = P D P^{-1}, \\quad D = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n).\n\\]\nThen:\n\\[\ne^{At} = P e^{Dt} P^{-1}, \\quad e^{Dt} = \\text{diag}(e^{\\lambda_1 t}, \\ldots, e^{\\lambda_n t}).\n\\]\nThus the solution is:\n\\[\nx(t) = P \\begin{bmatrix} e^{\\lambda_1 t} & & \\\\ & \\ddots & \\\\ & & e^{\\lambda_n t} \\end{bmatrix} P^{-1} x(0).\n\\]\nEach eigenvalue \\(\\lambda_i\\) dictates the time behavior along its eigenvector direction.\n\n\nCase 2: Non-Diagonalizable Matrices\nIf \\(A\\) is defective, use its Jordan form \\(J = P^{-1}AP\\):\n\\[\ne^{At} = P e^{Jt} P^{-1}.\n\\]\nFor a Jordan block of size 2:\n\\[\nJ = \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}, \\quad\ne^{Jt} = e^{\\lambda t} \\begin{bmatrix} 1 & t \\\\ 0 & 1 \\end{bmatrix}.\n\\]\nPolynomial terms in \\(t\\) appear, multiplying the exponential part. This explains why repeated eigenvalues with insufficient eigenvectors yield solutions with extra polynomial factors.\n\n\nReal vs. Complex Eigenvalues\n\nReal eigenvalues: solutions grow or decay exponentially along eigenvector directions.\n\nIf \\(\\lambda &lt; 0\\): exponential decay → stability.\nIf \\(\\lambda &gt; 0\\): exponential growth → instability.\n\nComplex eigenvalues: \\(\\lambda = a \\pm bi\\). Solutions involve oscillations:\n\\[\ne^{(a+bi)t} = e^{at}(\\cos(bt) + i \\sin(bt)).\n\\]\n\nIf \\(a &lt; 0\\): decaying oscillations.\nIf \\(a &gt; 0\\): growing oscillations.\nIf \\(a = 0\\): pure oscillations, neutrally stable.\n\n\n\n\nExample 1: Real Eigenvalues\n\\[\nA = \\begin{bmatrix} -2 & 0 \\\\ 0 & -3 \\end{bmatrix}.\n\\]\nEigenvalues: \\(-2, -3\\). Solution:\n\\[\nx(t) = \\begin{bmatrix} c_1 e^{-2t} \\\\ c_2 e^{-3t} \\end{bmatrix}.\n\\]\nBoth terms decay → stable equilibrium at the origin.\n\n\nExample 2: Complex Eigenvalues\n\\[\nA = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}.\n\\]\nEigenvalues: \\(\\pm i\\). Solution:\n\\[\nx(t) = c_1 \\begin{bmatrix} \\cos t \\\\ \\sin t \\end{bmatrix} + c_2 \\begin{bmatrix} -\\sin t \\\\ \\cos t \\end{bmatrix}.\n\\]\nPure oscillation → circular motion around the origin.\n\n\nExample 3: Mixed Stability\n\\[\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & -2 \\end{bmatrix}.\n\\]\nEigenvalues: \\(1, -2\\). Solution:\n\\[\nx(t) = \\begin{bmatrix} c_1 e^t \\\\ c_2 e^{-2t} \\end{bmatrix}.\n\\]\nOne direction grows, one decays → unstable overall, since divergence in one direction dominates.\n\n\nGeometric Meaning\n\nThe eigenvectors form the “axes of flow” of the system.\nThe eigenvalues determine whether the flow along those axes spirals, grows, or shrinks.\nThe phase portrait of the system-trajectories in the plane-is shaped by this interplay.\n\nFor example:\n\nNegative eigenvalues → trajectories funnel into the origin.\nPositive eigenvalues → trajectories repel outward.\nComplex eigenvalues → spirals or circles.\n\n\n\nApplications\n\nControl theory: Stability analysis of systems requires eigenvalue placement in the left-half plane.\nPhysics: Vibrations, quantum oscillations, and decay processes all follow eigenvalue rules.\nBiology: Population models evolve according to linear differential equations.\nEconomics: Linear models of markets converge or diverge depending on eigenvalues.\nNeuroscience: Neural firing dynamics can be modeled as linear ODE systems.\n\n\n\nWhy It Matters\n\nLinear differential systems bridge linear algebra with real-world dynamics.\nEigenvalues determine not just numbers, but behaviors over time: growth, decay, oscillation, or equilibrium.\nThey provide the foundation for analyzing nonlinear systems, which are often studied by linearizing around equilibrium points.\n\n\n\nTry It Yourself\n\nSolve \\(\\frac{dx}{dt} = \\begin{bmatrix} -1 & 2 \\\\ -2 & -1 \\end{bmatrix}x\\). Interpret the solution.\nFor \\(A = \\begin{bmatrix} 0 & -2 \\\\ 2 & 0 \\end{bmatrix}\\), compute eigenvalues and describe the motion.\nVerify that \\(e^{At} = P e^{Dt} P^{-1}\\) works when \\(A\\) is diagonalizable.\nChallenge: Show that if all eigenvalues of \\(A\\) have negative real parts, then \\(\\lim_{t \\to \\infty} x(t) = 0\\) for any initial condition.\n\nLinear differential systems show how eigenvalues control the flow of time itself in models. They explain why some processes die out, others oscillate, and others grow without bound-providing the mathematical skeleton behind countless real-world phenomena.\n\n\nClosing\nSpectra guide the flow,\ngrowth and decay intertwining,\nfuture sings through roots.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-8.-orthogonality-least-squars-and-qr",
    "href": "books/en-US/book.html#chapter-8.-orthogonality-least-squars-and-qr",
    "title": "The Book",
    "section": "Chapter 8. Orthogonality, least squars, and QR",
    "text": "Chapter 8. Orthogonality, least squars, and QR\n\nOpening\nPerpendiculars,\nmeeting without crossing paths,\nbalance in silence.\n\n\n71. Inner Products Beyond Dot Product\nThe dot product is the first inner product most students encounter. In \\(\\mathbb{R}^n\\), it is defined as\n\\[\n\\langle x, y \\rangle = x \\cdot y = \\sum_{i=1}^n x_i y_i,\n\\]\nand it provides a way to measure length, angle, and orthogonality. But the dot product is just one special case of a much broader concept. Inner products generalize the dot product, extending its geometric intuition to more abstract vector spaces.\n\nDefinition of an Inner Product\nAn inner product on a real vector space \\(V\\) is a function\n\\[\n\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\n\\]\nthat satisfies the following axioms for all \\(x,y,z \\in V\\) and scalar \\(\\alpha \\in \\mathbb{R}\\):\n\nPositivity: \\(\\langle x, x \\rangle \\geq 0\\), and \\(\\langle x, x \\rangle = 0 \\iff x=0\\).\nSymmetry: \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\).\nLinearity in the first argument: \\(\\langle \\alpha x + y, z \\rangle = \\alpha \\langle x, z \\rangle + \\langle y, z \\rangle\\).\n\nIn complex vector spaces, the symmetry condition changes to conjugate symmetry: \\(\\langle x, y \\rangle = \\overline{\\langle y, x \\rangle}\\).\n\n\nNorms and Angles from Inner Products\nOnce an inner product is defined, we immediately get:\n\nNorm (length): \\(\\|x\\| = \\sqrt{\\langle x, x \\rangle}\\).\nDistance: \\(d(x,y) = \\|x-y\\|\\).\nAngle between vectors: \\(\\cos \\theta = \\frac{\\langle x, y \\rangle}{\\|x\\|\\|y\\|}\\).\n\nThus, inner products generalize the familiar geometry of \\(\\mathbb{R}^n\\) to broader contexts.\n\n\nExamples Beyond the Dot Product\n\nWeighted Inner Product (in \\(\\mathbb{R}^n\\)):\n\\[\n\\langle x, y \\rangle_W = x^T W y,\n\\]\nwhere \\(W\\) is a symmetric positive definite matrix.\n\nHere, lengths and angles depend on the weights encoded in \\(W\\).\nUseful when some dimensions are more important than others (e.g., weighted least squares).\n\nFunction Spaces (continuous inner product): On \\(V = C[a,b]\\), the space of continuous functions on \\([a,b]\\):\n\\[\n\\langle f, g \\rangle = \\int_a^b f(t) g(t) \\, dt.\n\\]\n\nLength: \\(\\|f\\| = \\sqrt{\\int_a^b f(t)^2 dt}\\).\nOrthogonality: \\(f\\) and \\(g\\) are orthogonal if their integral product is zero.\nThis inner product underpins Fourier series.\n\nComplex Inner Product (in \\(\\mathbb{C}^n\\)):\n\\[\n\\langle x, y \\rangle = \\sum_{i=1}^n x_i \\overline{y_i}.\n\\]\n\nConjugation ensures positivity.\nCritical for quantum mechanics, where states are vectors in complex Hilbert spaces.\n\nPolynomial Spaces: For polynomials on \\([-1,1]\\):\n\\[\n\\langle p, q \\rangle = \\int_{-1}^1 p(x) q(x) \\, dx.\n\\]\n\nLeads to orthogonal polynomials (Legendre, Chebyshev), fundamental in approximation theory.\n\n\n\n\nGeometric Interpretation\n\nInner products reshape geometry. Instead of measuring lengths and angles with the Euclidean metric, we measure them with the metric induced by the chosen inner product.\nDifferent inner products create different geometries on the same vector space.\n\nExample: A weighted inner product distorts circles into ellipses, changing which vectors count as “orthogonal.”\n\n\nApplications\n\nSignal Processing: Correlation between signals is an inner product. Orthogonality means two signals carry independent information.\nFourier Analysis: Fourier coefficients come from inner products with sine and cosine functions.\nMachine Learning: Kernel methods generalize inner products to infinite-dimensional spaces.\nQuantum Mechanics: Probabilities are squared magnitudes of complex inner products.\nOptimization: Weighted least squares problems use weighted inner products.\n\n\n\nWhy It Matters\n\nInner products generalize geometry to new contexts: weighted spaces, functions, polynomials, quantum states.\nThey provide the foundation for defining orthogonality, projections, and orthonormal bases in spaces far beyond \\(\\mathbb{R}^n\\).\nThey unify ideas across pure mathematics, physics, engineering, and computer science.\n\n\n\nTry It Yourself\n\nShow that the weighted inner product \\(\\langle x, y \\rangle_W = x^T W y\\) satisfies the inner product axioms if \\(W\\) is positive definite.\nCompute \\(\\langle f, g \\rangle = \\int_0^\\pi \\sin(t)\\cos(t)\\, dt\\). Are \\(f=\\sin\\) and \\(g=\\cos\\) orthogonal?\nIn \\(\\mathbb{C}^2\\), verify that \\(\\langle (1,i), (i,1) \\rangle = 0\\). What does this mean geometrically?\nChallenge: Prove that every inner product induces a norm, and that different inner products can lead to different geometries on the same space.\n\nThe dot product is just the beginning. Inner products provide the language to extend geometry into weighted spaces, continuous functions, and infinite dimensions-transforming how we measure similarity, distance, and structure across mathematics and science.\n\n\n\n72. Orthogonality and Orthonormal Bases\nOrthogonality is one of the most powerful ideas in linear algebra. It generalizes the familiar concept of perpendicularity in Euclidean space to abstract vector spaces equipped with an inner product. When orthogonality is combined with normalization (making vectors have unit length), we obtain orthonormal bases, which simplify computations, clarify geometry, and underpin many algorithms.\n\nOrthogonality\nTwo vectors \\(x, y \\in V\\) are orthogonal if\n\\[\n\\langle x, y \\rangle = 0.\n\\]\n\nIn \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\), this means the vectors are perpendicular.\nIn function spaces, it means the integral of their product is zero.\nIn signal processing, it means the signals are independent and non-overlapping.\n\nOrthogonality captures the idea of “no overlap” or “independence” under the geometry of the inner product.\n\n\nProperties of Orthogonal Vectors\n\nIf \\(x \\perp y\\), then \\(\\|x+y\\|^2 = \\|x\\|^2 + \\|y\\|^2\\) (Pythagoras’ theorem generalized).\nOrthogonality is symmetric: if \\(x \\perp y\\), then \\(y \\perp x\\).\nAny set of mutually orthogonal nonzero vectors is automatically linearly independent.\n\nThis last property is critical: orthogonality guarantees independence.\n\n\nOrthonormal Sets\nAn orthonormal set is a collection of vectors \\(\\{u_1, \\dots, u_k\\}\\) such that\n\\[\n\\langle u_i, u_j \\rangle = \\begin{cases}\n1 & \\text{if } i=j, \\\\\n0 & \\text{if } i \\neq j.  \n\\end{cases}\n\\]\n\nEach vector has unit length.\nDistinct vectors are mutually orthogonal.\n\nThis structure makes computations with coordinates as simple as possible.\n\n\nOrthonormal Bases\nA basis \\(\\{u_1, \\dots, u_n\\}\\) for a vector space is orthonormal if it is orthonormal as a set.\n\nAny vector \\(x \\in V\\) can be written as\n\\[\nx = \\sum_{i=1}^n \\langle x, u_i \\rangle u_i.\n\\]\nThe coefficients are just inner products, no need to solve systems of equations.\n\nThis is why orthonormal bases are the most convenient: they make representation and projection effortless.\n\n\nExamples\n\nStandard Basis in \\(\\mathbb{R}^n\\): \\(\\{e_1, e_2, \\dots, e_n\\}\\), where \\(e_i\\) has 1 in the \\(i\\)-th coordinate and 0 elsewhere.\n\nOrthonormal under the standard dot product.\n\nFourier Basis: Functions \\(\\{\\sin(nx), \\cos(nx)\\}\\) on \\([0,2\\pi]\\) are orthogonal under the inner product \\(\\langle f,g\\rangle = \\int_0^{2\\pi} f(x)g(x)dx\\).\n\nThis basis decomposes signals into pure frequencies.\n\nPolynomial Basis: Legendre polynomials \\(P_n(x)\\) are orthogonal on \\([-1,1]\\) with respect to \\(\\langle f,g\\rangle = \\int_{-1}^1 f(x)g(x)\\,dx\\).\n\n\n\nGeometric Meaning\nOrthogonality splits space into independent “directions.”\n\nOrthonormal bases are like perfectly aligned coordinate axes.\nAny vector decomposes uniquely as a sum of independent contributions along these axes.\nDistances and angles are preserved, making the geometry transparent.\n\n\n\nApplications\n\nSignal Processing: Decompose signals into orthogonal frequency components.\nMachine Learning: Principal components form an orthonormal basis capturing variance directions.\nNumerical Methods: Orthonormal bases improve numerical stability.\nQuantum Mechanics: States are orthogonal if they represent mutually exclusive outcomes.\nComputer Graphics: Rotations are represented by orthogonal matrices with orthonormal columns.\n\n\n\nWhy It Matters\n\nOrthogonality provides independence; orthonormality provides normalization.\nTogether they make computations, decompositions, and projections clean and efficient.\nThey underlie Fourier analysis, principal component analysis, and countless modern algorithms.\n\n\n\nTry It Yourself\n\nShow that \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\) is an orthonormal basis of \\(\\mathbb{R}^3\\).\nCheck whether \\(\\{(1,1,0), (1,-1,0), (0,0,1)\\}\\) is orthonormal under the dot product. If not, normalize it.\nCompute the coefficients of \\(x=(3,4)\\) in the basis \\(\\{(1,0), (0,1)\\}\\) and in the rotated orthonormal basis \\(\\{(1/\\sqrt{2}, 1/\\sqrt{2}), (-1/\\sqrt{2}, 1/\\sqrt{2})\\}\\).\nChallenge: Prove that in any finite-dimensional inner product space, an orthonormal basis always exists (hint: Gram–Schmidt).\n\nOrthogonality and orthonormal bases are the backbone of linear algebra: they transform messy problems into elegant decompositions, giving us the cleanest possible language for describing vectors, signals, and data.\n\n\n\n73. Gram–Schmidt Process\nThe Gram–Schmidt process is a systematic method for turning any linearly independent set of vectors into an orthonormal basis. This process is one of the most elegant bridges between algebra and geometry: it takes arbitrary vectors and makes them mutually perpendicular, while preserving the span.\n\nThe Problem It Solves\nGiven a set of linearly independent vectors \\(\\{v_1, v_2, \\dots, v_n\\}\\) in an inner product space:\n\nThey span some subspace \\(W\\).\nBut they are not necessarily orthogonal or normalized.\n\nGoal: Construct an orthonormal basis \\(\\{u_1, u_2, \\dots, u_n\\}\\) for \\(W\\).\n\n\nThe Gram–Schmidt Algorithm\n\nStart with the first vector:\n\\[\nu_1 = \\frac{v_1}{\\|v_1\\|}.\n\\]\nFor the second vector, subtract the projection onto \\(u_1\\):\n\\[\nw_2 = v_2 - \\langle v_2, u_1 \\rangle u_1, \\quad u_2 = \\frac{w_2}{\\|w_2\\|}.\n\\]\nFor the third vector, subtract projections onto both \\(u_1\\) and \\(u_2\\):\n\\[\nw_3 = v_3 - \\langle v_3, u_1 \\rangle u_1 - \\langle v_3, u_2 \\rangle u_2, \\quad u_3 = \\frac{w_3}{\\|w_3\\|}.\n\\]\nContinue inductively:\n\\[\nw_k = v_k - \\sum_{j=1}^{k-1} \\langle v_k, u_j \\rangle u_j, \\quad u_k = \\frac{w_k}{\\|w_k\\|}.\n\\]\n\nAt each step, \\(w_k\\) is made orthogonal to all previous \\(u_j\\), and then normalized to form \\(u_k\\).\n\n\nExample in \\(\\mathbb{R}^2\\)\nStart with \\(v_1 = (1,1)\\), \\(v_2 = (1,0)\\).\n\nNormalize first vector:\n\\[\nu_1 = \\frac{(1,1)}{\\sqrt{2}} = \\left(\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right).\n\\]\nSubtract projection of \\(v_2\\) on \\(u_1\\):\n\\[\nw_2 = (1,0) - \\left(\\tfrac{1}{\\sqrt{2}}\\cdot1 + \\tfrac{1}{\\sqrt{2}}\\cdot0\\right)\\left(\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right).\n\\]\n\\[\n= (1,0) - \\tfrac{1}{\\sqrt{2}}\\left(\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right).\n\\]\n\\[\n= (1,0) - (0.5,0.5) = (0.5,-0.5).\n\\]\nNormalize:\n\\[\nu_2 = \\frac{(0.5,-0.5)}{\\sqrt{0.5^2+(-0.5)^2}} = \\frac{(0.5,-0.5)}{\\sqrt{0.5}} = \\left(\\tfrac{1}{\\sqrt{2}}, -\\tfrac{1}{\\sqrt{2}}\\right).\n\\]\n\nFinal orthonormal basis:\n\\[\nu_1 = \\left(\\tfrac{1}{\\sqrt{2}}, \\tfrac{1}{\\sqrt{2}}\\right), \\quad u_2 = \\left(\\tfrac{1}{\\sqrt{2}}, -\\tfrac{1}{\\sqrt{2}}\\right).\n\\]\n\n\nGeometric Intuition\n\nEach step removes “overlap” with previously chosen directions.\nThink of it as building new perpendicular coordinate axes inside the span of the original vectors.\nThe result is like rotating and scaling the original set into a perfectly orthogonal system.\n\n\n\nNumerical Stability\n\nClassical Gram–Schmidt can suffer from round-off errors in computer calculations.\nA numerically stable alternative is Modified Gram–Schmidt (MGS), which reorders the projection steps to reduce loss of orthogonality.\nIn practice, QR factorization algorithms often implement MGS or Householder reflections.\n\n\n\nApplications\n\nQR Factorization: Gram–Schmidt provides the foundation: \\(A = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.\nData Compression: Orthonormal bases from Gram–Schmidt lead to efficient representations.\nSignal Processing: Ensures independent frequency or wave components.\nMachine Learning: Used in orthogonalization of features and dimensionality reduction.\nPhysics: Orthogonal states in quantum mechanics can be constructed from arbitrary states using Gram–Schmidt.\n\n\n\nWhy It Matters\n\nGram–Schmidt guarantees that any independent set can be reshaped into an orthonormal basis.\nIt underlies computational methods like QR decomposition, least squares, and numerical PDE solvers.\nIt makes projections, coordinates, and orthogonality explicit and manageable.\n\n\n\nTry It Yourself\n\nApply Gram–Schmidt to \\((1,0,1)\\), \\((1,1,0)\\), \\((0,1,1)\\) in \\(\\mathbb{R}^3\\). Verify orthonormality.\nShow that the span of the orthonormal basis equals the span of the original vectors.\nUse Gram–Schmidt to find an orthonormal basis for polynomials \\(\\{1,x,x^2\\}\\) on \\([-1,1]\\) with inner product \\(\\langle f,g\\rangle = \\int_{-1}^1 f(x)g(x)\\,dx\\).\nChallenge: Prove that Gram–Schmidt always works for linearly independent sets, but fails if the set is dependent.\n\nThe Gram–Schmidt process is the algorithmic heart of orthogonality: it takes the messy and redundant and reshapes it into clean, perpendicular building blocks for the spaces we study.\n\n\n\n74. Projections onto Subspaces\nProjections are a natural extension of orthogonality: they describe how to “drop” a vector onto a subspace in the most natural way, minimizing the distance. Understanding projections is crucial for solving least squares problems, decomposing vectors, and interpreting data in terms of simpler, lower-dimensional structures.\n\nProjection onto a Vector\nStart with the simplest case: projecting a vector \\(x\\) onto a nonzero vector \\(u\\).\n\nThe projection is the component of \\(x\\) that lies in the direction of \\(u\\).\nFormula:\n\\[\n\\text{proj}_u(x) = \\frac{\\langle x, u \\rangle}{\\langle u, u \\rangle} u.\n\\]\nIf \\(u\\) is normalized (\\(\\|u\\|=1\\)), this simplifies to\n\\[\n\\text{proj}_u(x) = \\langle x, u \\rangle u.\n\\]\n\nGeometrically, this is the foot of the perpendicular from \\(x\\) onto the line spanned by \\(u\\).\n\n\nProjection onto an Orthonormal Basis\nSuppose we have an orthonormal basis \\(\\{u_1, u_2, \\dots, u_k\\}\\) for a subspace \\(W\\). Then the projection of \\(x\\) onto \\(W\\) is:\n\\[\n\\text{proj}_W(x) = \\sum_{i=1}^k \\langle x, u_i \\rangle u_i.\n\\]\nThis formula is powerful:\n\nEach coefficient \\(\\langle x, u_i \\rangle\\) captures how much of \\(x\\) aligns with \\(u_i\\).\nThe sum reconstructs the best approximation of \\(x\\) inside \\(W\\).\n\n\n\nProjection Matrix\nWhen working in coordinates, projections can be represented by matrices.\n\nIf \\(U\\) is the \\(n \\times k\\) matrix with orthonormal columns \\(\\{u_1, \\dots, u_k\\}\\), then\n\\[\nP = UU^T\n\\]\nis the projection matrix onto \\(W\\).\n\nProperties of \\(P\\):\n\nIdempotence: \\(P^2 = P\\).\nSymmetry: \\(P^T = P\\).\nBest approximation: For any \\(x\\), \\(\\|x - Px\\|\\) is minimized.\n\n\n\nProjection and Orthogonal Complements\nIf \\(W\\) is a subspace of \\(V\\), then every vector \\(x \\in V\\) can be decomposed uniquely as\n\\[\nx = \\text{proj}_W(x) + \\text{proj}_{W^\\perp}(x),\n\\]\nwhere \\(W^\\perp\\) is the orthogonal complement of \\(W\\).\nThis decomposition is the orthogonal decomposition theorem. It says: space splits cleanly into “in” and “out of” components relative to a subspace.\n\n\nExample in \\(\\mathbb{R}^2\\)\nLet \\(u = (2,1)\\), and project \\(x = (3,4)\\) onto span\\(\\{u\\}\\).\n\nCompute inner product: \\(\\langle x,u\\rangle = 3\\cdot 2 + 4\\cdot 1 = 10\\).\nCompute norm squared: \\(\\langle u,u\\rangle = 2^2 + 1^2 = 5\\).\nProjection:\n\\[\n\\text{proj}_u(x) = \\frac{10}{5}(2,1) = 2(2,1) = (4,2).\n\\]\nOrthogonal error:\n\\[\nx - \\text{proj}_u(x) = (3,4) - (4,2) = (-1,2).\n\\]\n\nNotice: \\((4,2)\\) lies on the line through \\(u\\), and the error vector \\((-1,2)\\) is orthogonal to \\(u\\).\n\n\nApplications\n\nLeast Squares Regression: The regression line is the projection of data onto the subspace spanned by predictor variables.\nDimensionality Reduction: Principal Component Analysis (PCA) projects data onto the subspace of top eigenvectors.\nComputer Graphics: 3D objects are projected onto 2D screens.\nNumerical Methods: Projections solve equations approximately when exact solutions don’t exist.\nPhysics: Work and energy are computed via projections of forces and velocities.\n\n\n\nWhy It Matters\n\nProjections are the essence of approximation: they give the “best possible” version of a vector inside a chosen subspace.\nThey formalize independence: the error vector is always orthogonal to the subspace.\nThey provide geometric intuition for statistics, machine learning, and numerical computation.\n\n\n\nTry It Yourself\n\nCompute the projection of \\(x = (2,3,4)\\) onto \\(u = (1,1,1)\\).\nVerify that the residual \\(x - \\text{proj}_u(x)\\) is orthogonal to \\(u\\).\nWrite the projection matrix for the subspace spanned by \\(\\{(1,0,0),(0,1,0)\\}\\) in \\(\\mathbb{R}^3\\).\nChallenge: Prove that projection matrices are idempotent and symmetric.\n\nProjections turn vector spaces into cleanly split components: what lies “inside” a subspace and what lies “outside.” This idea, simple yet profound, runs through geometry, data analysis, and physics alike.\n\n\n\n75. Orthogonal Decomposition Theorem\nOne of the cornerstones of linear algebra is the orthogonal decomposition theorem, which states that every vector in an inner product space can be uniquely split into two parts: one lying inside a subspace and the other lying in its orthogonal complement. This gives us a clear way to organize information, separate influences, and simplify computations.\n\nStatement of the Theorem\nLet \\(V\\) be an inner product space and \\(W\\) a subspace of \\(V\\). Then for every vector \\(x \\in V\\), there exist unique vectors \\(w \\in W\\) and \\(z \\in W^\\perp\\) such that\n\\[\nx = w + z.\n\\]\nHere:\n\n\\(w = \\text{proj}_W(x)\\), the projection of \\(x\\) onto \\(W\\).\n\\(z = x - \\text{proj}_W(x)\\), the orthogonal component.\n\nThis decomposition is unique: no other pair of vectors from \\(W\\) and \\(W^\\perp\\) adds up to \\(x\\).\n\n\nExample in \\(\\mathbb{R}^2\\)\nTake \\(W\\) to be the line spanned by \\(u = (1,2)\\). For \\(x = (4,1)\\):\n\nProjection:\n\\[\n\\text{proj}_u(x) = \\frac{\\langle x,u \\rangle}{\\langle u,u \\rangle} u.\n\\]\nCompute: \\(\\langle x,u\\rangle = 4\\cdot 1 + 1\\cdot 2 = 6\\), and \\(\\langle u,u\\rangle = 1^2+2^2=5\\). So\n\\[\n\\text{proj}_u(x) = \\frac{6}{5}(1,2) = \\left(\\tfrac{6}{5}, \\tfrac{12}{5}\\right).\n\\]\nOrthogonal component:\n\\[\nz = x - \\text{proj}_u(x) = (4,1) - \\left(\\tfrac{6}{5}, \\tfrac{12}{5}\\right) = \\left(\\tfrac{14}{5}, -\\tfrac{7}{5}\\right).\n\\]\nVerify: \\(\\langle u, z\\rangle = 1\\cdot \\tfrac{14}{5} + 2\\cdot (-\\tfrac{7}{5}) = 0\\). Thus, \\(z \\in W^\\perp\\).\n\nSo we have\n\\[\nx = \\underbrace{\\left(\\tfrac{6}{5}, \\tfrac{12}{5}\\right)}_{\\in W} + \\underbrace{\\left(\\tfrac{14}{5}, -\\tfrac{7}{5}\\right)}_{\\in W^\\perp}.\n\\]\n\n\nGeometric Meaning\n\nThe decomposition splits \\(x\\) into its “in-subspace” part and its “out-of-subspace” part.\n\\(w\\) is the closest point in \\(W\\) to \\(x\\).\n\\(z\\) is the leftover “error,” always perpendicular to \\(W\\).\n\nGeometrically, the shortest path from \\(x\\) to a subspace is always orthogonal.\n\n\nOrthogonal Complements\n\nThe orthogonal complement \\(W^\\perp\\) contains all vectors orthogonal to every vector in \\(W\\).\nDimensional relationship:\n\\[\n\\dim(W) + \\dim(W^\\perp) = \\dim(V).\n\\]\nTogether, \\(W\\) and \\(W^\\perp\\) partition the space \\(V\\).\n\n\n\nProjection Matrices and Decomposition\nIf \\(P\\) is the projection matrix onto \\(W\\):\n\\[\nx = Px + (I-P)x,\n\\]\nwhere \\(Px \\in W\\) and \\((I-P)x \\in W^\\perp\\).\nThis formulation is used constantly in numerical linear algebra.\n\n\nApplications\n\nLeast Squares Approximation: The best-fit solution is the projection; the error lies in the orthogonal complement.\nFourier Analysis: Any signal decomposes into a sum of components along orthogonal basis functions plus residuals.\nStatistics: Regression decomposes data into explained variance (in the subspace of predictors) and residual variance (orthogonal).\nEngineering: Splitting forces into parallel and perpendicular components relative to a surface.\nComputer Graphics: Decomposing movement into screen-plane projection and depth (orthogonal direction).\n\n\n\nWhy It Matters\n\nOrthogonal decomposition gives clarity: every vector splits into “relevant” and “irrelevant” parts relative to a chosen subspace.\nIt provides the foundation for least squares, regression, and signal approximation.\nIt ensures uniqueness, stability, and interpretability in vector computations.\n\n\n\nTry It Yourself\n\nIn \\(\\mathbb{R}^3\\), decompose \\(x = (1,2,3)\\) into components in span\\((1,0,0)\\) and its orthogonal complement.\nShow that if \\(W\\) is spanned by \\((1,1,0)\\) and \\((0,1,1)\\), then any vector in \\(\\mathbb{R}^3\\) can be uniquely split into \\(W\\) and \\(W^\\perp\\).\nWrite down the projection matrix \\(P\\) for \\(W = \\text{span}\\{(1,0,0),(0,1,0)\\}\\) in \\(\\mathbb{R}^3\\). Verify that \\(I-P\\) projects onto \\(W^\\perp\\).\nChallenge: Prove the orthogonal decomposition theorem using projection matrices and the fact that \\(P^2 = P\\).\n\nThe orthogonal decomposition theorem guarantees that every vector finds its closest approximation in a chosen subspace and a perfectly perpendicular remainder-an elegant structure that makes analysis and computation possible in countless domains.\n\n\n\n76. Orthogonal Projections and Least Squares\nOne of the deepest connections in linear algebra is between orthogonal projections and the least squares method. When equations don’t have an exact solution, least squares finds the “best approximate” one. The theory behind it is entirely geometric: the best solution is the projection of a vector onto a subspace.\n\nThe Setup: Overdetermined Systems\nConsider a system of equations \\(Ax = b\\), where\n\n\\(A\\) is an \\(m \\times n\\) matrix with \\(m &gt; n\\) (more equations than unknowns).\n\\(b \\in \\mathbb{R}^m\\) may not lie in the column space of \\(A\\).\n\nThis means:\n\nThere may be no exact solution.\nInstead, we want \\(x\\) that makes \\(Ax\\) as close as possible to \\(b\\).\n\n\n\nLeast Squares Problem\nThe least squares solution minimizes the error:\n\\[\n\\min_x \\|Ax - b\\|^2.\n\\]\nHere:\n\n\\(Ax\\) is the projection of \\(b\\) onto the column space of \\(A\\).\nThe error vector \\(b - Ax\\) is orthogonal to the column space.\n\nThis is exactly the orthogonal decomposition theorem applied to \\(b\\).\n\n\nDerivation of Normal Equations\nWe want \\(r = b - Ax\\) to be orthogonal to every column of \\(A\\):\n\\[\nA^T (b - Ax) = 0.\n\\]\nRearranging:\n\\[\nA^T A x = A^T b.\n\\]\nThis system is called the normal equations. Its solution \\(x\\) gives the least squares approximation.\n\n\nProjection Matrix in Least Squares\nThe projection of \\(b\\) onto \\(\\text{Col}(A)\\) is\n\\[\n\\hat{b} = A(A^T A)^{-1} A^T b,\n\\]\nassuming \\(A^T A\\) is invertible.\nHere,\n\n\\(P = A(A^T A)^{-1} A^T\\) is the projection matrix onto the column space of \\(A\\).\nThe fitted vector is \\(\\hat{b} = Pb\\).\nThe residual \\(r = b - \\hat{b}\\) lies in the orthogonal complement of \\(\\text{Col}(A)\\).\n\n\n\nExample\nSuppose \\(A = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}\\), \\(b = \\begin{bmatrix}2 \\\\ 2 \\\\ 4\\end{bmatrix}\\).\n\nColumn space of \\(A\\): span of \\((1,2,3)\\).\nProjection formula:\n\\[\n\\hat{b} = \\frac{\\langle b, A \\rangle}{\\langle A, A \\rangle} A.\n\\]\nCompute: \\(\\langle b,A\\rangle = 2\\cdot1+2\\cdot2+4\\cdot3 = 18\\). \\(\\langle A,A\\rangle = 1^2+2^2+3^2=14\\).\nProjection:\n\\[\n\\hat{b} = \\frac{18}{14}(1,2,3) = \\left(\\tfrac{9}{7}, \\tfrac{18}{7}, \\tfrac{27}{7}\\right).\n\\]\nResidual:\n\\[\nr = b - \\hat{b} = \\left(\\tfrac{5}{7}, -\\tfrac{4}{7}, \\tfrac{1}{7}\\right).\n\\]\n\nCheck: \\(\\langle r,A\\rangle = 0\\), so it’s orthogonal.\n\n\nGeometric Meaning\n\nThe least squares solution is the point in \\(\\text{Col}(A)\\) closest to \\(b\\).\nThe error vector is orthogonal to the subspace.\nThis is like dropping a perpendicular from \\(b\\) to the subspace \\(\\text{Col}(A)\\).\n\n\n\nApplications\n\nStatistics: Linear regression uses least squares to fit models to data.\nEngineering: Curve fitting, system identification, and calibration.\nComputer Graphics: Best-fit transformations (e.g., Procrustes analysis).\nMachine Learning: Optimization of linear models (before moving to nonlinear methods).\nNumerical Methods: Solving inconsistent systems of equations.\n\n\n\nWhy It Matters\n\nOrthogonal projections explain why least squares gives the best approximation.\nThey reveal the geometry behind regression: data is projected onto the model space.\nThey connect linear algebra with statistics, optimization, and applied sciences.\n\n\n\nTry It Yourself\n\nSolve \\(\\min_x \\|Ax-b\\|\\) for \\(A = \\begin{bmatrix}1 & 1 \\\\ 1 & 2 \\\\ 1 & 3\\end{bmatrix}\\), \\(b=(1,2,2)^T\\). Interpret the result.\nDerive the projection matrix \\(P\\) for this system.\nShow that the residual is orthogonal to each column of \\(A\\).\nChallenge: Prove that among all possible approximations \\(Ax\\), the least squares solution is unique if and only if \\(A^T A\\) is invertible.\n\nOrthogonal projections turn the messy, unsolvable world of overdetermined equations into one of best possible approximations. Least squares is not just an algebraic trick-it is the geometric essence of “closeness” in higher-dimensional spaces.\n\n\n\n77. QR Decomposition\nQR decomposition is a factorization of a matrix into an orthogonal part and a triangular part. It grows directly out of orthogonality and the Gram–Schmidt process, and it plays a central role in numerical linear algebra, providing a stable and efficient way to solve systems, compute least squares solutions, and analyze matrices.\n\nDefinition\nFor a real \\(m \\times n\\) matrix \\(A\\) with linearly independent columns:\n\\[\nA = QR,\n\\]\nwhere:\n\n\\(Q\\) is an \\(m \\times n\\) matrix with orthonormal columns (\\(Q^T Q = I\\)).\n\\(R\\) is an \\(n \\times n\\) upper triangular matrix.\n\nThis decomposition is unique if we require \\(R\\) to have positive diagonal entries.\n\n\nConnection to Gram–Schmidt\nThe Gram–Schmidt process applied to the columns of \\(A\\) produces the orthonormal columns of \\(Q\\). The coefficients used during the orthogonalization steps naturally form the entries of \\(R\\).\n\nEach column of \\(A\\) is expressed as a combination of the orthonormal columns of \\(Q\\).\nThe coefficients of this expression populate the triangular matrix \\(R\\).\n\n\n\nExample\nLet\n\\[\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n\\]\n\nApply Gram–Schmidt to the columns:\n\n\\(v_1 = (1,1,0)^T\\), normalize:\n\\[\nu_1 = \\frac{1}{\\sqrt{2}}(1,1,0)^T.\n\\]\nSubtract projection from \\(v_2=(1,0,1)^T\\):\n\\[\nw_2 = v_2 - \\langle v_2,u_1\\rangle u_1.\n\\]\nCompute \\(\\langle v_2,u_1\\rangle = \\tfrac{1}{\\sqrt{2}}(1+0+0)=\\tfrac{1}{\\sqrt{2}}\\). So\n\\[\nw_2 = (1,0,1)^T - \\tfrac{1}{\\sqrt{2}}(1,1,0)^T = \\left(\\tfrac{1}{2}, -\\tfrac{1}{2}, 1\\right)^T.\n\\]\nNormalize:\n\\[\nu_2 = \\frac{1}{\\sqrt{1.5}} \\left(\\tfrac{1}{2}, -\\tfrac{1}{2}, 1\\right)^T.\n\\]\n\nConstruct \\(Q = [u_1, u_2]\\).\nCompute \\(R = Q^T A\\).\n\nThe result is \\(A = QR\\), with \\(Q\\) orthonormal and \\(R\\) triangular.\n\n\nGeometric Meaning\n\n\\(Q\\) represents an orthogonal change of basis-rotations and reflections that preserve length and angle.\n\\(R\\) encodes scaling and shear in the new orthonormal coordinate system.\nTogether, they show how \\(A\\) transforms space: first rotate into a clean basis, then apply triangular distortion.\n\n\n\nApplications\n\nLeast Squares: Instead of solving \\(A^T A x = A^T b\\), we use \\(QR\\):\n\\[\nAx = b \\quad \\Rightarrow \\quad QRx = b.\n\\]\nMultiply by \\(Q^T\\):\n\\[\nRx = Q^T b.\n\\]\nSince \\(R\\) is triangular, solving for \\(x\\) is efficient and numerically stable.\nEigenvalue Algorithms: The QR algorithm iteratively applies QR factorizations to approximate eigenvalues.\nNumerical Stability: Orthogonal transformations minimize numerical errors compared to solving normal equations.\nMachine Learning: Many algorithms (e.g., linear regression, PCA) use QR decomposition for efficiency and stability.\nComputer Graphics: Orthogonal factors preserve shapes; triangular factors simplify transformations.\n\n\n\nWhy It Matters\n\nQR decomposition bridges theory (Gram–Schmidt orthogonalization) and computation (matrix factorization).\nIt avoids pitfalls of normal equations, improving numerical stability.\nIt underpins algorithms across statistics, engineering, and computer science.\n\n\n\nTry It Yourself\n\nCompute the QR decomposition of\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 2 & 3 \\\\ 4 & 5\\end{bmatrix}.\n\\]\nVerify that \\(Q^T Q = I\\) and \\(R\\) is upper triangular.\nUse QR to solve the least squares problem \\(Ax \\approx b\\) with \\(b=(1,1,1)^T\\).\nChallenge: Show that if \\(A\\) is square and orthogonal, then \\(R=I\\) and \\(Q=A\\).\n\nQR decomposition turns the messy process of solving least squares into a clean, geometric procedure-rotating into a better coordinate system before solving. It is one of the most powerful tools in the linear algebra toolkit.\n\n\n\n78. Orthogonal Matrices\nOrthogonal matrices are square matrices whose rows and columns form an orthonormal set. They are the algebraic counterpart of rigid motions in geometry: transformations that preserve lengths, angles, and orientation (except for reflections).\n\nDefinition\nA square matrix \\(Q \\in \\mathbb{R}^{n \\times n}\\) is orthogonal if\n\\[\nQ^T Q = QQ^T = I.\n\\]\nThis means:\n\nThe columns of \\(Q\\) are orthonormal.\nThe rows of \\(Q\\) are also orthonormal.\n\n\n\nProperties\n\nInverse Equals Transpose:\n\\[\nQ^{-1} = Q^T.\n\\]\nThis makes orthogonal matrices especially easy to invert.\nPreservation of Norms: For any vector \\(x\\),\n\\[\n\\|Qx\\| = \\|x\\|.\n\\]\nOrthogonal transformations never stretch or shrink vectors.\nPreservation of Inner Products:\n\\[\n\\langle Qx, Qy \\rangle = \\langle x, y \\rangle.\n\\]\nAngles are preserved.\nDeterminant: \\(\\det(Q) = \\pm 1\\).\n\nIf \\(\\det(Q) = 1\\), \\(Q\\) is a rotation.\nIf \\(\\det(Q) = -1\\), \\(Q\\) is a reflection combined with rotation.\n\n\n\n\nExamples\n\n2D Rotation Matrix:\n\\[\nQ = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}.\n\\]\nRotates vectors by angle \\(\\theta\\).\n2D Reflection Matrix: Reflection across the \\(x\\)-axis:\n\\[\nQ = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n\\]\nPermutation Matrices: Swapping coordinates is orthogonal because it preserves lengths. Example in 3D:\n\\[\nQ = \\begin{bmatrix}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}.\n\\]\n\n\n\nGeometric Meaning\nOrthogonal matrices represent isometries: transformations that preserve the shape of objects.\n\nThey can rotate, reflect, or permute axes.\nThey never distort lengths or angles.\n\nThis is why in computer graphics, orthogonal matrices model pure rotations and reflections without scaling.\n\n\nApplications\n\nComputer Graphics: Rotations of 3D models use orthogonal matrices to avoid distortion.\nNumerical Linear Algebra: Orthogonal transformations are numerically stable, widely used in QR factorization and eigenvalue algorithms.\nData Compression: Orthogonal transforms like the Fourier and cosine transforms preserve energy.\nSignal Processing: Orthogonal filters separate signals into independent components.\nPhysics: Orthogonal matrices describe rotations in rigid body dynamics.\n\n\n\nWhy It Matters\n\nOrthogonal matrices are the building blocks of stable algorithms.\nThey describe symmetry, structure, and invariance in physical and computational systems.\nThey serve as the simplest and most powerful class of transformations that preserve geometry exactly.\n\n\n\nTry It Yourself\n\nVerify that\n\\[\nQ = \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix}\n\\]\nis orthogonal. What geometric transformation does it represent?\nProve that the determinant of an orthogonal matrix must be \\(\\pm 1\\).\nShow that multiplying two orthogonal matrices gives another orthogonal matrix.\nChallenge: Prove that eigenvalues of orthogonal matrices lie on the complex unit circle (i.e., \\(|\\lambda|=1\\)).\n\nOrthogonal matrices capture the essence of symmetry: transformations that preserve structure exactly. They lie at the heart of geometry, physics, and computation.\n\n\n\n79. Fourier Viewpoint\nThe Fourier viewpoint is one of the most profound connections in linear algebra: the idea that complex signals, data, or functions can be decomposed into sums of simpler, orthogonal waves. Instead of describing information in its raw form (time, space, or coordinates), we express it in terms of frequencies. This perspective reshapes how we analyze, compress, and understand information across mathematics, physics, and engineering.\n\nFourier Series: The Basic Idea\nSuppose we have a periodic function \\(f(x)\\) defined on \\([-\\pi, \\pi]\\). The Fourier series expresses \\(f(x)\\) as:\n\\[\nf(x) = a_0 + \\sum_{n=1}^\\infty \\left( a_n \\cos(nx) + b_n \\sin(nx) \\right).\n\\]\n\nThe coefficients \\(a_n, b_n\\) are found using inner products with sine and cosine functions.\nEach sine and cosine is orthogonal to the others under the inner product\n\\[\n\\langle f, g \\rangle = \\int_{-\\pi}^\\pi f(x) g(x) \\, dx.\n\\]\n\nThus, Fourier series is nothing more than expanding a function in an orthonormal basis of trigonometric functions.\n\n\nFourier Transform: From Time to Frequency\nFor non-periodic signals, the Fourier transform generalizes this expansion. For a function \\(f(t)\\),\n\\[\n\\hat{f}(\\omega) = \\int_{-\\infty}^\\infty f(t) e^{-i \\omega t} dt\n\\]\ntransforms it into frequency space. The inverse transform reconstructs \\(f(t)\\) from its frequencies.\nThis is again an inner product viewpoint: the exponential functions \\(e^{i \\omega t}\\) act as orthogonal basis functions on \\(\\mathbb{R}\\).\n\n\nOrthogonality of Waves\nThe trigonometric functions \\(\\{\\cos(nx), \\sin(nx)\\}\\) and the complex exponentials \\(\\{e^{i\\omega t}\\}\\) form orthogonal families.\n\nTwo different sine waves have zero inner product over a full period.\nLikewise, exponentials with different frequencies are orthogonal.\n\nThis is exactly like orthogonal vectors in \\(\\mathbb{R}^n\\), except here the space is infinite-dimensional.\n\n\nDiscrete Fourier Transform (DFT)\nIn computational settings, we don’t work with infinite integrals but with finite data. The DFT expresses an \\(n\\)-dimensional vector \\(x = (x_0, \\dots, x_{n-1})\\) as a linear combination of orthogonal complex exponentials:\n\\[\nX_k = \\sum_{j=0}^{n-1} x_j e^{-2\\pi i jk / n}, \\quad k=0,\\dots,n-1.\n\\]\nThis is simply a change of basis: from the standard basis (time domain) to the Fourier basis (frequency domain).\nThe Fast Fourier Transform (FFT) computes this in \\(O(n \\log n)\\) time, making Fourier analysis practical at scale.\n\n\nGeometric Meaning\n\nIn the time domain, data is expressed as a sequence of raw values.\nIn the frequency domain, data is expressed as amplitudes of orthogonal waves.\nThe Fourier viewpoint is just a rotation into a new orthogonal coordinate system, exactly like diagonalizing a matrix or changing basis.\n\n\n\nApplications\n\nSignal Processing: Filtering unwanted noise corresponds to removing high-frequency components.\nImage Compression: JPEG uses Fourier-like transforms (cosine transforms) to compress images.\nData Analysis: Identifying cycles and periodic patterns in time series.\nPhysics: Quantum states are represented in both position and momentum bases, linked by Fourier transform.\nPartial Differential Equations: Solutions are simplified by moving to frequency space, where derivatives become multipliers.\n\n\n\nWhy It Matters\n\nFourier methods turn difficult problems into simpler ones: convolution becomes multiplication, differentiation becomes scaling.\nThey provide a universal language for analyzing periodicity, oscillation, and wave phenomena.\nThey are linear algebra at heart: orthogonal expansions in special bases.\n\n\n\nTry It Yourself\n\nCompute the Fourier series coefficients for \\(f(x) = x\\) on \\([-\\pi,\\pi]\\).\nFor the sequence \\((1,0,0,0)\\), compute the 4-point DFT and interpret the result.\nShow that \\(\\int_{-\\pi}^\\pi \\sin(mx)\\cos(nx) dx = 0\\).\nChallenge: Prove that the Fourier basis \\(\\{e^{i2\\pi k t}\\}_{k=0}^{n-1}\\) is orthonormal in \\(\\mathbb{C}^n\\).\n\nThe Fourier viewpoint reveals that every signal, no matter how complex, can be seen as a combination of simple, orthogonal waves. It is a perfect marriage of geometry, algebra, and analysis, and one of the most important ideas in modern mathematics.\n\n\n\n80. Polynomial and Multifeature Least Squares\nLeast squares problems become especially powerful when extended to fitting polynomials or handling multiple features at once. Instead of a single straight line through data, we can fit curves of higher degree or surfaces in higher dimensions. These generalizations lie at the heart of regression, data analysis, and scientific modeling.\n\nFrom Line to Polynomial\nThe simplest least squares model is a straight line:\n\\[\ny \\approx \\beta_0 + \\beta_1 x.\n\\]\nBut many relationships are nonlinear. Polynomial least squares generalizes the model to:\n\\[\ny \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d.\n\\]\nHere, each power of \\(x\\) is treated as a new feature. The problem reduces to ordinary least squares on the design matrix:\n\\[\nA = \\begin{bmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{bmatrix},\n\\quad\n\\beta = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_d\\end{bmatrix}.\n\\]\nThe least squares solution minimizes \\(\\|A\\beta - y\\|\\).\n\n\nMultiple Features\nWhen data involves several predictors, we extend the model to:\n\\[\ny \\approx \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p.\n\\]\nIn matrix form:\n\\[\ny \\approx A\\beta,\n\\]\nwhere \\(A\\) is the design matrix with columns corresponding to features (including a column of ones for the intercept).\nThe least squares solution is still given by the normal equations:\n\\[\n\\hat{\\beta} = (A^T A)^{-1} A^T y,\n\\]\nor more stably by QR or SVD factorizations.\n\n\nExample: Polynomial Fit\nSuppose we have data points \\((1,1), (2,2.2), (3,2.9), (4,4.1)\\). Fitting a quadratic model \\(y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2\\):\n\nConstruct design matrix:\n\\[\nA = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 2 & 4 \\\\\n1 & 3 & 9 \\\\\n1 & 4 & 16\n\\end{bmatrix}.\n\\]\nSolve least squares problem \\(\\min \\|A\\beta - y\\|\\).\nThe result gives coefficients \\(\\beta_0, \\beta_1, \\beta_2\\) that best approximate the curve.\n\nThe same process works for higher-degree polynomials or multiple features.\n\n\nGeometric Meaning\n\nIn polynomial least squares, the feature space expands: instead of points on a line, data lives in a higher-dimensional feature space \\((1, x, x^2, \\dots, x^d)\\).\nIn multifeature least squares, the column space of \\(A\\) spans all possible linear combinations of features.\nThe least squares solution projects the observed output vector \\(y\\) onto this subspace.\n\nThus, whether polynomial or multifeature, the geometry is the same: projection onto the model space.\n\n\nPractical Challenges\n\nOverfitting: Higher-degree polynomials fit noise, not just signal.\nMulticollinearity: Features may be correlated, making \\(A^T A\\) nearly singular.\nScaling: Features with different magnitudes should be normalized.\nRegularization: Adding penalty terms (ridge or LASSO) stabilizes the solution.\n\n\n\nApplications\n\nRegression in Statistics: Extending linear regression to handle multiple predictors or polynomial terms.\nMachine Learning: Basis expansion for feature engineering (before neural nets, this was the standard).\nEngineering: Curve fitting for calibration, modeling, and prediction.\nEconomics: Forecasting models with many variables (inflation, interest rates, spending).\nPhysics and Chemistry: Polynomial regression to model experimental data.\n\n\n\nWhy It Matters\n\nPolynomial least squares captures curvature in data.\nMultifeature least squares allows multiple predictors to explain outcomes.\nBoth generalizations turn linear algebra into a practical modeling tool across science and society.\n\n\n\nTry It Yourself\n\nFit a quadratic curve through points \\((0,1), (1,2), (2,5), (3,10)\\). Compare to a straight-line fit.\nConstruct a multifeature design matrix for predicting exam scores based on hours studied, sleep, and prior grades.\nShow that polynomial regression is just linear regression on transformed features.\nChallenge: Derive the bias–variance tradeoff in polynomial least squares-why higher degrees increase variance.\n\nPolynomial and multifeature least squares extend the reach of linear algebra from straight lines to complex patterns, giving us a universal framework for modeling relationships in data.\n\n\nClosing\nClosest lines are drawn,\nerrors fall away to rest,\nangles guard the truth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-9.-svd-pca-and-conditioning",
    "href": "books/en-US/book.html#chapter-9.-svd-pca-and-conditioning",
    "title": "The Book",
    "section": "Chapter 9. SVD, PCA, and conditioning",
    "text": "Chapter 9. SVD, PCA, and conditioning\n\nOpening\nClosest lines are drawn,\nerrors fall away to rest,\nangles guard the truth.\n\n\n81. Singular Values and SVD\nThe Singular Value Decomposition (SVD) is one of the most powerful tools in linear algebra. It generalizes eigen-decomposition, works for all rectangular matrices (not just square ones), and provides deep insights into geometry, computation, and data analysis. At its core, the SVD tells us that every matrix can be factored into three pieces: rotations/reflections, scaling, and rotations/reflections again.\n\nDefinition of SVD\nFor any real \\(m \\times n\\) matrix \\(A\\), the SVD is:\n\\[\nA = U \\Sigma V^T,\n\\]\nwhere:\n\n\\(U\\) is an \\(m \\times m\\) orthogonal matrix (columns = left singular vectors).\n\\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix with nonnegative entries \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0\\) (singular values).\n\\(V\\) is an \\(n \\times n\\) orthogonal matrix (columns = right singular vectors).\n\nEven if \\(A\\) is rectangular or not diagonalizable, this factorization always exists.\n\n\nGeometric Meaning\nThe SVD describes how \\(A\\) transforms space:\n\nFirst rotation/reflection: Multiply by \\(V^T\\) to rotate or reflect coordinates into the right singular vector basis.\nScaling: Multiply by \\(\\Sigma\\), stretching/shrinking each axis by a singular value.\nSecond rotation/reflection: Multiply by \\(U\\) to reorient into the output space.\n\nThus, \\(A\\) acts as a rotation, followed by scaling, followed by another rotation.\n\n\nSingular Values\n\nThe singular values \\(\\sigma_i\\) are the square roots of the eigenvalues of \\(A^T A\\).\nThey measure how much \\(A\\) stretches space in particular directions.\nThe largest singular value \\(\\sigma_1\\) is the operator norm of \\(A\\): the maximum stretch factor.\nIf some singular values are zero, they correspond to directions collapsed by \\(A\\).\n\n\n\nExample in \\(\\mathbb{R}^2\\)\nLet\n\\[\nA = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2 \\end{bmatrix}.\n\\]\n\nCompute \\(A^T A = \\begin{bmatrix} 9 & 3 \\\\ 3 & 5 \\end{bmatrix}\\).\nFind its eigenvalues: \\(\\lambda_1, \\lambda_2 = 10 \\pm \\sqrt{10}\\).\nSingular values: \\(\\sigma_i = \\sqrt{\\lambda_i}\\).\nThe corresponding eigenvectors form the right singular vectors \\(V\\).\nLeft singular vectors \\(U\\) are obtained by \\(U = AV/\\Sigma\\).\n\nThis decomposition reveals how \\(A\\) reshapes circles into ellipses.\n\n\nLinks to Eigen-Decomposition\n\nEigen-decomposition works only for square, diagonalizable matrices.\nSVD works for all matrices, square or rectangular, diagonalizable or not.\nInstead of eigenvalues (which may be complex or negative), we get singular values (always real and nonnegative).\nEigenvectors can fail to exist in a full basis; singular vectors always form orthonormal bases.\n\n\n\nApplications\n\nData Compression: Truncate small singular values to approximate matrices with fewer dimensions (used in JPEG).\nPrincipal Component Analysis (PCA): SVD on centered data finds principal components, directions of maximum variance.\nLeast Squares Problems: SVD provides stable solutions, even for ill-conditioned or singular systems.\nNoise Filtering: Discard small singular values to remove noise in signals and images.\nNumerical Stability: SVD helps diagnose conditioning-how sensitive solutions are to input errors.\n\n\n\nWhy It Matters\n\nSVD is the “Swiss army knife” of linear algebra: versatile, always applicable, and rich in interpretation.\nIt provides geometric, algebraic, and computational clarity.\nIt is indispensable for modern applications in machine learning, statistics, engineering, and physics.\n\n\n\nTry It Yourself\n\nCompute the SVD of\n\\[\nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 2 \\\\ 0 & 0\\end{bmatrix}.\n\\]\nInterpret the scaling and rotations.\nShow that for any vector \\(x\\), \\(\\|Ax\\| \\leq \\sigma_1 \\|x\\|\\).\nUse SVD to approximate the matrix\n\\[\n\\begin{bmatrix}1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1\\end{bmatrix}\n\\]\nwith rank 1.\nChallenge: Prove that the Frobenius norm of \\(A\\) is the square root of the sum of squares of its singular values.\n\nThe singular value decomposition is universal: every matrix can be dissected into rotations and scalings, revealing its structure and enabling powerful techniques across mathematics and applied sciences.\n\n\n\n82. Geometry of SVD\nThe Singular Value Decomposition (SVD) is not just an algebraic factorization-it has a precise geometric meaning. It explains exactly how any linear transformation reshapes space: stretching, rotating, compressing, and possibly collapsing dimensions. Understanding this geometry turns SVD from a formal tool into an intuitive picture of what matrices do.\n\nTransformation of the Unit Sphere\nTake the unit sphere (or circle, in 2D) in the input space. When we apply a matrix \\(A\\):\n\nThe sphere is transformed into an ellipsoid.\nThe axes of this ellipsoid correspond to the right singular vectors \\(v_i\\).\nThe lengths of the axes are the singular values \\(\\sigma_i\\).\nThe directions of the axes in the output space are the left singular vectors \\(u_i\\).\n\nThus, SVD tells us:\n\\[\nA v_i = \\sigma_i u_i.\n\\]\nEvery matrix maps orthogonal basis directions into orthogonal ellipsoid axes, scaled by singular values.\n\n\nStep-by-Step Geometry\nThe decomposition \\(A = U \\Sigma V^T\\) can be read geometrically:\n\nRotate/reflect by \\(V^T\\): Align input coordinates with the “principal directions” of \\(A\\).\nScale by \\(\\Sigma\\): Stretch or compress each axis by its singular value. Some singular values may be zero, flattening dimensions.\nRotate/reflect by \\(U\\): Reorient the scaled axes into the output space.\n\nThis process is universal: no matter how irregular a matrix seems, it always reshapes space by rotation → scaling → rotation.\n\n\n2D Example\nTake\n\\[\nA = \\begin{bmatrix}3 & 1 \\\\ 0 & 2\\end{bmatrix}.\n\\]\n\nA circle in \\(\\mathbb{R}^2\\) is mapped into an ellipse.\nThe ellipse’s major and minor axes align with the right singular vectors of \\(A\\).\nTheir lengths equal the singular values.\nThe ellipse itself is then oriented in the output plane according to the left singular vectors.\n\nThis makes SVD the perfect tool for visualizing how \\(A\\) “distorts” geometry.\n\n\nStretching and Rank\n\nIf all singular values are positive, the ellipsoid has full dimension (no collapse).\nIf some singular values are zero, \\(A\\) flattens the sphere along certain directions, lowering the rank.\nThe rank of \\(A\\) equals the number of nonzero singular values.\n\nThus, rank-deficient matrices literally squash space into lower dimensions.\n\n\nDistance and Energy Preservation\n\nThe largest singular value \\(\\sigma_1\\) is how much \\(A\\) can stretch a vector.\nThe smallest nonzero singular value \\(\\sigma_r\\) (where \\(r = \\text{rank}(A)\\)) measures how much the matrix compresses.\nThe condition number \\(\\kappa(A) = \\sigma_1 / \\sigma_r\\) measures distortion: small values mean nearly spherical stretching, large values mean extreme elongation.\n\n\n\nApplications of the Geometry\n\nData Compression: Keeping only the largest singular values keeps the “major axes” of variation.\nPCA: Data is analyzed along orthogonal axes of greatest variance (singular vectors).\nNumerical Analysis: Geometry of SVD shows why ill-conditioned systems amplify errors-because some directions are squashed almost flat.\nSignal Processing: Elliptical distortions correspond to filtering out certain frequency components.\nMachine Learning: Dimensionality reduction is essentially projecting data onto the largest singular directions.\n\n\n\nWhy It Matters\n\nSVD transforms algebraic equations into geometric pictures.\nIt reveals exactly how matrices warp space, offering intuition behind abstract operations.\nBy interpreting ellipses, singular values, and orthogonal vectors, we gain visual clarity for problems in data, physics, and computation.\n\n\n\nTry It Yourself\n\nDraw the unit circle in \\(\\mathbb{R}^2\\), apply the matrix\n\\[\nA = \\begin{bmatrix}2 & 0 \\\\ 1 & 3\\end{bmatrix},\n\\]\nand sketch the resulting ellipse. Identify its axes and lengths.\nVerify numerically that \\(Av_i = \\sigma_i u_i\\) for computed singular vectors and singular values.\nFor a rank-1 matrix, sketch how the unit circle collapses to a line segment.\nChallenge: Prove that the set of vectors with maximum stretch under \\(A\\) are precisely the first right singular vectors.\n\nThe geometry of SVD gives us a universal lens: every linear transformation is a controlled distortion of space, built from orthogonal rotations and directional scalings.\n\n\n\n83. Relation to Eigen-Decompositions\nThe Singular Value Decomposition (SVD) is often introduced as something entirely new, but it is deeply tied to eigen-decomposition. In fact, singular values and singular vectors emerge from the eigen-decomposition of certain symmetric matrices constructed from \\(A\\). Understanding this connection shows why SVD always exists, why singular values are nonnegative, and how it generalizes eigen-analysis to all matrices, even rectangular ones.\n\nEigen-Decomposition Recap\nFor a square matrix \\(M \\in \\mathbb{R}^{n \\times n}\\), an eigen-decomposition is:\n\\[\nM = X \\Lambda X^{-1},\n\\]\nwhere \\(\\Lambda\\) is a diagonal matrix of eigenvalues and the columns of \\(X\\) are eigenvectors.\nHowever:\n\nNot all matrices are diagonalizable.\nEigenvalues may be complex.\nRectangular matrices don’t have eigenvalues at all.\n\nThis is where SVD provides a universal framework.\n\n\nFrom \\(A^T A\\) to Singular Values\nFor any \\(m \\times n\\) matrix \\(A\\):\n\nConsider the symmetric, positive semidefinite matrix \\(A^T A \\in \\mathbb{R}^{n \\times n}\\).\n\nSymmetry ensures all eigenvalues are real.\nPositive semidefiniteness ensures they are nonnegative.\n\nThe eigenvalues of \\(A^T A\\) are squares of the singular values of \\(A\\):\n\\[\n\\lambda_i(A^T A) = \\sigma_i^2.\n\\]\nThe eigenvectors of \\(A^T A\\) are the right singular vectors \\(v_i\\).\nSimilarly, for \\(AA^T\\), eigenvalues are the same \\(\\sigma_i^2\\), and eigenvectors are the left singular vectors \\(u_i\\).\n\nThus:\n\\[\nAv_i = \\sigma_i u_i, \\quad A^T u_i = \\sigma_i v_i.\n\\]\nThis pair of relationships binds eigen-decomposition and SVD together.\n\n\nWhy Eigen-Decomposition Is Not Enough\n\nEigen-decomposition requires a square matrix. SVD works for rectangular matrices.\nEigenvalues can be negative or complex; singular values are always real and nonnegative.\nEigenvectors may not exist as a complete basis; singular vectors always form orthonormal bases.\n\nIn short, SVD provides the robustness that eigen-decomposition lacks.\n\n\nExample\nLet\n\\[\nA = \\begin{bmatrix}3 & 0 \\\\ 4 & 0 \\\\ 0 & 5\\end{bmatrix}.\n\\]\n\nCompute \\(A^T A = \\begin{bmatrix}25 & 0 \\\\ 0 & 25\\end{bmatrix}\\).\n\nEigenvalues: \\(25, 25\\).\nSingular values: \\(\\sigma_1 = \\sigma_2 = 5\\).\n\nRight singular vectors are eigenvectors of \\(A^T A\\). Here, they form the standard basis.\nLeft singular vectors come from \\(Av_i / \\sigma_i\\).\n\nSo the geometry of SVD is fully encoded in eigen-analysis of \\(A^T A\\) and \\(AA^T\\).\n\n\nGeometric Picture\n\nEigenvectors of \\(A^T A\\) describe directions in input space where \\(A\\) stretches without mixing directions.\nEigenvectors of \\(AA^T\\) describe the corresponding directions in output space.\nSingular values tell us how much stretching occurs.\n\nThus, SVD is essentially eigen-decomposition in disguise-but applied to the right symmetric companions.\n\n\nApplications of the Connection\n\nPCA: Data covariance matrix \\(X^T X\\) uses eigen-decomposition, but PCA is implemented with SVD directly.\nNumerical Methods: Algorithms for SVD rely on eigen-analysis of \\(A^T A\\).\nStability Analysis: The relationship ensures singular values are reliable measures of conditioning.\nSignal Processing: Power in signals (variance) is explained by eigenvalues of covariance, which connect to singular values.\nMachine Learning: Kernel PCA and related methods depend on this link to handle nonlinear features.\n\n\n\nWhy It Matters\n\nSVD explains every matrix transformation in terms of orthogonal bases and scalings.\nIts relationship with eigen-decomposition ensures that SVD is not an alien tool, but a generalization.\nThe eigenview shows why SVD is guaranteed to exist and why singular values are always real and nonnegative.\n\n\n\nTry It Yourself\n\nProve that if \\(v\\) is an eigenvector of \\(A^T A\\) with eigenvalue \\(\\lambda\\), then \\(Av\\) is either zero or a left singular vector of \\(A\\) with singular value \\(\\sqrt{\\lambda}\\).\nFor the matrix\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 2 & 1\\end{bmatrix},\n\\]\ncompute both eigen-decomposition and SVD. Compare the results.\nShow that \\(A^T A\\) and \\(AA^T\\) always share the same nonzero eigenvalues.\nChallenge: Explain why an orthogonal diagonalization of \\(A^T A\\) is enough to guarantee existence of the full SVD of \\(A\\).\n\nThe relationship between SVD and eigen-decomposition unifies two of linear algebra’s deepest ideas: every matrix transformation is built from eigen-geometry, stretched into a form that always exists and always makes sense.\n\n\n\n84. Low-Rank Approximation (Best Small Models)\nA central idea in data analysis, scientific computing, and machine learning is that many datasets or matrices are far more complicated in raw form than they truly need to be. Much of the apparent complexity hides redundancy, noise, or low-dimensional patterns. Low-rank approximation is the process of compressing a large, complicated matrix into a smaller, simpler version that preserves the most important information. This concept, grounded in the Singular Value Decomposition (SVD), lies at the heart of dimensionality reduction, recommender systems, and modern AI.\n\nThe General Problem\nSuppose we have a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), perhaps representing:\n\nAn image, with rows as pixels and columns as color channels.\nA ratings table, with rows as users and columns as movies.\nA word embedding matrix, with rows as words and columns as features.\n\nOften, \\(A\\) is very large but highly structured. The question is:\nCan we find a smaller matrix \\(B\\) of rank \\(k\\) (where \\(k \\ll \\min(m, n)\\)) that approximates \\(A\\) well?\n\n\nRank and Complexity\nThe rank of a matrix is the number of independent directions it encodes. High rank means complexity; low rank means redundancy.\n\nA rank-1 matrix can be written as an outer product of two vectors: \\(uv^T\\).\nA rank-\\(k\\) matrix is a sum of \\(k\\) such outer products.\nLimiting rank controls how much structure we allow the approximation to capture.\n\n\n\nThe SVD Solution\nThe SVD provides a natural decomposition:\n\\[\nA = U \\Sigma V^T,\n\\]\nwhere singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r\\) measure importance.\nTo approximate \\(A\\) with rank \\(k\\):\n\\[\nA_k = U_k \\Sigma_k V_k^T,\n\\]\nwhere we keep only the top \\(k\\) singular values and vectors.\nThis is not just a heuristic: it is the Eckart–Young theorem:\n\nAmong all rank-\\(k\\) matrices, \\(A_k\\) minimizes the error \\(\\|A - B\\|\\) (both in Frobenius and spectral norm).\n\nThus, SVD provides the best possible low-rank approximation.\n\n\nGeometric Intuition\n\nEach singular value \\(\\sigma_i\\) measures how strongly \\(A\\) stretches in the direction of singular vector \\(v_i\\).\nKeeping the top \\(k\\) singular values means keeping the most important stretches and ignoring weaker directions.\nThe approximation captures the “essence” of \\(A\\) while discarding small, noisy, or redundant effects.\n\n\n\nExamples\n\nImages A grayscale image can be stored as a matrix of pixel intensities. Using SVD, one can compress it by keeping only the largest singular values:\n\n\n\\(k = 10\\): blurry but recognizable image.\n\\(k = 50\\): much sharper, yet storage cost is far less than full.\n\\(k = 200\\): nearly indistinguishable from the original.\n\nThis is practical image compression: fewer numbers, same perception.\n\nRecommender Systems Consider a user–movie rating matrix. Although it may be huge, the true patterns (genre preferences, popularity trends) live in a low-dimensional subspace. A rank-\\(k\\) approximation captures these patterns, predicting missing ratings by filling in the structure.\nNatural Language Processing (NLP) Word embeddings often arise from co-occurrence matrices. Low-rank approximation via SVD extracts semantic structure, enabling words like “king,” “queen,” and “crown” to cluster together.\n\n\n\nError and Trade-Offs\n\nError decay: If singular values drop quickly, small \\(k\\) gives a great approximation. If they decay slowly, more terms are needed.\nEnergy preserved: The squared singular values \\(\\sigma_i^2\\) represent variance captured. Keeping the first \\(k\\) terms preserves most of the “energy.”\nBalance: Too low rank = oversimplification (loss of structure). Too high rank = no compression.\n\n\n\nPractical Computation\nFor very large matrices, full SVD is expensive (\\(O(mn^2)\\) for \\(m \\geq n\\)). Alternatives include:\n\nTruncated SVD algorithms (Lanczos, randomized methods).\nIterative methods that compute only the top \\(k\\) singular values.\nIncremental approaches that update low-rank models as new data arrives.\n\nThese are vital in modern data science, where datasets often have millions of entries.\n\n\nAnalogy\n\nMusic playlist: Imagine a playlist with hundreds of songs, but most are variations on a few themes. A low-rank approximation is like keeping only the core melodies while discarding repetitive riffs.\nPhotograph compression: Keeping only the brightest and most important strokes of light, while ignoring faint and irrelevant details.\nBook summary: Instead of the full text, you read the essential plot points. That’s low-rank approximation.\n\n\n\nWhy It Matters\n\nReveals hidden structure in high-dimensional data.\nReduces storage and computational cost.\nFilters noise while preserving the signal.\nProvides the foundation for PCA, recommender systems, and dimensionality reduction.\n\n\n\nTry It Yourself\n\nTake a small \\(5 \\times 5\\) random matrix. Compute its SVD. Construct the best rank-1 approximation. Compare to the original.\nDownload a grayscale image (e.g., \\(256 \\times 256\\)). Reconstruct it with 10, 50, and 100 singular values. Visually compare.\nProve the Eckart–Young theorem for the spectral norm: why can no other rank-\\(k\\) approximation do better than truncated SVD?\nFor a dataset with many features, compute PCA and explain why it is equivalent to finding a low-rank approximation.\n\nLow-rank approximation shows how linear algebra captures the essence of complexity: most of what matters lives in a small number of dimensions. The art is in finding and using them effectively.\n\n\n\n85. Principal Component Analysis (Variance and Directions)\nPrincipal Component Analysis (PCA) is one of the most widely used techniques in statistics, data analysis, and machine learning. It provides a method to reduce the dimensionality of a dataset while retaining as much important information as possible. The central insight is that data often varies more strongly in some directions than others, and by focusing on those directions we can summarize the dataset with fewer dimensions, less noise, and more interpretability.\n\nThe Basic Question\nSuppose we have data points in high-dimensional space, say \\(x_1, x_2, \\dots, x_m \\in \\mathbb{R}^n\\). Each point might be:\n\nA face image flattened into thousands of pixels.\nA customer’s shopping history across hundreds of products.\nA gene expression profile across thousands of genes.\n\nStoring and working with all features directly is expensive, and many features may be redundant or correlated. PCA asks:\nCan we re-express this data in a smaller set of directions that capture the most variability?\n\n\nVariance as Information\nThe guiding principle of PCA is variance.\n\nVariance measures how spread out the data is along a direction.\nHigh variance directions capture meaningful structure (e.g., different facial expressions, major spending habits).\nLow variance directions often correspond to noise or unimportant fluctuations.\n\nThus, PCA searches for the directions (called principal components) along which the variance of the data is maximized.\n\n\nCentering and Covariance\nTo begin, we center the data by subtracting the mean vector:\n\\[\nX_c = X - \\mathbf{1}\\mu^T,\n\\]\nwhere \\(\\mu\\) is the average of all data points.\nThe covariance matrix is then:\n\\[\nC = \\frac{1}{m} X_c^T X_c.\n\\]\n\nThe diagonal entries measure variance of each feature.\nOff-diagonal entries measure how features vary together.\n\nFinding principal components is equivalent to finding the eigenvectors of this covariance matrix.\n\n\nThe Eigenview\n\nThe eigenvectors of \\(C\\) are the directions (principal components).\nThe corresponding eigenvalues tell us how much variance lies along each component.\nSorting eigenvalues from largest to smallest gives the most informative to least informative directions.\n\nIf we keep the top \\(k\\) eigenvectors, we project data into a \\(k\\)-dimensional subspace that preserves most variance.\n\n\nThe SVD View\nAnother perspective uses the Singular Value Decomposition (SVD):\n\\[\nX_c = U \\Sigma V^T.\n\\]\n\nColumns of \\(V\\) are the principal directions.\nSingular values squared (\\(\\sigma_i^2\\)) correspond to eigenvalues of the covariance matrix.\nProjecting onto the first \\(k\\) columns of \\(V\\) gives the reduced representation.\n\nThis makes PCA and SVD essentially the same computation.\n\n\nA Simple Example\nImagine we measure height and weight of 1000 people. Plotting them shows a strong correlation: taller people are often heavier. The cloud of points stretches along a diagonal.\n\nPCA’s first component is this diagonal line: the direction of maximum variance.\nThe second component is perpendicular, capturing the much smaller differences (like people of equal height but slightly different weights).\nKeeping only the first component reduces two features into one while retaining most of the information.\n\n\n\nGeometric Picture\n\nPCA rotates the coordinate system so that axes align with directions of greatest variance.\nProjecting onto the top \\(k\\) components flattens the data into a lower-dimensional space, like flattening a tilted pancake onto its broadest plane.\n\n\n\nApplications\n\nData Compression: Reduce storage by keeping only leading components (e.g., compressing images).\nNoise Reduction: Small-variance directions often correspond to measurement noise; discarding them yields cleaner data.\nVisualization: Reducing data to 2D or 3D for scatterplots helps us see clusters and patterns.\nPreprocessing in Machine Learning: Many models train faster and generalize better on PCA-transformed data.\nGenomics and Biology: PCA finds major axes of variation across thousands of genes.\nFinance: PCA summarizes correlated movements of stocks into a few principal “factors.”\n\n\n\nTrade-Offs and Limitations\n\nInterpretability: Principal components are linear combinations of original features, sometimes hard to explain in plain terms.\nLinearity: PCA only captures linear relationships; nonlinear methods (like kernel PCA, t-SNE, or UMAP) may be better for curved manifolds.\nScaling: Features must be normalized properly; otherwise, PCA might overemphasize units with large raw variance.\nGlobal Method: PCA captures overall variance, not local structures (e.g., small clusters within the data).\n\n\n\nMathematical Guarantees\nPCA has an optimality guarantee:\n\nAmong all \\(k\\)-dimensional linear subspaces, the PCA subspace minimizes the reconstruction error (squared Euclidean distance between data and its projection).\nThis is essentially the low-rank approximation theorem seen earlier, applied to covariance matrices.\n\n\n\nWhy It Matters\nPCA shows how linear algebra transforms raw data into insight. By focusing on variance, it provides a principled way to filter noise, compress information, and reveal hidden patterns. It is simple, computationally efficient, and foundational-almost every modern data pipeline uses PCA, explicitly or implicitly.\n\n\nTry It Yourself\n\nTake a dataset of two correlated features (like height and weight). Compute the covariance matrix, eigenvectors, and project onto the first component. Visualize before and after.\nFor a grayscale image stored as a matrix, flatten it into vectors and apply PCA. How many components are needed to reconstruct it with 90% accuracy?\nUse PCA on the famous Iris dataset (4 features). Plot the data in 2D using the first two components. Notice how species separate in this reduced space.\nProve that the first principal component is the unit vector \\(v\\) that maximizes \\(\\|X_c v\\|^2\\).\n\nPCA distills complexity into clarity: it tells us not just where the data is, but where it really goes.\n\n\n\n86. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems\nIn linear algebra, the inverse of a matrix is a powerful tool: if \\(A\\) is invertible, then solving \\(Ax = b\\) is as simple as \\(x = A^{-1}b\\). But what happens when \\(A\\) is not square, or not invertible? In practice, this is the norm: many problems involve rectangular matrices (more equations than unknowns, or more unknowns than equations), or square matrices that are singular. The Moore–Penrose pseudoinverse, usually denoted \\(A^+\\), generalizes the idea of an inverse to all matrices, providing a systematic way to find solutions-or best approximations-when ordinary inversion fails.\n\nWhy Ordinary Inverses Fail\n\nNon-square matrices: If \\(A\\) is \\(m \\times n\\) with \\(m \\neq n\\), no standard inverse exists.\nSingular matrices: Even if \\(A\\) is square, if \\(\\det(A) = 0\\), it has no inverse.\nIll-posed problems: In real-world data, exact solutions may not exist (inconsistent systems) or may not be unique (underdetermined systems).\n\nDespite these obstacles, we still want a systematic way to solve or approximate \\(Ax = b\\).\n\n\nDefinition of the Pseudoinverse\nThe Moore–Penrose pseudoinverse \\(A^+\\) is defined as the unique matrix that satisfies four properties:\n\n\\(AA^+A = A\\).\n\\(A^+AA^+ = A^+\\).\n\\((AA^+)^T = AA^+\\).\n\\((A^+A)^T = A^+A\\).\n\nThese conditions ensure \\(A^+\\) acts as an “inverse” in the broadest consistent sense.\n\n\nConstructing the Pseudoinverse with SVD\nGiven the SVD of \\(A\\):\n\\[\nA = U \\Sigma V^T,\n\\]\nwhere \\(\\Sigma\\) is diagonal with singular values \\(\\sigma_1, \\dots, \\sigma_r\\), the pseudoinverse is:\n\\[\nA^+ = V \\Sigma^+ U^T,\n\\]\nwhere \\(\\Sigma^+\\) is formed by inverting nonzero singular values and transposing the matrix. Specifically:\n\nIf \\(\\sigma_i \\neq 0\\), replace it with \\(1/\\sigma_i\\).\nIf \\(\\sigma_i = 0\\), leave it as 0.\n\nThis definition works for all matrices, square or rectangular.\n\n\nSolving Linear Systems with \\(A^+\\)\n\nOverdetermined systems (\\(m &gt; n\\), more equations than unknowns):\n\nOften no exact solution exists.\nThe pseudoinverse gives the least-squares solution:\n\\[\nx = A^+ b,\n\\]\nwhich minimizes \\(\\|Ax - b\\|\\).\n\nUnderdetermined systems (\\(m &lt; n\\), more unknowns than equations):\n\nInfinitely many solutions exist.\nThe pseudoinverse chooses the solution with the smallest norm:\n\\[\nx = A^+ b,\n\\]\nwhich minimizes \\(\\|x\\|\\) among all solutions.\n\nSquare but singular systems:\n\nSome solutions exist, but not uniquely.\nThe pseudoinverse again picks the least-norm solution.\n\n\n\n\nExample 1: Overdetermined\nSuppose we want to solve:\n\\[\n\\begin{bmatrix}1 & 1 \\\\ 1 & -1 \\\\ 1 & 0\\end{bmatrix} x = \\begin{bmatrix}2 \\\\ 0 \\\\ 1\\end{bmatrix}.\n\\]\nThis \\(3 \\times 2\\) system has no exact solution. Using the pseudoinverse, we obtain the least-squares solution that best fits all three equations simultaneously.\n\n\nExample 2: Underdetermined\nFor\n\\[\n\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix} x = \\begin{bmatrix}3 \\\\ 4\\end{bmatrix},\n\\]\nthe system has infinitely many solutions because \\(x_3\\) is free. The pseudoinverse gives:\n\\[\nx = \\begin{bmatrix}3 \\\\ 4 \\\\ 0\\end{bmatrix},\n\\]\nchoosing the solution with minimum norm.\n\n\nGeometric Interpretation\n\nThe pseudoinverse acts like projecting onto subspaces.\nFor overdetermined systems, it projects \\(b\\) onto the column space of \\(A\\), then finds the closest \\(x\\).\nFor underdetermined systems, it picks the point in the solution space closest to the origin.\n\nSo \\(A^+\\) embodies the principle of “best possible inverse” under the circumstances.\n\n\nApplications\n\nLeast-Squares Regression: Solving \\(\\min_x \\|Ax - b\\|^2\\) via \\(A^+\\).\nSignal Processing: Reconstructing signals from incomplete or noisy data.\nControl Theory: Designing inputs when exact control is impossible.\nMachine Learning: Training models with non-invertible design matrices.\nStatistics: Computing generalized inverses of covariance matrices.\n\n\n\nLimitations\n\nSensitive to very small singular values: numerical instability may occur.\nRegularization (like ridge regression) is often preferred in noisy settings.\nComputationally expensive for very large matrices, though truncated SVD can help.\n\n\n\nWhy It Matters\nThe pseudoinverse is a unifying idea: it handles inconsistent, underdetermined, or singular problems with one formula. It ensures we always have a principled answer, even when classical algebra says “no solution” or “infinitely many solutions.” In real data analysis, almost every problem is ill-posed to some degree, making the pseudoinverse a practical cornerstone of modern applied linear algebra.\n\n\nTry It Yourself\n\nCompute the pseudoinverse of a simple \\(2 \\times 2\\) singular matrix by hand using SVD.\nSolve both an overdetermined (\\(3 \\times 2\\)) and underdetermined (\\(2 \\times 3\\)) system using \\(A^+\\). Compare with intuitive expectations.\nExplore what happens numerically when singular values are very small. Try truncating them-this connects to regularization.\n\nThe Moore–Penrose pseudoinverse shows that even when linear systems are “broken,” linear algebra still provides a systematic way forward.\n\n\n\n87. Conditioning and Sensitivity (How Errors Amplify)\nLinear algebra is not only about exact solutions-it is also about how stable those solutions are when data is perturbed. In real-world applications, every dataset contains noise: measurement errors in physics experiments, rounding errors in financial computations, or floating-point precision limits in numerical software. Conditioning is the study of how sensitive the solution of a problem is to small changes in input. A well-conditioned problem reacts gently to perturbations; an ill-conditioned one amplifies errors dramatically.\n\nThe Basic Idea\nSuppose we solve the linear system:\n\\[\nAx = b.\n\\]\nNow imagine we slightly change \\(b\\) to \\(b + \\delta b\\). The new solution is \\(x + \\delta x\\).\n\nIf \\(\\|\\delta x\\|\\) is about the same size as \\(\\|\\delta b\\|\\), the problem is well-conditioned.\nIf \\(\\|\\delta x\\|\\) is much larger, the problem is ill-conditioned.\n\nConditioning measures this amplification factor.\n\n\nCondition Number\nThe central tool is the condition number of a matrix \\(A\\):\n\\[\n\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|,\n\\]\nwhere \\(\\|\\cdot\\|\\) is a matrix norm (often the 2-norm).\n\nIf \\(\\kappa(A)\\) is close to 1, the problem is well-conditioned.\nIf \\(\\kappa(A)\\) is large (say, \\(10^6\\) or higher), the problem is ill-conditioned.\n\nInterpretation:\n\n\\(\\kappa(A)\\) estimates the maximum relative error in the solution compared to the relative error in the data.\nIn practical terms, every digit of accuracy in \\(b\\) may be lost in \\(x\\) if \\(\\kappa(A)\\) is too large.\n\n\n\nSingular Values and Conditioning\nCondition number in 2-norm can be expressed using singular values:\n\\[\n\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}},\n\\]\nwhere \\(\\sigma_{\\max}\\) and \\(\\sigma_{\\min}\\) are the largest and smallest singular values of \\(A\\).\n\nIf the smallest singular value is tiny compared to the largest, \\(A\\) nearly collapses some directions, making inversion unstable.\nThis explains why nearly singular matrices are so problematic in numerical computation.\n\n\n\nExample 1: A Stable System\n\\[\nA = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}.\n\\]\nHere, \\(\\sigma_{\\max} = 3, \\sigma_{\\min} = 2\\). So \\(\\kappa(A) = 3/2 = 1.5\\). Very well-conditioned: small changes in input produce small changes in output.\n\n\nExample 2: An Ill-Conditioned System\n\\[\nA = \\begin{bmatrix}1 & 1 \\\\ 1 & 1.0001\\end{bmatrix}.\n\\]\nThe determinant is very small, so the system is nearly singular.\n\nOne singular value is about 2.0.\nThe other is about 0.0001.\nCondition number: \\(\\kappa(A) \\approx 20000\\).\n\nThis means even tiny changes in \\(b\\) can wildly change \\(x\\).\n\n\nGeometric Intuition\nA matrix transforms a unit sphere into an ellipse.\n\nThe longest axis of the ellipse = \\(\\sigma_{\\max}\\).\nThe shortest axis = \\(\\sigma_{\\min}\\).\nThe ratio \\(\\sigma_{\\max} / \\sigma_{\\min}\\) shows how stretched the transformation is.\n\nIf the ellipse is nearly flat, directions aligned with the short axis almost vanish, and recovering them is highly unstable.\n\n\nWhy Conditioning Matters in Computation\n\nNumerical Precision: Computers store numbers with limited precision (floating-point). An ill-conditioned system magnifies rounding errors, leading to unreliable results.\nRegression: In statistics, highly correlated features make the design matrix ill-conditioned, destabilizing coefficient estimates.\nMachine Learning: Ill-conditioning leads to unstable training, exploding or vanishing gradients.\nEngineering: Control systems based on ill-conditioned models may be hypersensitive to measurement errors.\n\n\n\nTechniques for Handling Ill-Conditioning\n\nRegularization: Add a penalty term, like ridge regression (\\(\\lambda I\\)), to stabilize inversion.\nTruncated SVD: Ignore tiny singular values that only amplify noise.\nScaling and Preconditioning: Rescale data or multiply by a well-chosen matrix to improve conditioning.\nAvoiding Explicit Inverses: Use factorizations (LU, QR, SVD) rather than computing \\(A^{-1}\\).\n\n\n\nConnection to Previous Topics\n\nPseudoinverse: Ill-conditioning is visible when singular values approach zero, making \\(A^+\\) unstable.\nLow-rank approximation: Truncating small singular values both compresses data and improves conditioning.\nPCA: Discarding low-variance components is essentially a conditioning improvement step.\n\n\n\nWhy It Matters\nConditioning bridges abstract algebra and numerical reality. Linear algebra promises solutions, but conditioning tells us whether those solutions are trustworthy. Without it, one might misinterpret noise as signal, or lose all accuracy in computations that look fine on paper.\n\n\nTry It Yourself\n\nCompute the condition number of \\(\\begin{bmatrix}1 & 1 \\\\ 1 & 1.0001\\end{bmatrix}\\). Solve for \\(x\\) in \\(Ax = b\\) for several slightly different \\(b\\). Watch how solutions swing dramatically.\nTake a dataset with nearly collinear features. Compute the condition number of its covariance matrix. Relate this to instability in regression coefficients.\nSimulate numerical errors: Add random noise of size \\(10^{-6}\\) to an ill-conditioned system and observe solution errors.\nProve that \\(\\kappa(A) \\geq 1\\) always holds.\n\nConditioning reveals the hidden fragility of problems. It warns us when algebra says “solution exists” but computation whispers “don’t trust it.”\n\n\n\n88. Matrix Norms and Singular Values (Measuring Size Properly)\nIn linear algebra, we often need to measure the “size” of a matrix. For vectors, this is straightforward: the length (norm) tells us how big the vector is. But for matrices, the question is more subtle: do we measure size by entries, by how much the matrix stretches vectors, or by some invariant property? Different contexts demand different answers, and matrix norms-closely tied to singular values-provide the framework for doing so.\n\nWhy Measure the Size of a Matrix?\n\nStability: To know how much error a matrix might amplify.\nConditioning: The ratio of largest to smallest stretching.\nOptimization: Many algorithms minimize some matrix norm.\nData analysis: Norms measure complexity or energy of data.\n\nWithout norms, we cannot compare matrices, analyze sensitivity, or judge approximation quality.\n\n\nMatrix Norms from Vector Norms\nA natural way to define a matrix norm is to ask: How much does this matrix stretch vectors?\nFormally, for a given vector norm \\(\\|\\cdot\\|\\):\n\\[\n\\|A\\| = \\max_{x \\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}.\n\\]\nThis is called the induced matrix norm.\n\n\nThe 2-Norm and Singular Values\nWhen we use the Euclidean norm (\\(\\|x\\|_2\\)) for vectors, the induced matrix norm becomes:\n\\[\n\\|A\\|_2 = \\sigma_{\\max}(A),\n\\]\nthe largest singular value of \\(A\\).\n\nThis means the 2-norm measures the maximum stretching factor.\nGeometrically: \\(A\\) maps the unit sphere into an ellipse; \\(\\|A\\|_2\\) is the length of the ellipse’s longest axis.\n\nThis link makes singular values the natural language for matrix size.\n\n\nOther Common Norms\n\nFrobenius Norm\n\n\\[\n\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}.\n\\]\n\nEquivalent to the Euclidean length of all entries stacked in one big vector.\nCan also be expressed as:\n\\[\n\\|A\\|_F^2 = \\sum_i \\sigma_i^2.\n\\]\nOften used in data science and machine learning because it is easy to compute and differentiable.\n\n\n1-Norm\n\n\\[\n\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|,\n\\]\nthe maximum absolute column sum.\n\nInfinity Norm\n\n\\[\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|,\n\\]\nthe maximum absolute row sum.\nBoth are computationally cheap, useful in numerical analysis.\n\nNuclear Norm (Trace Norm)\n\n\\[\n\\|A\\|_* = \\sum_i \\sigma_i,\n\\]\nthe sum of singular values.\n\nImportant in low-rank approximation and machine learning (matrix completion, recommender systems).\n\n\n\nSingular Values as the Unifying Thread\n\nSpectral norm (2-norm): maximum singular value.\nFrobenius norm: root of the sum of squared singular values.\nNuclear norm: sum of singular values.\n\nThus, norms capture different ways of summarizing singular values: maximum, sum, or energy.\n\n\nExample: Small Matrix\nTake\n\\[\nA = \\begin{bmatrix}3 & 4 \\\\ 0 & 0\\end{bmatrix}.\n\\]\n\nSingular values: \\(\\sigma_1 = 5, \\sigma_2 = 0\\).\n\\(\\|A\\|_2 = 5\\).\n\\(\\|A\\|_F = \\sqrt{3^2 + 4^2} = 5\\).\n\\(\\|A\\|_* = 5\\).\n\nHere, different norms coincide, but generally they highlight different aspects of the matrix.\n\n\nGeometric Intuition\n\n2-norm: “How much can \\(A\\) stretch a vector?”\nFrobenius norm: “What is the overall energy in all entries?”\n1-norm / ∞-norm: “What is the heaviest column or row load?”\nNuclear norm: “How much total stretching power does \\(A\\) have?”\n\nEach is a lens, giving a different perspective.\n\n\nApplications\n\nNumerical Stability: Condition number \\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\) uses the spectral norm.\nMachine Learning: Nuclear norm is used for matrix completion (Netflix Prize).\nImage Compression: Frobenius norm measures reconstruction error.\nControl Theory: 1-norm and ∞-norm bound system responses.\nOptimization: Norms serve as penalties or constraints, shaping solutions.\n\n\n\nWhy It Matters\nMatrix norms provide the language to compare, approximate, and control matrices. Singular values ensure that this language is not arbitrary but grounded in geometry. Together, they explain how matrices distort space, how error grows, and how we can measure complexity.\n\n\nTry It Yourself\n\nFor \\(A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\), compute \\(\\|A\\|_1\\), \\(\\|A\\|_\\infty\\), \\(\\|A\\|_F\\), and \\(\\|A\\|_2\\) (using SVD for the last). Compare.\nProve that \\(\\|A\\|_F^2 = \\sum \\sigma_i^2\\).\nShow that \\(\\|A\\|_2 \\leq \\|A\\|_F \\leq \\|A\\|_*\\). Interpret geometrically.\nConsider a rank-1 matrix \\(uv^T\\). What are its norms? Which are equal?\n\nMatrix norms and singular values are the measuring sticks of linear algebra-they tell us not just how big a matrix is, but how it acts, where it is stable, and when it is fragile.\n\n\n\n89. Regularization (Ridge/Tikhonov to Tame Instability)\nWhen solving linear systems or regression problems, instability often arises because the system is ill-conditioned: tiny errors in data lead to huge swings in the solution. Regularization is the strategy of adding stability by deliberately modifying the problem, sacrificing exactness for robustness. The two most common approaches-ridge regression and Tikhonov regularization-embody this principle.\n\nThe Problem of Instability\nConsider the least-squares problem:\n\\[\n\\min_x \\|Ax - b\\|_2^2.\n\\]\nIf \\(A\\) has nearly dependent columns, or if \\(\\sigma_{\\min}(A)\\) is very small, then:\n\nSolutions are unstable.\nCoefficients \\(x\\) can explode in magnitude.\nPredictions vary wildly with small changes in \\(b\\).\n\nRegularization modifies the objective so that the solution prefers stability over exactness.\n\n\nRidge / Tikhonov Regularization\nThe modified problem is:\n\\[\n\\min_x \\big( \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\big),\n\\]\nwhere \\(\\lambda &gt; 0\\) is the regularization parameter.\n\nThe first term enforces data fit.\nThe second term penalizes large coefficients, discouraging unstable solutions.\n\nThis is called ridge regression in statistics and Tikhonov regularization in numerical analysis.\n\n\nThe Closed-Form Solution\nExpanding the objective and differentiating gives:\n\\[\nx_\\lambda = (A^T A + \\lambda I)^{-1} A^T b.\n\\]\nKey points:\n\nThe added \\(\\lambda I\\) makes the matrix invertible, even if \\(A^T A\\) is singular.\nAs \\(\\lambda \\to 0\\), the solution approaches the ordinary least-squares solution.\nAs \\(\\lambda \\to \\infty\\), the solution shrinks toward 0.\n\n\n\nSVD View\nIf \\(A = U \\Sigma V^T\\), then the least-squares solution is:\n\\[\nx = \\sum_i \\frac{u_i^T b}{\\sigma_i} v_i.\n\\]\nIf \\(\\sigma_i\\) is very small, the term \\(\\frac{1}{\\sigma_i}\\) causes instability.\nWith regularization:\n\\[\nx_\\lambda = \\sum_i \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} (u_i^T b) v_i.\n\\]\n\nSmall singular values (unstable directions) are suppressed.\nLarge singular values (stable directions) are mostly preserved.\n\nThis explains why ridge regression stabilizes solutions: it damps noise-amplifying directions.\n\n\nGeometric Interpretation\n\nThe unregularized problem fits \\(b\\) exactly in the column space of \\(A\\).\nRegularization tilts the solution toward the origin, shrinking coefficients.\nGeometrically, the feasible region (ellipsoid from \\(Ax\\)) intersects with a ball constraint from \\(\\|x\\|_2\\). The solution is where these two shapes balance.\n\n\n\nExtensions\n\nLasso (\\(\\ell_1\\) regularization): Replaces \\(\\|x\\|_2^2\\) with \\(\\|x\\|_1\\), encouraging sparse solutions.\nElastic Net: Combines ridge and lasso penalties.\nGeneral Tikhonov: Uses \\(\\|Lx\\|_2^2\\) with some matrix \\(L\\), tailoring the penalty (e.g., smoothing in signal processing).\nBayesian View: Ridge regression corresponds to placing a Gaussian prior on coefficients.\n\n\n\nApplications\n\nMachine Learning: Prevents overfitting in regression and classification.\nSignal Processing: Suppresses noise when reconstructing signals.\nImage Reconstruction: Stabilizes inverse problems like deblurring.\nNumerical PDEs: Adds smoothness constraints to solutions.\nEconometrics and Finance: Controls instability from highly correlated variables.\n\n\n\nWhy It Matters\nRegularization transforms fragile problems into reliable ones. It acknowledges the reality of noise and finite precision, and instead of chasing impossible exactness, it provides usable, stable answers. In modern data-driven fields, almost every large-scale model relies on regularization for robustness.\n\n\nTry It Yourself\n\nSolve the system \\(Ax = b\\) where\n\\[\nA = \\begin{bmatrix}1 & 1 \\\\ 1 & 1.0001\\end{bmatrix}, \\quad b = \\begin{bmatrix}2 \\\\ 2\\end{bmatrix}.\n\\]\nCompare the unregularized least-squares solution with ridge-regularized solutions for \\(\\lambda = 0.01, 1, 10\\).\nUsing the SVD, show how coefficients for small singular values are shrunk.\nIn regression with many correlated features, compute coefficient paths as \\(\\lambda\\) varies. Observe how they stabilize.\nExplore image denoising: apply ridge regularization to a blurred/noisy image reconstruction problem.\n\nRegularization shows the wisdom of linear algebra in practice: sometimes the best solution is not the exact one, but the stable one.\n\n\n\n90. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)\nRank-the number of independent directions in a matrix-is central to linear algebra. It tells us about solvability of systems, redundancy of features, and the dimensionality of data. But in practice, computing rank is not as simple as counting pivots or checking determinants. Real-world data is noisy, nearly dependent, or high-dimensional. Rank-revealing QR (RRQR) factorization and related diagnostics provide stable, practical tools for uncovering rank and structure.\n\nWhy Rank Matters\n\nLinear systems: Rank determines if a system has a unique solution, infinitely many, or none.\nData science: Rank measures intrinsic dimensionality, guiding dimensionality reduction.\nNumerics: Small singular values make effective rank ambiguous-exact vs. numerical rank diverge.\n\nThus, we need reliable algorithms to decide “how many directions matter” in a matrix.\n\n\nExact Rank vs. Numerical Rank\n\nExact rank: Defined over exact arithmetic. A column is independent if it cannot be expressed as a linear combination of others.\nNumerical rank: In floating-point computation, tiny singular values cannot be trusted. A threshold \\(\\epsilon\\) determines when we treat them as zero.\n\nFor example, if the smallest singular value of \\(A\\) is \\(10^{-12}\\), and computations are in double precision (\\(\\sim 10^{-16}\\)), we might consider the effective rank smaller than full.\n\n\nThe QR Factorization Recap\nThe basic QR factorization expresses a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) as:\n\\[\nA = QR,\n\\]\nwhere:\n\n\\(Q\\) is orthogonal (\\(Q^T Q = I\\)), preserving lengths.\n\\(R\\) is upper triangular, holding the “essence” of \\(A\\).\n\nQR is stable, fast, and forms the backbone of many algorithms.\n\n\nRank-Revealing QR (RRQR)\nRRQR is an enhancement of QR with column pivoting:\n\\[\nA P = Q R,\n\\]\nwhere \\(P\\) is a permutation matrix that reorders columns.\n\nThe pivoting ensures that the largest independent directions come first.\nThe diagonal entries of \\(R\\) indicate which columns are significant.\nSmall values on the diagonal signal dependent (or nearly dependent) directions.\n\nIn practice, RRQR allows us to approximate rank by examining the decay of \\(R\\)’s diagonal.\n\n\nComparing RRQR and SVD\n\nSVD: Gold standard for determining rank; singular values give exact scaling of each direction.\nRRQR: Faster and cheaper; sufficient when approximate rank is enough.\nTrade-off: SVD is more accurate, RRQR is more efficient.\n\nBoth are used depending on the balance of precision and cost.\n\n\nExample\nLet\n\\[\nA = \\begin{bmatrix}1 & 1 & 1 \\\\ 1 & 1.0001 & 2 \\\\ 1 & 2 & 3\\end{bmatrix}.\n\\]\n\nExact arithmetic: rank = 3.\nNumerically: second column is nearly dependent on the first. SVD shows a singular value near zero.\nRRQR with pivoting identifies the near-dependence by revealing a tiny diagonal in \\(R\\).\n\nThus, RRQR “reveals” effective rank without fully computing SVD.\n\n\nPractical Diagnostics for Rank\n\nCondition Number: A high condition number suggests near-rank-deficiency.\nDiagonal of R in RRQR: Monitors independence of columns.\nSingular Values in SVD: Most reliable indicator, but expensive.\nDeterminants/Minors: Useful in theory, unstable in practice.\n\n\n\nApplications\n\nData Compression: Identifying effective rank allows truncation.\nRegression: Detecting multicollinearity by examining rank of the design matrix.\nControl Systems: Rank tests stability and controllability.\nMachine Learning: Dimensionality reduction pipelines (e.g., PCA) start with rank estimation.\nSignal Processing: Identifying number of underlying sources from mixtures.\n\n\n\nWhy It Matters\nRank is simple in theory, but elusive in practice. RRQR and related diagnostics bridge the gap between exact mathematics and noisy data. They allow practitioners to say, with stability and confidence: this is how many independent directions really matter.\n\n\nTry It Yourself\n\nImplement RRQR with column pivoting on a small \\(5 \\times 5\\) nearly dependent matrix. Compare estimated rank with SVD.\nExplore the relationship between diagonal entries of \\(R\\) and numerical rank.\nConstruct a dataset with 100 features, where 95 are random noise but 5 are linear combinations. Use RRQR to detect redundancy.\nProve that column pivoting does not change the column space of \\(A\\), only its numerical stability.\n\nRank-revealing QR shows that linear algebra is not only about exact formulas but also about practical diagnostics-knowing when two directions are truly different and when they are essentially the same.\n\n\nClosing\nNoise reduced to still,\nsingular values unfold space,\nessence shines within.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-10.-applications-and-computation",
    "href": "books/en-US/book.html#chapter-10.-applications-and-computation",
    "title": "The Book",
    "section": "Chapter 10. Applications and computation",
    "text": "Chapter 10. Applications and computation\n\nOpening\nWorlds in numbers bloom,\ngraphs and data interlace,\nalgebra takes flight.\n\n\n91. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)\nLinear algebra is the silent backbone of modern graphics, robotics, and computer vision. Every time an image is rendered on a screen, a camera captures a scene, or a robot arm moves in space, a series of matrix multiplications are transforming points from one coordinate system to another. These geometry pipelines map 3D reality into 2D representations, ensuring that objects appear in the correct position, orientation, and scale.\n\nThe Geometry of Coordinates\nA point in 3D space is represented as a column vector:\n\\[\np = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}.\n\\]\nBut computers often extend this to homogeneous coordinates, embedding the point in 4D:\n\\[\np_h = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}.\n\\]\nThe extra coordinate allows translations to be represented as matrix multiplications, keeping the entire pipeline consistent: every step is just multiplying by a matrix.\n\n\nTransformations in 2D and 3D\n\nTranslation Moves a point by \\((t_x, t_y, t_z)\\).\n\\[\nT = \\begin{bmatrix}\n1 & 0 & 0 & t_x \\\\\n0 & 1 & 0 & t_y \\\\\n0 & 0 & 1 & t_z \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n\\]\nScaling Expands or shrinks space along each axis.\n\\[\nS = \\begin{bmatrix}\ns_x & 0 & 0 & 0 \\\\\n0 & s_y & 0 & 0 \\\\\n0 & 0 & s_z & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n\\]\nRotation In 3D, rotation around the z-axis is:\n\\[\nR_z(\\theta) = \\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta & 0 & 0 \\\\\n\\sin\\theta & \\cos\\theta & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n\\]\nSimilar forms exist for rotations around the x- and y-axes.\n\nEach transformation is linear (or affine), and chaining them is just multiplying matrices.\n\n\nThe Camera Pipeline\nRendering a 3D object to a 2D image follows a sequence of steps, each one a matrix multiplication:\n\nModel Transform Moves the object from its local coordinates into world coordinates.\nView Transform Puts the camera at the origin and aligns its axes with the world, effectively changing the point of view.\nProjection Transform Projects 3D points into 2D. Two types:\n\nOrthographic: parallel projection, no perspective.\nPerspective: distant objects appear smaller, closer to human vision.\n\nExample of perspective projection:\n\\[\nP = \\begin{bmatrix}\nf & 0 & 0 & 0 \\\\\n0 & f & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix},\n\\]\nwhere \\(f\\) is focal length.\nViewport Transform Maps normalized 2D coordinates to screen pixels.\n\nThis sequence-from object to image-is the geometry pipeline.\n\n\nExample: Rendering a Cube\n\nStart with cube vertices in local coordinates (\\([-1,1]^3\\)).\nApply a scaling matrix to stretch it.\nApply a rotation matrix to tilt it.\nApply a translation matrix to move it into the scene.\nApply a projection matrix to flatten it onto the screen.\n\nEvery step is linear algebra, and the final picture is the result of multiplying many matrices in sequence.\n\n\nRobotics Connection\nRobotic arms use similar pipelines: each joint contributes a rotation or translation, encoded as a matrix. By multiplying them, we get the forward kinematics-the position and orientation of the hand given the joint angles.\n\n\nWhy It Matters\nGeometry pipelines unify graphics, robotics, and vision. They show how linear algebra powers the everyday visuals of video games, animations, simulations, and even self-driving cars. Without the consistency of matrix multiplication, the complexity of managing transformations would be unmanageable.\n\n\nTry It Yourself\n\nWrite down the sequence of matrices that rotate a square by 45°, scale it by 2, and translate it by \\((3, 1)\\). Multiply them to get the combined transformation.\nConstruct a cube in 3D and simulate a perspective projection by hand for one vertex.\nFor a simple 2-joint robotic arm, represent each joint with a rotation matrix and compute the final hand position.\nProve that composing affine transformations is closed under multiplication-why does this make pipelines possible?\n\nGeometry pipelines are the bridge between abstract linear algebra and tangible visual and mechanical systems. They are how math becomes movement, light, and image.\n\n\n\n92. Computer Graphics and Robotics (Homogeneous Tricks in Action)\nLinear algebra doesn’t just stay on the chalkboard-it drives the engines of computer graphics and robotics. Both fields need to describe and manipulate objects in space, often moving between multiple coordinate systems. The homogeneous coordinate trick-adding one extra dimension-makes this elegant: translations, scalings, and rotations all fit into a single framework of matrix multiplication. This uniformity allows efficient computation and consistent pipelines.\n\nHomogeneous Coordinates Recap\nIn 2D, a point \\((x, y)\\) becomes \\([x, y, 1]^T\\). In 3D, a point \\((x, y, z)\\) becomes \\([x, y, z, 1]^T\\).\nWhy add the extra 1? Because then translations-normally not linear-become linear in the higher-dimensional embedding. Every affine transformation (rotations, scalings, shears, reflections, and translations) is just a single multiplication by a homogeneous matrix.\nExample:\n\\[\nT = \\begin{bmatrix}\n1 & 0 & 0 & t_x \\\\\n0 & 1 & 0 & t_y \\\\\n0 & 0 & 1 & t_z \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}, \\quad\np_h' = T p_h.\n\\]\nThis trick makes pipelines modular: just multiply the matrices in order.\n\n\nComputer Graphics Pipelines\nGraphics engines (like OpenGL or DirectX) rely entirely on homogeneous transformations:\n\nModel Matrix: Puts the object in the scene.\n\nExample: Rotate a car 90° and translate it 10 units forward.\n\nView Matrix: Positions the virtual camera.\n\nEquivalent to moving the world so the camera sits at the origin.\n\nProjection Matrix: Projects 3D points to 2D.\n\nPerspective projection shrinks faraway objects, orthographic doesn’t.\n\nViewport Matrix: Converts normalized 2D coordinates into screen pixels.\n\nEvery pixel you see in a video game has passed through this stack of matrices.\n\n\nRobotics Pipelines\nIn robotics, the same principle applies:\n\nA robot arm with joints is modeled as a chain of rigid-body transformations.\nEach joint contributes a rotation or translation matrix.\nMultiplying them gives the final pose of the robot’s end-effector (hand, tool, or gripper).\n\nThis is called forward kinematics.\nExample: A 2D robotic arm with two joints:\n\\[\np = R(\\theta_1) T(l_1) R(\\theta_2) T(l_2) \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\nHere \\(R(\\theta_i)\\) are rotation matrices and \\(T(l_i)\\) are translations along the arm length. Multiplying them gives the position of the hand.\n\n\nShared Challenges in Graphics and Robotics\n\nPrecision: Numerical round-off errors can accumulate; stable algorithms are critical.\nSpeed: Both fields demand real-time computation-60 frames per second for graphics, millisecond reaction times for robots.\nHierarchy: Objects in graphics may be nested (a car’s wheel rotates relative to the car), just like robot joints. Homogeneous transforms naturally handle these hierarchies.\nInverse Problems: Graphics uses inverse transforms for camera movement; robotics uses them for inverse kinematics (finding joint angles to reach a point).\n\n\n\nWhy Homogeneous Tricks Are Powerful\n\nUniformity: One system (matrix multiplication) handles all transformations.\nEfficiency: Hardware (GPUs, controllers) can optimize matrix operations directly.\nScalability: Works the same in 2D, 3D, or higher.\nComposability: Long pipelines are just products of matrices, avoiding special cases.\n\n\n\nApplications\n\nGraphics: Rendering engines, VR/AR, CAD software, motion capture.\nRobotics: Arm manipulators, drones, autonomous vehicles, humanoid robots.\nCrossover: Simulation platforms use the same math to test robots and render virtual environments.\n\n\n\nTry It Yourself\n\nBuild a 2D transformation pipeline: rotate a triangle, translate it, and project it into screen space. Write down the final transformation matrix.\nModel a simple 2-joint robotic arm. Derive the forward kinematics using homogeneous matrices.\nImplement a camera transform: place a cube at \\((0,0,5)\\), move the camera to \\((0,0,0)\\), and compute its 2D screen projection.\nShow that composing a rotation and translation directly is equivalent to embedding them into a homogeneous matrix and multiplying.\n\nHomogeneous coordinates are the hidden secret that lets graphics and robots share the same mathematical DNA. They unify how we move pixels, machines, and virtual worlds.\n\n\n\n93. Graphs, Adjacency, and Laplacians (Networks via Matrices)\nLinear algebra provides a powerful language for studying graphs-networks of nodes connected by edges. From social networks to electrical circuits, from the internet’s structure to biological pathways, graphs appear everywhere. Matrices give graphs a numerical form, making it possible to analyze their structure using algebraic techniques.\n\nGraph Basics Recap\n\nA graph \\(G = (V, E)\\) has a set of vertices \\(V\\) (nodes) and edges \\(E\\) (connections).\nGraphs may be undirected or directed, weighted or unweighted.\nMany graph properties-connectivity, flow, clusters-can be studied through matrices.\n\n\n\nThe Adjacency Matrix\nFor a graph with \\(n\\) vertices, the adjacency matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) encodes connections:\n\\[\nA_{ij} = \\begin{cases}\nw_{ij}, & \\text{if there is an edge from node \\(i\\) to node \\(j\\)} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\nUnweighted graphs: entries are 0 or 1.\nWeighted graphs: entries are edge weights (distances, costs, capacities).\nUndirected graphs: \\(A\\) is symmetric.\nDirected graphs: \\(A\\) may be asymmetric.\n\nThe adjacency matrix is the algebraic fingerprint of the graph.\n\n\nPowers of the Adjacency Matrix\nThe entry \\((A^k)_{ij}\\) counts the number of walks of length \\(k\\) from node \\(i\\) to node \\(j\\).\n\n\\(A^2\\) tells how many two-step connections exist.\nThis property is used in algorithms for detecting paths, clustering, and network flow.\n\n\n\nThe Degree Matrix\nThe degree of a vertex is the number of edges connected to it (or the sum of weights in weighted graphs).\nThe degree matrix \\(D\\) is diagonal:\n\\[\nD_{ii} = \\sum_j A_{ij}.\n\\]\nThis matrix measures how “connected” each node is.\n\n\nThe Graph Laplacian\nThe combinatorial Laplacian is defined as:\n\\[\nL = D - A.\n\\]\nKey properties:\n\n\\(L\\) is symmetric (for undirected graphs).\nEach row sums to zero.\nThe smallest eigenvalue is always 0, with eigenvector \\([1, 1, \\dots, 1]^T\\).\n\nThe Laplacian encodes connectivity: if the graph splits into \\(k\\) connected components, then \\(L\\) has exactly \\(k\\) zero eigenvalues.\n\n\nNormalized Laplacians\nTwo common normalized versions are:\n\\[\nL_{sym} = D^{-1/2} L D^{-1/2}, \\quad L_{rw} = D^{-1} L.\n\\]\nThese rescale the Laplacian for applications like spectral clustering.\n\n\nSpectral Graph Theory\nEigenvalues and eigenvectors of \\(A\\) or \\(L\\) reveal structure:\n\nAlgebraic connectivity: The second-smallest eigenvalue of \\(L\\) measures how well connected the graph is.\nSpectral clustering: Eigenvectors of \\(L\\) partition graphs into communities.\nRandom walks: Transition probabilities relate to \\(D^{-1}A\\).\n\n\n\nExample: A Simple Graph\nTake a triangle graph with 3 nodes, each connected to the other two.\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{bmatrix}, \\quad\nD = \\begin{bmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{bmatrix}, \\quad\nL = \\begin{bmatrix}\n2 & -1 & -1 \\\\\n-1 & 2 & -1 \\\\\n-1 & -1 & 2\n\\end{bmatrix}.\n\\]\n\nEigenvalues of \\(L\\): \\(0, 3, 3\\).\nThe single zero eigenvalue confirms the graph is connected.\n\n\n\nApplications\n\nCommunity Detection: Spectral clustering finds natural divisions in social or biological networks.\nGraph Drawing: Eigenvectors of \\(L\\) provide coordinates for visually embedding graphs.\nRandom Walks & PageRank: Transition matrices from adjacency define importance scores.\nPhysics: Laplacians appear in discrete versions of diffusion and vibration problems.\nMachine Learning: Graph neural networks (GNNs) use Laplacians to propagate signals across graph structure.\n\n\n\nWhy It Matters\nGraphs and matrices are two sides of the same coin: one combinatorial, one algebraic. By turning a network into a matrix, linear algebra gives us access to the full toolbox of eigenvalues, norms, and factorizations, enabling deep insights into connectivity, flow, and structure.\n\n\nTry It Yourself\n\nCompute adjacency, degree, and Laplacian matrices for a square graph (4 nodes in a cycle). Find eigenvalues of \\(L\\).\nProve that the Laplacian always has at least one zero eigenvalue.\nShow that if a graph has \\(k\\) components, then the multiplicity of zero as an eigenvalue is exactly \\(k\\).\nFor a random walk on a graph, derive the transition matrix \\(P = D^{-1}A\\). Interpret its eigenvectors.\n\nGraphs demonstrate how linear algebra stretches beyond geometry and data tables-it becomes a universal language for networks, from molecules to megacities.\n\n\n\n94. Data Preprocessing as Linear Operations (Centering, Whitening, Scaling)\nBefore any sophisticated model can be trained, raw data must be preprocessed. Surprisingly, many of the most common preprocessing steps-centering, scaling, whitening-are nothing more than linear algebra operations in disguise. Understanding them this way not only clarifies why they work, but also shows how they connect to broader concepts like covariance, eigenvalues, and singular value decomposition.\n\nThe Nature of Preprocessing\nMost datasets are stored as a matrix: rows correspond to samples (observations) and columns correspond to features (variables). For instance, in a dataset of 1,000 people with height, weight, and age recorded, we’d have a \\(1000 \\times 3\\) matrix. Linear algebra allows us to systematically reshape, scale, and rotate this matrix to prepare it for downstream analysis.\n\n\nCentering: Shifting the Origin\nCentering means subtracting the mean of each column (feature) from all entries in that column.\n\\[\nX_{centered} = X - \\mathbf{1}\\mu^T\n\\]\n\nHere \\(X\\) is the data matrix, \\(\\mu\\) is the vector of column means, and \\(\\mathbf{1}\\) is a column of ones.\nEffect: moves the dataset so that each feature has mean zero.\nGeometric view: translates the cloud of points so its “center of mass” sits at the origin.\nWhy important: covariance and correlation formulas assume data are mean-centered; otherwise, cross-terms are skewed.\n\nExample: If people’s heights average 170 cm, subtract 170 from every height. After centering, “height = 0” corresponds to the average person.\n\n\nScaling: Normalizing Variability\nRaw features can have different units or magnitudes (e.g., weight in kg, income in thousands of dollars). To compare them fairly, we scale:\n\\[\nX_{scaled} = X D^{-1}\n\\]\nwhere \\(D\\) is a diagonal matrix of feature standard deviations.\n\nEach feature now has variance 1.\nGeometric view: rescales axes so all dimensions have equal “spread.”\nCommon in machine learning: ensures gradient descent does not disproportionately focus on features with large raw values.\n\nExample: If weight varies around 60 kg ± 15, dividing by 15 makes its spread comparable to that of height (±10 cm).\n\n\nWhitening: Removing Correlations\nEven after centering and scaling, features can remain correlated (e.g., height and weight). Whitening transforms the data so features become uncorrelated with unit variance.\n\nLet \\(\\Sigma = \\frac{1}{n} X^T X\\) be the covariance matrix of centered data.\nPerform eigendecomposition: \\(\\Sigma = Q \\Lambda Q^T\\).\nWhitening transform:\n\n\\[\nX_{white} = X Q \\Lambda^{-1/2} Q^T\n\\]\nResult:\n\nThe covariance matrix of \\(X_{white}\\) is the identity matrix.\nEach new feature is a rotated combination of old features, with no redundancy.\n\nGeometric view: whitening “spheres” the data cloud, turning an ellipse into a perfect circle.\n\n\nCovariance Matrix as the Key Player\nThe covariance matrix itself arises naturally from preprocessing:\n\\[\n\\Sigma = \\frac{1}{n} X^T X \\quad \\text{(if \\(X\\) is centered).}\n\\]\n\nDiagonal entries: variances of features.\nOff-diagonal entries: covariances, measuring linear relationships.\nPreprocessing operations (centering, scaling, whitening) are designed to reshape data so \\(\\Sigma\\) becomes easier to interpret and more stable for learning algorithms.\n\n\n\nConnections to PCA\n\nCentering is required before PCA, otherwise the first component just points to the mean.\nScaling ensures PCA does not overweight large-variance features.\nWhitening is closely related to PCA itself: PCA diagonalizes the covariance, and whitening goes one step further by rescaling eigenvalues to unity.\n\nThus, PCA can be seen as a preprocessing pipeline plus an analysis step.\n\n\nPractical Workflows\n\nCentering and Scaling (Standardization): The default for many algorithms like logistic regression or SVM.\nWhitening: Often used in signal processing (e.g., removing correlations in audio or images).\nBatch Normalization in Deep Learning: A variant of centering + scaling applied layer by layer during training.\nWhitening in Image Processing: Ensures features like pixel intensities are decorrelated, improving compression and recognition.\n\n\n\nWorked Example\nSuppose we have three features: height, weight, and age.\n\nRaw data:\n\nMean height = 170 cm, mean weight = 65 kg, mean age = 35 years.\nVariance differs widely: age varies less, weight more.\n\nAfter centering:\n\nMean of each feature is zero.\nA person of average height now has value 0 in that feature.\n\nAfter scaling:\n\nAll features have unit variance.\nAlgorithms can treat age and weight equally.\n\nAfter whitening:\n\nCorrelation between height and weight disappears.\nFeatures become orthogonal directions in feature space.\n\n\n\n\nWhy It Matters\nWithout preprocessing, models may be misled by scale, units, or correlations. Preprocessing makes features comparable, balanced, and independent-a crucial condition for algorithms that rely on geometry (distances, angles, inner products).\nIn essence, preprocessing is the bridge from messy, real-world data to the clean structures linear algebra expects.\n\n\nTry It Yourself\n\nFor a small dataset, compute the covariance matrix before and after centering. What changes?\nScale the dataset so each feature has unit variance. Check the new covariance.\nPerform whitening via eigendecomposition and verify the covariance matrix becomes the identity.\nPlot the data points in 2D before and after whitening. Notice how an ellipse becomes a circle.\n\nPreprocessing through linear algebra shows that preparing data is not just housekeeping-it’s a fundamental reshaping of the problem’s geometry.\n\n\n\n95. Linear Regression and Classification (From Model to Matrix)\nLinear algebra provides the foundation for two of the most widely used tools in data science and applied statistics: linear regression (predicting continuous outcomes) and linear classification (separating categories). Both problems reduce to expressing data in matrix form and then applying linear operations to estimate parameters.\n\nThe Regression Setup\nSuppose we want to predict an output \\(y \\in \\mathbb{R}^n\\) from features collected in a data matrix \\(X \\in \\mathbb{R}^{n \\times p}\\), where:\n\n\\(n\\) = number of observations (samples).\n\\(p\\) = number of features (variables).\n\nWe assume a linear model:\n\\[\ny \\approx X\\beta,\n\\]\nwhere \\(\\beta \\in \\mathbb{R}^p\\) is the vector of coefficients (weights). Each entry of \\(\\beta\\) tells us how much its feature contributes to the prediction.\n\n\nThe Normal Equations\nWe want to minimize the squared error:\n\\[\n\\min_\\beta \\|y - X\\beta\\|^2.\n\\]\nDifferentiating leads to the normal equations:\n\\[\nX^T X \\beta = X^T y.\n\\]\n\nIf \\(X^T X\\) is invertible:\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T y.\n\\]\n\nIf not invertible (multicollinearity, too many features), we use the pseudoinverse via SVD:\n\n\\[\n\\hat{\\beta} = X^+ y.\n\\]\n\n\nGeometric Interpretation\n\n\\(X\\beta\\) is the projection of \\(y\\) onto the column space of \\(X\\).\nThe residual \\(r = y - X\\hat{\\beta}\\) is orthogonal to all columns of \\(X\\).\nThis “closest fit” property is why regression is a projection problem.\n\n\n\nClassification with Linear Models\nInstead of predicting continuous outputs, sometimes we want to separate categories (e.g., spam vs. not spam).\n\nLinear classifier: decides based on the sign of a linear function.\n\n\\[\n\\hat{y} = \\text{sign}(w^T x + b).\n\\]\n\nGeometric view: \\(w\\) defines a hyperplane in feature space. Points on one side are labeled positive, on the other side negative.\nRelation to regression: logistic regression replaces squared error with a log-likelihood loss, but still solves for weights via iterative linear-algebraic methods.\n\n\n\nMulticlass Extension\n\nFor \\(k\\) classes, we use a weight matrix \\(W \\in \\mathbb{R}^{p \\times k}\\).\nPrediction:\n\n\\[\n\\hat{y} = \\arg \\max_j (XW)_{ij}.\n\\]\n\nEach class has a column of \\(W\\), and the classifier picks the column with the largest score.\n\n\n\nExample: Predicting House Prices\n\nFeatures: size, number of rooms, distance to city center.\nTarget: price.\n\\(X\\) = matrix of features, \\(y\\) = price vector.\nRegression solves for coefficients showing how strongly each factor influences price.\n\nIf we switch to classification (predicting “expensive” vs. “cheap”), we treat price as a label and solve for a hyperplane separating the two categories.\n\n\nComputational Aspects\n\nDirectly solving normal equations: \\(O(p^3)\\) (matrix inversion).\nQR factorization: numerically more stable.\nSVD: best when \\(X\\) is ill-conditioned or rank-deficient.\nModern libraries: exploit sparsity or use gradient-based methods for large datasets.\n\n\n\nConnections to Other Topics\n\nLeast Squares (Chapter 8): Regression is the canonical least-squares problem.\nSVD (Chapter 9): Pseudoinverse gives regression when columns are dependent.\nRegularization (Chapter 9): Ridge regression adds a penalty \\(\\lambda \\|\\beta\\|^2\\) to improve stability.\nClassification (Chapter 10): Forms the foundation of more complex models like support vector machines and neural networks.\n\n\n\nWhy It Matters\nLinear regression and classification show the direct link between linear algebra and real-world decisions. They combine geometry (projection, hyperplanes), algebra (solving systems), and computation (factorizations). Despite their simplicity, they remain indispensable: they are interpretable, fast, and often competitive with more complex models.\n\n\nTry It Yourself\n\nGiven three features and five samples, construct \\(X\\) and \\(y\\). Solve for \\(\\beta\\) using the normal equations.\nShow that residuals are orthogonal to all columns of \\(X\\).\nWrite down a linear classifier separating two clusters of points in 2D. Sketch the separating hyperplane.\nExplore what happens when two features are highly correlated (collinear). Use pseudoinverse to recover a stable solution.\n\nLinear regression and classification are proof that linear algebra is not just abstract-it is the engine of practical prediction.\n\n\n\n96. PCA in Practice (Dimensionality Reduction Workflow)\nPrincipal Component Analysis (PCA) is one of the most widely used tools in applied linear algebra. At its heart, PCA identifies the directions (principal components) along which data varies the most, and then re-expresses the data in terms of those directions. In practice, PCA is not just a mathematical curiosity-it is a complete workflow for reducing dimensionality, denoising data, and extracting patterns from high-dimensional datasets.\n\nThe Motivation\nModern datasets often have thousands or even millions of features:\n\nImages: each pixel is a feature.\nGenomics: each gene expression level is a feature.\nText: each word in a vocabulary becomes a dimension.\n\nWorking in such high dimensions is expensive (computationally) and fragile (noise accumulates). PCA provides a systematic way to reduce the feature space to a smaller set of dimensions that still captures most of the variability.\n\n\nStep 1: Organizing the Data\nWe start with a data matrix \\(X \\in \\mathbb{R}^{n \\times p}\\):\n\n\\(n\\): number of samples (observations).\n\\(p\\): number of features (variables).\n\nEach row is a sample; each column is a feature.\nCentering is the first preprocessing step: subtract the mean of each column so the dataset has mean zero. This ensures that PCA describes variance rather than being biased by offsets.\n\\[\nX_{centered} = X - \\mathbf{1}\\mu^T\n\\]\n\n\nStep 2: Covariance Matrix\nNext, compute the covariance matrix:\n\\[\n\\Sigma = \\frac{1}{n} X_{centered}^T X_{centered}.\n\\]\n\nDiagonal entries: variance of each feature.\nOff-diagonal entries: how features co-vary.\n\nThe structure of \\(\\Sigma\\) determines the directions of maximal variation in the data.\n\n\nStep 3: Eigen-Decomposition or SVD\nTwo equivalent approaches:\n\nEigen-decomposition: Solve \\(\\Sigma v = \\lambda v\\).\n\nEigenvectors \\(v\\) are the principal components.\nEigenvalues \\(\\lambda\\) measure variance along those directions.\n\nSingular Value Decomposition (SVD): Directly decompose the centered data matrix:\n\\[\nX_{centered} = U \\Sigma V^T.\n\\]\n\nColumns of \\(V\\) = principal directions.\nSquared singular values correspond to variances.\n\n\nSVD is preferred in practice for numerical stability and efficiency, especially when \\(p\\) is very large.\n\n\nStep 4: Choosing the Number of Components\nWe order eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\\).\n\nExplained variance ratio:\n\\[\n\\text{EVR}(k) = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}.\n\\]\nWe choose \\(k\\) such that EVR exceeds some threshold (e.g., 90–95%).\nThis balances dimensionality reduction with information preservation.\n\nGraphically, a scree plot shows eigenvalues, and we look for the “elbow” point where additional components add little variance.\n\n\nStep 5: Projecting Data\nOnce we select \\(k\\) components, we project onto them:\n\\[\nX_{PCA} = X_{centered} V_k,\n\\]\nwhere \\(V_k\\) contains the top \\(k\\) eigenvectors.\nResult:\n\n\\(X_{PCA} \\in \\mathbb{R}^{n \\times k}\\).\nEach row is now a \\(k\\)-dimensional representation of the original sample.\n\n\n\nWorked Example: Face Images\nSuppose we have a dataset of grayscale images, each \\(100 \\times 100\\) pixels (\\(p = 10,000\\)).\n\nCenter each pixel value.\nCompute covariance across all images.\nFind eigenvectors = eigenfaces. These are characteristic patterns like “glasses,” “mouth shape,” or “lighting direction.”\nKeep top 50 components. Each face is now represented as a 50-dimensional vector instead of 10,000.\n\nThis drastically reduces storage and speeds up recognition while keeping key features.\n\n\nPractical Considerations\n\nStandardization: If features have different scales (e.g., age in years vs. income in thousands), we must scale them before PCA.\nComputational shortcuts: For very large \\(p\\), it’s often faster to compute PCA via truncated SVD on \\(X\\) directly.\nNoise filtering: Small eigenvalues often correspond to noise; truncating them denoises the dataset.\nInterpretability: Principal components are linear combinations of features. Sometimes these combinations are interpretable, sometimes not.\n\n\n\nConnections to Other Concepts\n\nWhitening (Chapter 94): PCA followed by scaling eigenvalues to 1 is whitening.\nSVD (Chapter 9): PCA is essentially an application of SVD.\nRegression (Chapter 95): PCA can be used before regression to reduce collinearity among predictors (PCA regression).\nMachine learning pipelines: PCA is often used before clustering, classification, or neural networks.\n\n\n\nWhy It Matters\nPCA turns raw, unwieldy data into a compact form without losing essential structure. It enables visualization (2D/3D plots of high-dimensional data), faster learning, and noise reduction. Many breakthroughs-from face recognition to gene expression analysis-rely on PCA as the first preprocessing step.\n\n\nTry It Yourself\n\nTake a dataset with 3 features. Manually compute covariance, eigenvalues, and eigenvectors.\nProject the data onto the first two principal components and plot. Compare to the original 3D scatter.\nDownload an image dataset and apply PCA to compress it. Reconstruct the images with 10, 50, 100 components. Observe the trade-off between compression and fidelity.\nCompute explained variance ratios and decide how many components to keep.\n\nPCA is the bridge between raw data and meaningful representation: it reduces complexity while sharpening patterns. It shows how linear algebra can reveal hidden order in high-dimensional chaos.\n\n\n\n97. Recommender Systems and Low-Rank Models (Fill the Missing Entries)\nRecommender systems-such as those used by Netflix, Amazon, or Spotify-are built on the principle that preferences can be captured by low-dimensional structures hidden inside large, sparse data. Linear algebra gives us the machinery to expose and exploit these structures, especially through low-rank models.\n\nThe Matrix of Preferences\nWe begin with a user–item matrix \\(R \\in \\mathbb{R}^{m \\times n}\\):\n\nRows represent users.\nColumns represent items (movies, books, songs).\nEntries \\(R_{ij}\\) store the rating (say 1–5 stars) or interaction (clicks, purchases).\n\nIn practice, most entries are missing-users rate only a small subset of items. The central challenge: predict the missing entries.\n\n\nWhy Low-Rank Structure?\nDespite its size, \\(R\\) often lies close to a low-rank approximation:\n\\[\nR \\approx U V^T\n\\]\n\n\\(U \\in \\mathbb{R}^{m \\times k}\\): user factors.\n\\(V \\in \\mathbb{R}^{n \\times k}\\): item factors.\n\\(k \\ll \\min(m, n)\\).\n\nHere, each user and each item is represented in a shared latent feature space.\n\nExample: For movies, latent dimensions might capture “action vs. romance,” “old vs. new,” or “mainstream vs. indie.”\nA user’s preference vector in this space interacts with an item’s feature vector to generate a predicted rating.\n\nThis factorization explains correlations: if you liked Movie A and B, and Movie C shares similar latent features, the system predicts you’ll like C too.\n\n\nSingular Value Decomposition (SVD) Approach\nIf \\(R\\) were complete (no missing entries), we could compute the SVD:\n\\[\nR = U \\Sigma V^T.\n\\]\n\nKeep the top \\(k\\) singular values to form a rank-\\(k\\) approximation.\nThis captures the dominant patterns in user preferences.\nGeometric view: project the massive data cloud onto a smaller \\(k\\)-dimensional subspace where structure is clearer.\n\nBut real data is incomplete. That leads to matrix completion problems.\n\n\nMatrix Completion\nMatrix completion tries to infer missing entries of \\(R\\) by assuming low rank. The optimization problem is:\n\\[\n\\min_{X} \\ \\text{rank}(X) \\quad \\text{s.t. } X_{ij} = R_{ij} \\text{ for observed entries}.\n\\]\nSince minimizing rank is NP-hard, practical algorithms instead minimize the nuclear norm (sum of singular values) or use alternating minimization:\n\nInitialize \\(U, V\\) randomly.\nIteratively solve for one while fixing the other.\nConverge to a low-rank factorization that fits the observed ratings.\n\n\n\nAlternating Least Squares (ALS)\nALS is a standard approach:\n\nFix \\(V\\), solve least squares for \\(U\\).\nFix \\(U\\), solve least squares for \\(V\\).\nRepeat until convergence.\n\nEach subproblem is straightforward linear regression, solvable with normal equations or QR decomposition.\n\n\nStochastic Gradient Descent (SGD)\nAnother approach: treat each observed rating as a training sample. Update latent vectors by minimizing squared error:\n\\[\n\\ell = (R_{ij} - u_i^T v_j)^2.\n\\]\nIteratively adjust user vector \\(u_i\\) and item vector \\(v_j\\) along gradients. This scales well to huge datasets, making it common in practice.\n\n\nRegularization\nTo prevent overfitting:\n\\[\n\\ell = (R_{ij} - u_i^T v_j)^2 + \\lambda (\\|u_i\\|^2 + \\|v_j\\|^2).\n\\]\n\nRegularization shrinks factors, discouraging extreme values.\nGeometrically, it keeps latent vectors within a reasonable ball in feature space.\n\n\n\nCold Start Problem\n\nNew users: Without ratings, \\(u_i\\) is unknown. Solutions: use demographic features or ask for a few initial ratings.\nNew items: Similarly, items need side information (metadata, tags) to generate initial latent vectors.\n\nThis is where hybrid models combine matrix factorization with content-based features.\n\n\nExample: Movie Ratings\nImagine 1,000 users and 5,000 movies.\n\nThe raw \\(R\\) matrix has 5 million entries, but each user has rated only ~50 movies.\nMatrix completion with rank \\(k = 20\\) recovers a dense approximation.\nEach user is represented by 20 latent “taste” factors; each movie by 20 latent “theme” factors.\nPrediction: the dot product of user and movie vectors.\n\n\n\nBeyond Ratings: Implicit Feedback\nIn practice, systems often lack explicit ratings. Instead, they use:\n\nViews, clicks, purchases, skips.\nThese signals are indirect but abundant.\nFactorization can handle them by treating interactions as weighted observations.\n\n\n\nConnections to Other Linear Algebra Tools\n\nSVD (Chapter 9): The backbone of factorization methods.\nPseudoinverse (Chapter 9): Useful when solving small regression subproblems in ALS.\nConditioning (Chapter 9): Factorization stability depends on well-scaled latent factors.\nPCA (Chapter 96): PCA is essentially a low-rank approximation, so PCA and recommenders share the same mathematics.\n\n\n\nWhy It Matters\nRecommender systems personalize the modern internet. Every playlist suggestion, book recommendation, or ad placement is powered by linear algebra hidden in a massive sparse matrix. Low-rank modeling shows how even incomplete, noisy data can be harnessed to reveal patterns of preference and behavior.\n\n\nTry It Yourself\n\nTake a small user–item matrix with missing entries. Apply rank-2 approximation via SVD to fill in gaps.\nImplement one step of ALS: fix movie factors and update user factors with least squares.\nCompare predictions with and without regularization. Notice how regularization stabilizes results.\nExplore the cold-start problem: simulate a new user and try predicting preferences from minimal data.\n\nLow-rank models reveal a powerful truth: behind the enormous variety of human choices lies a surprisingly small set of underlying patterns-and linear algebra is the key to uncovering them.\n\n\n\n98. PageRank and Random Walks (Ranking with Eigenvectors)\nPageRank, the algorithm that once powered Google’s search engine dominance, is a striking example of how linear algebra and eigenvectors can measure importance in a network. At its core, it models the web as a graph and asks a simple question: if you randomly surf the web forever, which pages will you visit most often?\n\nThe Web as a Graph\n\nEach web page is a node.\nEach hyperlink is a directed edge.\nThe adjacency matrix \\(A\\) encodes which pages link to which:\n\n\\[\nA_{ij} = 1 \\quad \\text{if page \\(j\\) links to page \\(i\\)}.\n\\]\nWhy columns instead of rows? Because links flow from source to destination, and PageRank naturally arises when analyzing column-stochastic transition matrices.\n\n\nTransition Matrix\nTo model random surfing, we define a column-stochastic matrix \\(P\\):\n\\[\nP_{ij} = \\frac{1}{\\text{outdeg}(j)} \\quad \\text{if \\(j \\to i\\)}.\n\\]\n\nEach column sums to 1.\n\\(P_{ij}\\) is the probability of moving from page \\(j\\) to page \\(i\\).\nThis defines a Markov chain: a random process where the next state depends only on the current one.\n\nIf a user is on page \\(j\\), they pick one outgoing link uniformly at random.\n\n\nRandom Walk Interpretation\nImagine a web surfer moving page by page according to \\(P\\). After many steps, the fraction of time spent on each page converges to a steady-state distribution vector \\(\\pi\\):\n\\[\n\\pi = P \\pi.\n\\]\nThis is an eigenvector equation: \\(\\pi\\) is the stationary eigenvector of \\(P\\) with eigenvalue 1.\n\n\\(\\pi_i\\) is the long-run probability of being on page \\(i\\).\nA higher \\(\\pi_i\\) means greater importance.\n\n\n\nThe PageRank Adjustment: Teleportation\nThe pure random walk has problems:\n\nDead ends: Pages with no outgoing links trap the surfer.\nSpider traps: Groups of pages linking only to each other hoard probability mass.\n\nSolution: add a teleportation mechanism:\n\nWith probability \\(\\alpha\\) (say 0.85), follow a link.\nWith probability \\(1-\\alpha\\), jump to a random page.\n\nThis defines the PageRank matrix:\n\\[\nM = \\alpha P + (1-\\alpha)\\frac{1}{n} ee^T,\n\\]\nwhere \\(e\\) is the all-ones vector.\n\n\\(M\\) is stochastic, irreducible, and aperiodic.\nBy the Perron–Frobenius theorem, it has a unique stationary distribution \\(\\pi\\).\n\n\n\nSolving the Eigenproblem\nThe PageRank vector \\(\\pi\\) satisfies:\n\\[\nM \\pi = \\pi.\n\\]\n\nComputing \\(\\pi\\) directly via eigen-decomposition is infeasible for billions of pages.\nInstead, use power iteration: repeatedly multiply a vector by \\(M\\) until convergence.\n\nThis works because the largest eigenvalue is 1, and the method converges to its eigenvector.\n\n\nWorked Example: A Tiny Web\nSuppose 3 pages with links:\n\nPage 1 → Page 2\nPage 2 → Page 3\nPage 3 → Page 1 and Page 2\n\nAdjacency matrix (columns = source):\n\\[\nA = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n\\]\nTransition matrix:\n\\[\nP = \\begin{bmatrix}\n0 & 0 & 1/2 \\\\\n1 & 0 & 1/2 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n\\]\nWith teleportation (\\(\\alpha=0.85\\)), we form \\(M\\). Power iteration quickly converges to \\(\\pi = [0.37, 0.34, 0.29]^T\\). Page 1 is ranked highest.\n\n\nBeyond the Web\nAlthough born in search engines, PageRank’s mathematics applies broadly:\n\nSocial networks: Rank influential users by their connections.\nCitation networks: Rank scientific papers by how they are referenced.\nBiology: Identify key proteins in protein–protein interaction networks.\nRecommendation systems: Rank products or movies via link structures.\n\nIn each case, importance is defined not by how many connections a node has, but by the importance of the nodes that point to it.\n\n\nComputational Challenges\n\nScale: Billions of pages mean \\(M\\) cannot be stored fully; sparse matrix techniques are essential.\nConvergence: Power iteration may take hundreds of steps; preconditioning and parallelization speed it up.\nPersonalization: Instead of uniform teleportation, adjust probabilities to bias toward user interests.\n\n\n\nWhy It Matters\nPageRank illustrates a deep principle: importance emerges from connectivity. Linear algebra captures this by identifying the dominant eigenvector of a transition matrix. This idea-ranking nodes in a network by stationary distributions-has transformed search engines, social media, and science itself.\n\n\nTry It Yourself\n\nConstruct a 4-page web graph and compute its PageRank manually with \\(\\alpha = 0.85\\).\nImplement power iteration in Python or MATLAB for a small adjacency matrix.\nCompare PageRank to simple degree counts. Notice how PageRank rewards links from important nodes more heavily.\nModify teleportation to bias toward a subset of pages (personalized PageRank). Observe how rankings change.\n\nPageRank is not only a milestone in computer science history-it is a living example of how eigenvectors can capture global importance from local structure.\n\n\n\n99. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)\nLinear algebra in theory is exact: numbers behave like real numbers, operations are deterministic, and results are precise. In practice, computations are carried out on computers, where numbers are represented in finite precision and algorithms must balance speed, accuracy, and stability. This intersection-numerical linear algebra-is what makes linear algebra usable at modern scales.\n\nFloating-Point Representation\nReal numbers cannot be stored exactly on a digital machine. Instead, they are approximated using the IEEE 754 floating-point standard.\n\nA floating-point number is stored as:\n\\[\nx = \\pm (1.m_1 m_2 m_3 \\dots) \\times 2^e\n\\]\nwhere \\(m\\) is the mantissa and \\(e\\) is the exponent.\nSingle precision (float32): 32 bits → ~7 decimal digits of precision.\nDouble precision (float64): 64 bits → ~16 decimal digits.\nMachine epsilon (\\(\\epsilon\\)): The smallest gap between 1 and the next representable number. For double precision, \\(\\epsilon \\approx 2.22 \\times 10^{-16}\\).\n\nImplication: operations like subtraction of nearly equal numbers cause catastrophic cancellation, where significant digits vanish.\n\n\nConditioning of Problems\nA linear algebra problem may be well-posed mathematically but still numerically difficult.\n\nThe condition number of a matrix \\(A\\):\n\\[\n\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|.\n\\]\nIf \\(\\kappa(A)\\) is large, small input errors cause large output errors.\nExample: solving \\(Ax = b\\). With ill-conditioned \\(A\\), the computed solution may be unstable even with perfect algorithms.\n\nGeometric intuition: ill-conditioned matrices stretch vectors unevenly, so small perturbations in direction blow up under inversion.\n\n\nStability of Algorithms\n\nAn algorithm is numerically stable if it controls the growth of errors from finite precision.\nGaussian elimination with partial pivoting is stable; without pivoting, it may fail catastrophically.\nOrthogonal factorizations (QR, SVD) are usually more stable than elimination methods.\n\nNumerical analysis focuses on designing algorithms that guarantee accuracy within a few multiples of machine epsilon.\n\n\nDirect vs. Iterative Methods\n\nDirect methods: Solve in a finite number of steps (e.g., Gaussian elimination, LU decomposition, Cholesky for positive definite systems).\n\nReliable for small/medium problems.\nComplexity ~ \\(O(n^3)\\).\n\nIterative methods: Generate successive approximations (e.g., Jacobi, Gauss–Seidel, Conjugate Gradient).\n\nUseful for very large, sparse systems.\nComplexity per iteration ~ \\(O(n^2)\\) or less, often leveraging sparsity.\n\n\n\n\nMatrix Factorizations in Computation\nMany algorithms rely on factorizing a matrix once, then reusing it:\n\nLU decomposition: Efficient for solving multiple right-hand sides.\nQR factorization: Stable approach for least squares.\nSVD: Gold standard for ill-conditioned problems, though expensive.\n\nThese factorizations reduce repeated operations into structured, cache-friendly steps.\n\n\nSparse vs. Dense Computations\n\nDense matrices: Most entries are nonzero. Use dense linear algebra packages like BLAS and LAPACK.\nSparse matrices: Most entries are zero. Store only nonzeros, use specialized algorithms to avoid wasted computation.\n\nLarge-scale problems (e.g., finite element simulations, web graphs) are feasible only because of sparse methods.\n\n\nBLAS and LAPACK: Standard Libraries\n\nBLAS (Basic Linear Algebra Subprograms): Defines kernels for vector and matrix operations (dot products, matrix–vector, matrix–matrix multiplication). Optimized BLAS implementations exploit cache, SIMD, and multi-core parallelism.\nLAPACK (Linear Algebra PACKage): Builds on BLAS to provide algorithms for solving systems, eigenvalue problems, SVD, etc. LAPACK is the backbone of many scientific computing environments (MATLAB, NumPy, Julia).\nMKL, OpenBLAS, cuBLAS: Vendor-specific implementations optimized for Intel CPUs, open-source systems, or NVIDIA GPUs.\n\nThese libraries make the difference between code that runs in minutes and code that runs in milliseconds.\n\n\nFloating-Point Pitfalls\n\nAccumulated round-off: Summing numbers of vastly different magnitudes may discard small contributions.\nLoss of orthogonality: Repeated Gram–Schmidt orthogonalization without reorthogonalization may drift numerically.\nOverflow/underflow: Extremely large/small numbers exceed representable range.\nNaNs and Infs: Divide-by-zero or invalid operations propagate errors.\n\nMitigation: use numerically stable algorithms, scale inputs, and check condition numbers.\n\n\nParallel and GPU Computing\nModern numerical linear algebra thrives on parallelism:\n\nGPUs accelerate dense linear algebra with thousands of cores (cuBLAS, cuSOLVER).\nDistributed libraries (ScaLAPACK, PETSc, Trilinos) allow solving problems with billions of unknowns across clusters.\nMixed precision methods: compute in float32 or even float16, then refine in float64, balancing speed and accuracy.\n\n\n\nApplications in the Real World\n\nEngineering simulations: Structural mechanics, fluid dynamics rely on sparse solvers.\nMachine learning: Training deep networks depends on optimized BLAS for matrix multiplications.\nFinance: Risk models solve huge regression problems with factorized covariance matrices.\nBig data: Dimensionality reduction (PCA, SVD) requires large-scale, stable algorithms.\n\n\n\nWhy It Matters\nLinear algebra in practice is about more than theorems: it’s about turning abstract models into computations that run reliably on imperfect hardware. Numerical linear algebra provides the essential toolkit-floating-point understanding, conditioning analysis, stable algorithms, and optimized libraries-that ensures results are both fast and trustworthy.\n\n\nTry It Yourself\n\nCompute the condition number of a nearly singular matrix (e.g., \\(\\begin{bmatrix} 1 & 1 \\\\ 1 & 1.0001 \\end{bmatrix}\\)) and solve \\(Ax=b\\). Compare results in single vs. double precision.\nImplement Gaussian elimination with and without pivoting. Compare errors for ill-conditioned matrices.\nUse NumPy with OpenBLAS to time large matrix multiplications; compare against a naive Python implementation.\nExplore iterative solvers: implement Conjugate Gradient for a sparse symmetric positive definite system.\n\nNumerical linear algebra is the bridge between mathematical elegance and computational reality. It teaches us that solving equations on a computer is not just about the equations-it’s about the algorithms, representations, and hardware that bring them to life.\n\n\n\n100. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)\nYou’ve now walked through the major landmarks of linear algebra: vectors, matrices, systems, transformations, determinants, eigenvalues, orthogonality, SVD, and applications to data and networks. The journey doesn’t end here. This last section is designed as a capstone, a way to tie things together and show you how to keep practicing, exploring, and deepening your understanding. Think of it as your “next steps” map.\n\nPracticing the Basics Until They Feel Natural\nLinear algebra may seem heavy at first, but the simplest drills build lasting confidence. Try solving a few systems of equations by hand using elimination, and notice how pivoting reveals where solutions exist-or don’t. Write down a small matrix and practice multiplying it by a vector. This might feel mechanical, but it’s how your intuition sharpens: every time you push numbers through the rules, you’re learning how the algebra reshapes space.\nEven a single concept, like the dot product, can teach a lot. Take two short vectors in the plane, compute their dot product, and then compare it to the cosine of the angle between them. Seeing algebra match geometry is what makes linear algebra come alive.\n\n\nMoving Beyond Computation: Understanding Structures\nOnce you’re comfortable with the mechanics, try reflecting on the bigger structures. What does it mean for a set of vectors to be a subspace? Can you tell whether a line through the origin is one? What about a line shifted off the origin? This is where the rules and axioms you’ve seen start to guide your reasoning.\nExperiment with bases and coordinates: pick two different bases for the plane and see how a single point looks different depending on the “ruler” you’re using. Write out the change-of-basis matrix and check that it transforms coordinates the way you expect. These exercises show that linear algebra isn’t just about numbers-it’s about perspective.\n\n\nBringing Ideas Together in Larger Problems\nThe real joy comes when different ideas collide. Suppose you have noisy data, like a scatter of points that should lie along a line. Try fitting a line using least squares. What you’re really doing is projecting the data onto a subspace. Or take a small Markov chain, like a random walk around three or four nodes, and compute its long-term distribution. That steady state is an eigenvector in disguise. These integrative problems demonstrate how the topics you’ve studied connect.\nProjects make this even more vivid. For example:\n\nIn computer graphics, write simple code that rotates or reflects a shape using a matrix.\nIn networks, use the Laplacian to identify clusters in a social graph of friends.\nIn recommendation systems, factorize a small user–item table to predict missing ratings.\n\nThese aren’t abstract puzzles-they show how linear algebra works in the real world.\n\n\nLooking Ahead: Where Linear Algebra Leads You\nBy now you know that linear algebra is not an isolated subject; it’s a foundation. The next steps depend on your interests.\nIf you enjoy computation, numerical linear algebra is the natural extension. It digs into how floating-point numbers behave on real machines, how to control round-off errors, and why some algorithms are more stable than others. You’ll learn why Gaussian elimination with pivoting is safe while without pivoting it can fail, and why QR and SVD are trusted in sensitive applications.\nIf abstraction intrigues you, then abstract linear algebra opens the door. Here you’ll move beyond \\(\\mathbb{R}^n\\) into general vector spaces: polynomials as vectors, functions as vectors, dual spaces, and eventually tensor products. These ideas power much of modern mathematics and physics.\nIf data excites you, statistics and machine learning are a natural path. Covariance matrices, principal component analysis, regression, and neural networks all rest on linear algebra. Understanding them deeply requires both the computation you’ve practiced and the geometric insights you’ve built.\nAnd if your curiosity points toward the sciences, linear algebra is everywhere: in quantum mechanics, where states are vectors and operators are matrices; in engineering, where vibrations and control systems rely on eigenvalues; in computer graphics, where every rotation and projection is a linear transformation.\n\n\nWhy This Capstone Matters\nThis final step is less about new theorems and more about perspective. The problems you solve now-whether small drills or large projects-train you to see structure, not just numbers. The roadmap is open-ended, because linear algebra itself is open-ended: once you learn to see the world through its lens, you notice it everywhere, from the patterns in networks to the behavior of algorithms to the geometry of space.\n\n\nTry It Yourself\n\nTake a dataset you care about-maybe sports scores, songs you listen to, or spending records. Organize it as a matrix. Compute simple things: averages (centering), a regression line, maybe even principal components. See what structure you uncover.\nWrite a short program that solves systems of equations using elimination. Test it on well-behaved and nearly singular matrices. Notice how stability changes.\nDraw a 2D scatterplot and fit a line with least squares. Plot the residuals. What does it mean geometrically that the residuals are orthogonal to the line?\nTry explaining eigenvalues to a friend without formulas-just pictures and stories. Teaching it will make it real.\n\nLinear algebra is both a tool and a way of thinking. You now have enough to stand on your own, but the road continues forward-into deeper math, into practical computation, and into the sciences that rely on these ideas every day. This capstone is an invitation: keep practicing, keep exploring, and let the structures of linear algebra sharpen the way you see the world.\n\n\nClosing\nFrom lines to the stars,\neach problem bends, transforms, grows—\npaths extend ahead.\n\n\n\nFinale\nA quiet closing, where lessons settle and the music of algebra carries on beyond the final page.\n1. Quiet Reflection\nLessons intertwining,\nthe book rests, but vectors stretch—\nsilence holds their song.\n2. Infinite Journey\nOne map now complete,\nyet beyond each line and plane\nnew horizons call.\n3. Structure and Growth\nRoots beneath the ground,\nbranches weaving endless skies,\nalgebra takes flight.\n4. Light After Study\nNumbers fade to light,\npatterns linger in the mind,\npaths remain open.\n5. Eternal Motion\nStillness finds its place,\ntransformations carry on,\nmovement without end.\n6. Gratitude and Closure\nSteps of thought complete,\nspaces carved with gentle care,\nthank you, wandering mind.\n7. Future Echo\nFrom shadows to form,\neach question births new echoes—\nthe journey goes on.\n8. Horizon Beyond\nThe book closes here,\nyet the lines refuse to end,\nthey stretch toward the stars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html",
    "href": "books/en-US/lab.html",
    "title": "The LAB",
    "section": "",
    "text": "Chapter 1. Vectors, scalars, and geometry",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-1.-vectors-scalars-and-geometry",
    "href": "books/en-US/lab.html#chapter-1.-vectors-scalars-and-geometry",
    "title": "The LAB",
    "section": "",
    "text": "1. Scalars, Vectors, and Coordinate Systems\nLet’s get our hands dirty! This lab is about playing with the building blocks of linear algebra: scalars and vectors. Think of a scalar as just a plain number, like 3 or -1.5. A vector is a small list of numbers, which you can picture as an arrow in space.\nWe’ll use Python (with NumPy) to explore them. Don’t worry if this is your first time with NumPy - we’ll go slowly.\n\nSet Up Your Lab\n\nimport numpy as np\n\nThat’s it - we’re ready! NumPy is the main tool we’ll use for linear algebra.\n\n\nStep-by-Step Code Walkthrough\nScalars are just numbers.\n\na = 5       # a scalar\nb = -2.5    # another scalar\n\nprint(a + b)   # add them\nprint(a * b)   # multiply them\n\n2.5\n-12.5\n\n\nVectors are lists of numbers.\n\nv = np.array([2, 3])      # a vector in 2D\nw = np.array([1, -1, 4])  # a vector in 3D\n\nprint(v)\nprint(w)\n\n[2 3]\n[ 1 -1  4]\n\n\nCoordinates tell us where we are. Think of [2, 3] as “go 2 steps in the x-direction, 3 steps in the y-direction.”\nWe can even draw it:\n\nimport matplotlib.pyplot as plt\n\n# plot vector v\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(0, 4)\nplt.ylim(0, 4)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis makes a little arrow from the origin (0,0) to (2,3).\n\n\nTry It Yourself\n\nChange the vector v to [4, 1]. Where does the arrow point now?\nTry making a 3D vector with 4 numbers, like [1, 2, 3, 4]. What happens?\nReplace np.array([2,3]) with np.array([0,0]). What does the arrow look like?\n\n\n\n\n2. Vector Notation, Components, and Arrows\nIn this lab, we’ll practice reading, writing, and visualizing vectors in different ways. A vector can look simple at first - just a list of numbers - but how we write it and how we interpret it really matters. This is where notation and components come into play.\nA vector has:\n\nA symbol (we might call it v, w, or even →AB in geometry).\nComponents (the individual numbers, like 2 and 3 in [2, 3]).\nAn arrow picture (a geometric way to see the vector as a directed line segment).\n\nLet’s see all three in action with Python.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nWriting vectors in Python\n\n\n# Two-dimensional vector\nv = np.array([2, 3])\n\n# Three-dimensional vector\nw = np.array([1, -1, 4])\n\nprint(\"v =\", v)\nprint(\"w =\", w)\n\nv = [2 3]\nw = [ 1 -1  4]\n\n\nHere v has components (2, 3) and w has components (1, -1, 4).\n\nAccessing components Each number in the vector is a component. We can pick them out using indexing.\n\n\nprint(\"First component of v:\", v[0])\nprint(\"Second component of v:\", v[1])\n\nFirst component of v: 2\nSecond component of v: 3\n\n\nNotice: in Python, indices start at 0, so v[0] is the first component.\n\nVisualizing vectors as arrows In 2D, it’s easy to draw a vector from the origin (0,0) to its endpoint (x,y).\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(-1, 4)\nplt.ylim(-2, 4)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis shows vector v as a red arrow from (0,0) to (2,3).\n\nDrawing multiple vectors We can plot several arrows at once to compare them.\n\n\nu = np.array([3, 1])\nz = np.array([-1, 2])\n\n# Draw v, u, z in different colors\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')\nplt.quiver(0, 0, z[0], z[1], angles='xy', scale_units='xy', scale=1, color='g', label='z')\n\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nNow you’ll see three arrows starting at the same point, each pointing in a different direction.\n\n\nTry It Yourself\n\nChange v to [5, 0]. What does the arrow look like now?\nTry a vector like [0, -3]. Which axis does it line up with?\nMake a new vector q = np.array([2, 0, 0]). What happens if you try to plot it with plt.quiver in 2D?\n\n\n\n\n3. Vector Addition and Scalar Multiplication\nIn this lab, we’ll explore the two most fundamental operations you can perform with vectors: adding them together and scaling them by a number (a scalar). These operations form the basis of everything else in linear algebra, from geometry to machine learning. Understanding how they work, both in code and visually, is key to building intuition.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nVector addition When you add two vectors, you simply add their components one by one.\n\n\nv = np.array([2, 3])\nu = np.array([1, -1])\n\nsum_vector = v + u\nprint(\"v + u =\", sum_vector)\n\nv + u = [3 2]\n\n\nHere, (2,3) + (1,-1) = (3,2).\n\nVisualizing vector addition (tip-to-tail method) Graphically, vector addition means placing the tail of one vector at the head of the other. The resulting vector goes from the start of the first to the end of the second.\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(v[0], v[1], u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u placed at end of v')\nplt.quiver(0, 0, sum_vector[0], sum_vector[1], angles='xy', scale_units='xy', scale=1, color='g', label='v + u')\n\nplt.xlim(-1, 5)\nplt.ylim(-2, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe green arrow is the result of adding v and u.\n\nScalar multiplication Multiplying a vector by a scalar stretches or shrinks it. If the scalar is negative, the vector flips direction.\n\n\nc = 2\nscaled_v = c * v\nprint(\"2 * v =\", scaled_v)\n\nd = -1\nscaled_v_neg = d * v\nprint(\"-1 * v =\", scaled_v_neg)\n\n2 * v = [4 6]\n-1 * v = [-2 -3]\n\n\nSo 2 * (2,3) = (4,6) and -1 * (2,3) = (-2,-3).\n\nVisualizing scalar multiplication\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, scaled_v[0], scaled_v[1], angles='xy', scale_units='xy', scale=1, color='b', label='2 * v')\nplt.quiver(0, 0, scaled_v_neg[0], scaled_v_neg[1], angles='xy', scale_units='xy', scale=1, color='g', label='-1 * v')\n\nplt.xlim(-5, 5)\nplt.ylim(-5, 7)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nHere, the blue arrow is twice as long as the red arrow, while the green arrow points in the opposite direction.\n\nCombining both operations We can scale vectors and then add them. This is called a linear combination (and it’s the foundation for the next section).\n\n\ncombo = 3*v + (-2)*u\nprint(\"3*v - 2*u =\", combo)\n\n3*v - 2*u = [ 4 11]\n\n\n\n\nTry It Yourself\n\nReplace c = 2 with c = 0.5. What happens to the vector?\nTry adding three vectors: v + u + np.array([-1,2]). Can you predict the result before printing?\nVisualize 3*v + 2*u using arrows. How does it compare to just v + u?\n\n\n\n\n4. Linear Combinations and Span\nNow that we know how to add vectors and scale them, we can combine these two moves to create linear combinations. A linear combination is just a recipe: multiply vectors by scalars, then add them together. The set of all possible results you can get from such recipes is called the span.\nThis idea is powerful because span tells us what directions and regions of space we can reach using given vectors.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nLinear combinations in Python\n\n\nv = np.array([2, 1])\nu = np.array([1, 3])\n\ncombo1 = 2*v + 3*u\ncombo2 = -1*v + 4*u\n\nprint(\"2*v + 3*u =\", combo1)\nprint(\"-v + 4*u =\", combo2)\n\n2*v + 3*u = [ 7 11]\n-v + 4*u = [ 2 11]\n\n\nHere, we multiplied and added vectors using scalars. Each result is a new vector.\n\nVisualizing linear combinations Let’s plot v, u, and their combinations.\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')\nplt.quiver(0, 0, combo1[0], combo1[1], angles='xy', scale_units='xy', scale=1, color='g', label='2v + 3u')\nplt.quiver(0, 0, combo2[0], combo2[1], angles='xy', scale_units='xy', scale=1, color='m', label='-v + 4u')\n\nplt.xlim(-5, 10)\nplt.ylim(-5, 10)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis shows how new arrows can be generated from scaling and adding the original ones.\n\nExploring the span The span of two 2D vectors is either:\n\n\nA line (if one is a multiple of the other).\nThe whole 2D plane (if they are independent).\n\n\n# Generate many combinations\ncoeffs = range(-5, 6)\npoints = []\nfor a in coeffs:\n    for b in coeffs:\n        point = a*v + b*u\n        points.append(point)\n\npoints = np.array(points)\n\nplt.scatter(points[:,0], points[:,1], s=10, color='gray')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe gray dots show all reachable points with combinations of v and u.\n\nSpecial case: dependent vectors\n\n\nw = np.array([4, 2])  # notice w = 2*v\ncoeffs = range(-5, 6)\npoints = []\nfor a in coeffs:\n    for b in coeffs:\n        points.append(a*v + b*w)\n\npoints = np.array(points)\n\nplt.scatter(points[:,0], points[:,1], s=10, color='gray')\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1, color='b')\n\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nHere, the span collapses to a line because w is just a scaled copy of v.\n\n\nTry It Yourself\n\nReplace u = [1,3] with u = [-1,2]. What does the span look like?\nTry three vectors in 2D (e.g., v, u, w). Do you get more than the whole plane?\nExperiment with 3D vectors. Use np.array([x,y,z]) and check whether different vectors span a plane or all of space.\n\n\n\n\n5. Length (Norm) and Distance\nIn this lab, we’ll measure how big a vector is (its length, also called its norm) and how far apart two vectors are (their distance). These ideas connect algebra to geometry: when we compute a norm, we’re measuring the size of an arrow; when we compute a distance, we’re measuring the gap between two points in space.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nVector length (norm) in 2D The length of a vector is computed using the Pythagorean theorem. For a vector (x, y), the length is sqrt(x² + y²).\n\n\nv = np.array([3, 4])\nlength = np.linalg.norm(v)\nprint(\"Length of v =\", length)\n\nLength of v = 5.0\n\n\nThis prints 5.0, because (3,4) forms a right triangle with sides 3 and 4, and sqrt(3²+4²)=5.\n\nManual calculation vs NumPy\n\n\nmanual_length = (v[0]**2 + v[1]**2)**0.5\nprint(\"Manual length =\", manual_length)\nprint(\"NumPy length =\", np.linalg.norm(v))\n\nManual length = 5.0\nNumPy length = 5.0\n\n\nBoth give the same result.\n\nVisualizing vector length\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r')\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.text(v[0]/2, v[1]/2, f\"Length={length}\", fontsize=10, color='blue')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll see the arrow (3,4) with its length labeled.\n\nDistance between two vectors The distance between v and another vector u is the length of their difference: ‖v - u‖.\n\n\nu = np.array([0, 0])   # the origin\ndist = np.linalg.norm(v - u)\nprint(\"Distance between v and u =\", dist)\n\nDistance between v and u = 5.0\n\n\nSince u is the origin, this is just the length of v.\n\nA more interesting distance\n\n\nu = np.array([1, 1])\ndist = np.linalg.norm(v - u)\nprint(\"Distance between v and u =\", dist)\n\nDistance between v and u = 3.605551275463989\n\n\nThis measures how far (3,4) is from (1,1).\n\nVisualizing distance between points\n\n\nplt.scatter([v[0], u[0]], [v[1], u[1]], color=['red','blue'])\nplt.plot([v[0], u[0]], [v[1], u[1]], 'k--')\nplt.text(v[0], v[1], 'v', fontsize=12, color='red')\nplt.text(u[0], u[1], 'u', fontsize=12, color='blue')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe dashed line shows the distance between the two points.\n\nHigher-dimensional vectors Norms and distances work the same way in any dimension:\n\n\na = np.array([1,2,3])\nb = np.array([4,0,8])\nprint(\"‖a‖ =\", np.linalg.norm(a))\nprint(\"‖b‖ =\", np.linalg.norm(b))\nprint(\"Distance between a and b =\", np.linalg.norm(a-b))\n\n‖a‖ = 3.7416573867739413\n‖b‖ = 8.94427190999916\nDistance between a and b = 6.164414002968976\n\n\nEven though we can’t draw 3D easily on paper, the formulas still apply.\n\n\nTry It Yourself\n\nCompute the length of np.array([5,12]). What do you expect?\nFind the distance between (2,3) and (7,7). Can you sketch it by hand and check?\nIn 3D, try vectors (1,1,1) and (2,2,2). Why is the distance exactly sqrt(3)?\n\n\n\n\n6. Dot Product\nThe dot product is one of the most important operations in linear algebra. It takes two vectors and gives you a single number. That number combines both the lengths of the vectors and how much they point in the same direction. In this lab, we’ll calculate dot products in several ways, see how they relate to geometry, and visualize their meaning.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nAlgebraic definition The dot product of two vectors is the sum of the products of their components:\n\n\nv = np.array([2, 3])\nu = np.array([4, -1])\n\ndot_manual = v[0]*u[0] + v[1]*u[1]\ndot_numpy = np.dot(v, u)\n\nprint(\"Manual dot product:\", dot_manual)\nprint(\"NumPy dot product:\", dot_numpy)\n\nManual dot product: 5\nNumPy dot product: 5\n\n\nHere, (2*4) + (3*-1) = 8 - 3 = 5.\n\nGeometric definition The dot product also equals the product of the lengths of the vectors times the cosine of the angle between them:\n\n\\[\nv \\cdot u = \\|v\\| \\|u\\| \\cos \\theta\n\\]\nWe can compute the angle:\n\nnorm_v = np.linalg.norm(v)\nnorm_u = np.linalg.norm(u)\n\ncos_theta = np.dot(v, u) / (norm_v * norm_u)\ntheta = np.arccos(cos_theta)\n\nprint(\"cos(theta) =\", cos_theta)\nprint(\"theta (in radians) =\", theta)\nprint(\"theta (in degrees) =\", np.degrees(theta))\n\ncos(theta) = 0.33633639699815626\ntheta (in radians) = 1.2277723863741932\ntheta (in degrees) = 70.3461759419467\n\n\nThis gives the angle between v and u.\n\nVisualizing the dot product Let’s draw the two vectors:\n\n\nplt.quiver(0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')\nplt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='b',label='u')\nplt.xlim(-1,5)\nplt.ylim(-2,4)\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe dot product is positive if the angle is less than 90°, negative if greater than 90°, and zero if the vectors are perpendicular.\n\nProjections and dot product The dot product lets us compute how much of one vector lies in the direction of another.\n\n\nproj_length = np.dot(v, u) / np.linalg.norm(u)\nprint(\"Projection length of v onto u:\", proj_length)\n\nProjection length of v onto u: 1.212678125181665\n\n\nThis is the length of the shadow of v onto u.\n\nSpecial cases\n\n\nIf vectors point in the same direction, the dot product is large and positive.\nIf vectors are perpendicular, the dot product is zero.\nIf vectors point in opposite directions, the dot product is negative.\n\n\na = np.array([1,0])\nb = np.array([0,1])\nc = np.array([-1,0])\n\nprint(\"a · b =\", np.dot(a,b))   # perpendicular\nprint(\"a · a =\", np.dot(a,a))   # length squared\nprint(\"a · c =\", np.dot(a,c))   # opposite\n\na · b = 0\na · a = 1\na · c = -1\n\n\n\n\nTry It Yourself\n\nCompute the dot product of (3,4) with (4,3). Is the result larger or smaller than the product of their lengths?\nTry (1,2,3) · (4,5,6). Does the geometric meaning still work in 3D?\nCreate two perpendicular vectors (e.g. (2,0) and (0,5)). Verify the dot product is zero.\n\n\n\n\n7. Angles Between Vectors and Cosine\nIn this lab, we’ll go deeper into the connection between vectors and geometry by calculating angles. Angles tell us how much two vectors “point in the same direction.” The bridge between algebra and geometry here is the cosine formula, which comes directly from the dot product.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nFormula for the angle The angle \\(\\theta\\) between two vectors \\(v\\) and \\(u\\) is given by:\n\n\\[\n\\cos \\theta = \\frac{v \\cdot u}{\\|v\\| \\, \\|u\\|}\n\\]\nThis means:\n\nIf \\(\\cos \\theta = 1\\), the vectors point in exactly the same direction.\nIf \\(\\cos \\theta = 0\\), they are perpendicular.\nIf \\(\\cos \\theta = -1\\), they point in opposite directions.\n\n\nComputing the angle in Python\n\n\nv = np.array([2, 3])\nu = np.array([3, -1])\n\ndot = np.dot(v, u)\nnorm_v = np.linalg.norm(v)\nnorm_u = np.linalg.norm(u)\n\ncos_theta = dot / (norm_v * norm_u)\ntheta = np.arccos(cos_theta)\n\nprint(\"cos(theta) =\", cos_theta)\nprint(\"theta in radians =\", theta)\nprint(\"theta in degrees =\", np.degrees(theta))\n\ncos(theta) = 0.2631174057921088\ntheta in radians = 1.3045442776439713\ntheta in degrees = 74.74488129694222\n\n\nThis gives both the cosine value and the actual angle.\n\nVisualizing the vectors\n\n\nplt.quiver(0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')\nplt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='b',label='u')\n\nplt.xlim(-1,4)\nplt.ylim(-2,4)\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou can see the angle between v and u as the gap between the red and blue arrows.\n\nChecking special cases\n\n\na = np.array([1,0])\nb = np.array([0,1])\nc = np.array([-1,0])\n\nprint(\"Angle between a and b =\", np.degrees(np.arccos(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))))\nprint(\"Angle between a and c =\", np.degrees(np.arccos(np.dot(a,c)/(np.linalg.norm(a)*np.linalg.norm(c)))))\n\nAngle between a and b = 90.0\nAngle between a and c = 180.0\n\n\n\nAngle between (1,0) and (0,1) is 90°.\nAngle between (1,0) and (-1,0) is 180°.\n\n\nUsing cosine as a similarity measure In data science and machine learning, people often use cosine similarity instead of raw angles. It’s just the cosine value itself:\n\n\ncosine_similarity = np.dot(v,u)/(np.linalg.norm(v)*np.linalg.norm(u))\nprint(\"Cosine similarity =\", cosine_similarity)\n\nCosine similarity = 0.2631174057921088\n\n\nValues close to 1 mean vectors are aligned, values near 0 mean unrelated, and values near -1 mean opposite.\n\n\nTry It Yourself\n\nCreate two random vectors with np.random.randn(3) and compute the angle between them.\nVerify that swapping the vectors gives the same angle (symmetry).\nFind two vectors where cosine similarity is exactly 0. Can you come up with an example in 2D?\n\n\n\n\n8. Projections and Decompositions\nIn this lab, we’ll learn how to split one vector into parts: one part that lies along another vector, and one part that is perpendicular. This process is called projection and decomposition. Projections let us measure “how much of a vector points in a given direction,” and decompositions give us a way to break vectors into useful components.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nProjection formula The projection of vector \\(v\\) onto vector \\(u\\) is:\n\n\\[\n\\text{proj}_u(v) = \\frac{v \\cdot u}{u \\cdot u} \\, u\n\\]\nThis gives the component of \\(v\\) that points in the direction of \\(u\\).\n\nComputing projection in Python\n\n\nv = np.array([3, 2])\nu = np.array([2, 0])\n\nproj_u_v = (np.dot(v, u) / np.dot(u, u)) * u\nprint(\"Projection of v onto u:\", proj_u_v)\n\nProjection of v onto u: [3. 0.]\n\n\nHere, \\(v = (3,2)\\) and \\(u = (2,0)\\). The projection of v onto u is a vector pointing along the x-axis.\n\nDecomposing into parallel and perpendicular parts\n\nWe can write:\n\\[\nv = \\text{proj}_u(v) + (v - \\text{proj}_u(v))\n\\]\nThe first part is parallel to u, the second part is perpendicular.\n\nperp = v - proj_u_v\nprint(\"Parallel part:\", proj_u_v)\nprint(\"Perpendicular part:\", perp)\n\nParallel part: [3. 0.]\nPerpendicular part: [0. 2.]\n\n\n\nVisualizing projection and decomposition\n\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')\nplt.quiver(0, 0, proj_u_v[0], proj_u_v[1], angles='xy', scale_units='xy', scale=1, color='g', label='proj_u(v)')\nplt.quiver(proj_u_v[0], proj_u_v[1], perp[0], perp[1], angles='xy', scale_units='xy', scale=1, color='m', label='perpendicular')\n\nplt.xlim(-1, 5)\nplt.ylim(-1, 4)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll see v (red), u (blue), the projection (green), and the perpendicular remainder (magenta).\n\nProjection in higher dimensions\n\nThis formula works in any dimension:\n\na = np.array([1,2,3])\nb = np.array([0,1,0])\n\nproj = (np.dot(a,b)/np.dot(b,b)) * b\nperp = a - proj\n\nprint(\"Projection of a onto b:\", proj)\nprint(\"Perpendicular component:\", perp)\n\nProjection of a onto b: [0. 2. 0.]\nPerpendicular component: [1. 0. 3.]\n\n\nEven in 3D or higher, projections are about splitting into “along” and “across.”\n\n\nTry It Yourself\n\nTry projecting (2,3) onto (0,5). Where does it land?\nTake a 3D vector like (4,2,6) and project it onto (1,0,0). What does this give you?\nChange the base vector u to something not aligned with the axes, like (1,1). Does the projection still work?\n\n\n\n\n9. Cauchy–Schwarz and Triangle Inequalities\nThis lab introduces two fundamental inequalities in linear algebra. They may look abstract at first, but they provide guarantees that always hold true for vectors. We’ll explore them with small examples in Python to see why they matter.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nCauchy–Schwarz inequality\n\nThe inequality states:\n\\[\n|v \\cdot u| \\leq \\|v\\| \\, \\|u\\|\n\\]\nIt means the dot product is never “bigger” than the product of the vector lengths. Equality happens only if the two vectors are pointing in exactly the same (or opposite) direction.\n\nv = np.array([3, 4])\nu = np.array([1, 2])\n\nlhs = abs(np.dot(v, u))\nrhs = np.linalg.norm(v) * np.linalg.norm(u)\n\nprint(\"Left-hand side (|v·u|):\", lhs)\nprint(\"Right-hand side (‖v‖‖u‖):\", rhs)\nprint(\"Inequality holds?\", lhs &lt;= rhs)\n\nLeft-hand side (|v·u|): 11\nRight-hand side (‖v‖‖u‖): 11.180339887498949\nInequality holds? True\n\n\n\nTesting Cauchy–Schwarz with different vectors\n\n\npairs = [\n    (np.array([1,0]), np.array([0,1])),  # perpendicular\n    (np.array([2,3]), np.array([4,6])),  # multiples\n    (np.array([-1,2]), np.array([3,-6])) # opposite multiples\n]\n\nfor v,u in pairs:\n    lhs = abs(np.dot(v, u))\n    rhs = np.linalg.norm(v) * np.linalg.norm(u)\n    print(f\"v={v}, u={u} -&gt; |v·u|={lhs}, ‖v‖‖u‖={rhs}, holds={lhs&lt;=rhs}\")\n\nv=[1 0], u=[0 1] -&gt; |v·u|=0, ‖v‖‖u‖=1.0, holds=True\nv=[2 3], u=[4 6] -&gt; |v·u|=26, ‖v‖‖u‖=25.999999999999996, holds=False\nv=[-1  2], u=[ 3 -6] -&gt; |v·u|=15, ‖v‖‖u‖=15.000000000000002, holds=True\n\n\n\nPerpendicular vectors give |v·u| = 0, far less than the product of norms.\nMultiples give equality (lhs = rhs).\n\n\nTriangle inequality\n\nThe triangle inequality states:\n\\[\n\\|v + u\\| \\leq \\|v\\| + \\|u\\|\n\\]\nGeometrically, the length of one side of a triangle can never be longer than the sum of the other two sides.\n\nv = np.array([3, 4])\nu = np.array([1, 2])\n\nlhs = np.linalg.norm(v + u)\nrhs = np.linalg.norm(v) + np.linalg.norm(u)\n\nprint(\"‖v+u‖ =\", lhs)\nprint(\"‖v‖ + ‖u‖ =\", rhs)\nprint(\"Inequality holds?\", lhs &lt;= rhs)\n\n‖v+u‖ = 7.211102550927978\n‖v‖ + ‖u‖ = 7.23606797749979\nInequality holds? True\n\n\n\nVisual demonstration with a triangle\n\n\nimport matplotlib.pyplot as plt\n\norigin = np.array([0,0])\npoints = np.array([origin, v, v+u, origin])\n\nplt.plot(points[:,0], points[:,1], 'ro-')  # triangle outline\nplt.text(v[0], v[1], 'v')\nplt.text(v[0]+u[0], v[1]+u[1], 'v+u')\nplt.text(u[0], u[1], 'u')\n\nplt.grid()\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nThis triangle shows why the inequality is called the “triangle” inequality.\n\nTesting triangle inequality with random vectors\n\n\nfor _ in range(5):\n    v = np.random.randn(2)\n    u = np.random.randn(2)\n    lhs = np.linalg.norm(v+u)\n    rhs = np.linalg.norm(v) + np.linalg.norm(u)\n    print(f\"‖v+u‖={lhs:.3f}, ‖v‖+‖u‖={rhs:.3f}, holds={lhs &lt;= rhs}\")\n\n‖v+u‖=2.226, ‖v‖+‖u‖=2.483, holds=True\n‖v+u‖=3.912, ‖v‖+‖u‖=4.423, holds=True\n‖v+u‖=3.225, ‖v‖+‖u‖=3.996, holds=True\n‖v+u‖=3.265, ‖v‖+‖u‖=4.307, holds=True\n‖v+u‖=3.899, ‖v‖+‖u‖=3.900, holds=True\n\n\nNo matter what vectors you try, the inequality always holds.\n\n\nThe Takeaway\n\nCauchy–Schwarz: The dot product is always bounded by the product of vector lengths.\nTriangle inequality: The length of one side of a triangle can’t exceed the sum of the other two.\nThese inequalities form the backbone of geometry, analysis, and many proofs in linear algebra.\n\n\n\n\n10. Orthonormal Sets in ℝ²/ℝ³\nIn this lab, we’ll explore orthonormal sets - collections of vectors that are both orthogonal (perpendicular) and normalized (length = 1). These sets are the “nicest” possible bases for vector spaces. In 2D and 3D, they correspond to the coordinate axes we already know, but we can also construct and test new ones.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nOrthogonal vectors Two vectors are orthogonal if their dot product is zero.\n\n\nx_axis = np.array([1, 0])\ny_axis = np.array([0, 1])\n\nprint(\"x_axis · y_axis =\", np.dot(x_axis, y_axis))  # should be 0\n\nx_axis · y_axis = 0\n\n\nSo the standard axes are orthogonal.\n\nNormalizing vectors Normalization means dividing a vector by its length to make its norm equal to 1.\n\n\nv = np.array([3, 4])\nv_normalized = v / np.linalg.norm(v)\n\nprint(\"Original v:\", v)\nprint(\"Normalized v:\", v_normalized)\nprint(\"Length of normalized v:\", np.linalg.norm(v_normalized))\n\nOriginal v: [3 4]\nNormalized v: [0.6 0.8]\nLength of normalized v: 1.0\n\n\nNow v_normalized points in the same direction as v but has unit length.\n\nBuilding an orthonormal set in 2D\n\n\nu1 = np.array([1, 0])\nu2 = np.array([0, 1])\n\nprint(\"u1 length:\", np.linalg.norm(u1))\nprint(\"u2 length:\", np.linalg.norm(u2))\nprint(\"u1 · u2 =\", np.dot(u1,u2))\n\nu1 length: 1.0\nu2 length: 1.0\nu1 · u2 = 0\n\n\nBoth have length 1, and their dot product is 0. That makes {u1, u2} an orthonormal set in 2D.\n\nVisualizing 2D orthonormal vectors\n\n\nplt.quiver(0,0,u1[0],u1[1],angles='xy',scale_units='xy',scale=1,color='r')\nplt.quiver(0,0,u2[0],u2[1],angles='xy',scale_units='xy',scale=1,color='b')\n\nplt.xlim(-1.5,1.5)\nplt.ylim(-1.5,1.5)\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll see the red and blue arrows at right angles, each of length 1.\n\nOrthonormal set in 3D In 3D, the standard basis vectors are:\n\n\ni = np.array([1,0,0])\nj = np.array([0,1,0])\nk = np.array([0,0,1])\n\nprint(\"‖i‖ =\", np.linalg.norm(i))\nprint(\"‖j‖ =\", np.linalg.norm(j))\nprint(\"‖k‖ =\", np.linalg.norm(k))\nprint(\"i·j =\", np.dot(i,j))\nprint(\"j·k =\", np.dot(j,k))\nprint(\"i·k =\", np.dot(i,k))\n\n‖i‖ = 1.0\n‖j‖ = 1.0\n‖k‖ = 1.0\ni·j = 0\nj·k = 0\ni·k = 0\n\n\nLengths are all 1, and dot products are 0. So {i, j, k} is an orthonormal set in ℝ³.\n\nTesting if a set is orthonormal We can write a helper function:\n\n\ndef is_orthonormal(vectors):\n    for i in range(len(vectors)):\n        for j in range(len(vectors)):\n            dot = np.dot(vectors[i], vectors[j])\n            if i == j:\n                if not np.isclose(dot, 1): return False\n            else:\n                if not np.isclose(dot, 0): return False\n    return True\n\nprint(is_orthonormal([i, j, k]))  # True\n\nTrue\n\n\n\nConstructing a new orthonormal pair Not all orthonormal sets look like the axes.\n\n\nu1 = np.array([1,1]) / np.sqrt(2)\nu2 = np.array([-1,1]) / np.sqrt(2)\n\nprint(\"u1·u2 =\", np.dot(u1,u2))\nprint(\"‖u1‖ =\", np.linalg.norm(u1))\nprint(\"‖u2‖ =\", np.linalg.norm(u2))\n\nu1·u2 = 0.0\n‖u1‖ = 0.9999999999999999\n‖u2‖ = 0.9999999999999999\n\n\nThis gives a rotated orthonormal basis in 2D.\n\n\nTry It Yourself\n\nNormalize (2,2,1) to make it a unit vector.\nTest whether the set {[1,0,0], [0,2,0], [0,0,3]} is orthonormal.\nConstruct two vectors in 2D that are not perpendicular. Normalize them and check if the dot product is still zero.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-2.-matrices-and-basic-operations",
    "href": "books/en-US/lab.html#chapter-2.-matrices-and-basic-operations",
    "title": "The LAB",
    "section": "Chapter 2. Matrices and basic operations",
    "text": "Chapter 2. Matrices and basic operations\n\n11. Matrices as Tables and as Machines\nMatrices can feel mysterious at first, but there are two simple ways to think about them:\n\nAs tables of numbers - just a grid you can store and manipulate.\nAs machines - something that takes a vector in and spits a new vector out.\n\nIn this lab, we’ll explore both views and see how they connect.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nA matrix as a table of numbers\n\n\nA = np.array([\n    [1, 2, 3],\n    [4, 5, 6]\n])\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Shape of A:\", A.shape)\n\nMatrix A:\n [[1 2 3]\n [4 5 6]]\nShape of A: (2, 3)\n\n\nHere, A is a 2×3 matrix (2 rows, 3 columns).\n\nRows = horizontal slices → [1,2,3] and [4,5,6]\nColumns = vertical slices → [1,4], [2,5], [3,6]\n\n\nAccessing rows and columns\n\n\nfirst_row = A[0]        # row 0\nsecond_column = A[:,1]  # column 1\n\nprint(\"First row:\", first_row)\nprint(\"Second column:\", second_column)\n\nFirst row: [1 2 3]\nSecond column: [2 5]\n\n\nRows are whole vectors too, and so are columns.\n\nA matrix as a machine\n\nA matrix can “act” on a vector. If x = [x1, x2, x3], then A·x is computed by taking linear combinations of the columns of A.\n\nx = np.array([1, 0, -1])  # a 3D vector\nresult = A.dot(x)\n\nprint(\"A·x =\", result)\n\nA·x = [-2 -2]\n\n\nInterpretation: multiply A by x = combine columns of A with weights from x.\n\\[\nA \\cdot x = 1 \\cdot \\text{(col 1)} + 0 \\cdot \\text{(col 2)} + (-1) \\cdot \\text{(col 3)}\n\\]\n\nVerifying column combination view\n\n\ncol1 = A[:,0]\ncol2 = A[:,1]\ncol3 = A[:,2]\n\nmanual = 1*col1 + 0*col2 + (-1)*col3\nprint(\"Manual combination:\", manual)\nprint(\"A·x result:\", result)\n\nManual combination: [-2 -2]\nA·x result: [-2 -2]\n\n\nThey match exactly. This shows the “machine” interpretation is just a shortcut for column combinations.\n\nGeometric intuition (2D example)\n\n\nB = np.array([\n    [2, 0],\n    [0, 1]\n])\n\nv = np.array([1,2])\nprint(\"B·v =\", B.dot(v))\n\nB·v = [2 2]\n\n\nHere, B scales the x-direction by 2 while leaving the y-direction alone. So (1,2) becomes (2,2).\n\n\nTry It Yourself\n\nCreate a 3×3 identity matrix with np.eye(3) and multiply it by different vectors. What happens?\nBuild a matrix [[0,-1],[1,0]]. Try multiplying it by (1,0) and (0,1). What transformation is this?\nCreate your own 2×2 matrix that flips vectors across the x-axis. Test it on (1,2) and (−3,4).\n\n\n\nThe Takeaway\n\nA matrix is both a grid of numbers and a machine that transforms vectors.\nMatrix–vector multiplication is the same as combining columns with given weights.\nThinking of matrices as machines helps build intuition for rotations, scalings, and other transformations later.\n\n\n\n\n12. Matrix Shapes, Indexing, and Block Views\nMatrices come in many shapes, and learning to read their structure is essential. Shape tells us how many rows and columns a matrix has. Indexing lets us grab specific entries, rows, or columns. Block views let us zoom in on submatrices, which is extremely useful for both theory and computation.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nMatrix shapes\n\nThe shape of a matrix is (rows, columns).\n\nA = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Shape of A:\", A.shape)\n\nMatrix A:\n [[1 2 3]\n [4 5 6]\n [7 8 9]]\nShape of A: (3, 3)\n\n\nHere, A is a 3×3 matrix.\n\nIndexing elements\n\nIn NumPy, rows and columns are 0-based. The first entry is A[0,0].\n\nprint(\"A[0,0] =\", A[0,0])  # top-left element\nprint(\"A[1,2] =\", A[1,2])  # second row, third column\n\nA[0,0] = 1\nA[1,2] = 6\n\n\n\nExtracting rows and columns\n\n\nrow1 = A[0]       # first row\ncol2 = A[:,1]     # second column\n\nprint(\"First row:\", row1)\nprint(\"Second column:\", col2)\n\nFirst row: [1 2 3]\nSecond column: [2 5 8]\n\n\nNotice: A[i] gives a row, A[:,j] gives a column.\n\nSlicing submatrices (block view)\n\nYou can slice multiple rows and columns to form a smaller matrix.\n\nblock = A[0:2, 1:3]  # rows 0–1, columns 1–2\nprint(\"Block submatrix:\\n\", block)\n\nBlock submatrix:\n [[2 3]\n [5 6]]\n\n\nThis block is:\n\\[\n\\begin{bmatrix}\n2 & 3 \\\\\n5 & 6\n\\end{bmatrix}\n\\]\n\nModifying parts of a matrix\n\n\nA[0,0] = 99\nprint(\"Modified A:\\n\", A)\n\nA[1,:] = [10, 11, 12]   # replace row 1\nprint(\"After replacing row 1:\\n\", A)\n\nModified A:\n [[99  2  3]\n [ 4  5  6]\n [ 7  8  9]]\nAfter replacing row 1:\n [[99  2  3]\n [10 11 12]\n [ 7  8  9]]\n\n\n\nNon-square matrices\n\nNot all matrices are square. Shapes can be rectangular, too.\n\nB = np.array([\n    [1, 2],\n    [3, 4],\n    [5, 6]\n])\n\nprint(\"Matrix B:\\n\", B)\nprint(\"Shape of B:\", B.shape)\n\nMatrix B:\n [[1 2]\n [3 4]\n [5 6]]\nShape of B: (3, 2)\n\n\nHere, B is 3×2 (3 rows, 2 columns).\n\nBlock decomposition idea\n\nWe can think of large matrices as made of smaller blocks. This is common in linear algebra proofs and algorithms.\n\nC = np.array([\n    [1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12],\n    [13,14,15,16]\n])\n\ntop_left = C[0:2, 0:2]\nbottom_right = C[2:4, 2:4]\n\nprint(\"Top-left block:\\n\", top_left)\nprint(\"Bottom-right block:\\n\", bottom_right)\n\nTop-left block:\n [[1 2]\n [5 6]]\nBottom-right block:\n [[11 12]\n [15 16]]\n\n\nThis is the start of block matrix notation.\n\n\nTry It Yourself\n\nCreate a 4×5 matrix with values 1–20 using np.arange(1,21).reshape(4,5). Find its shape.\nExtract the middle row and last column.\nCut it into four 2×2 blocks. Can you reassemble them in a different order?\n\n\n\n\n13. Matrix Addition and Scalar Multiplication\nNow that we understand matrix shapes and indexing, let’s practice two of the simplest but most important operations: adding matrices and scaling them with numbers (scalars). These operations extend the rules we already know for vectors.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nAdding two matrices You can add two matrices if (and only if) they have the same shape. Addition happens entry by entry.\n\n\nA = np.array([\n    [1, 2],\n    [3, 4]\n])\n\nB = np.array([\n    [5, 6],\n    [7, 8]\n])\n\nC = A + B\nprint(\"A + B =\\n\", C)\n\nA + B =\n [[ 6  8]\n [10 12]]\n\n\nEach element in C is the sum of corresponding elements in A and B.\n\nScalar multiplication Multiplying a matrix by a scalar multiplies every entry by that number.\n\n\nk = 3\nD = k * A\nprint(\"3 * A =\\n\", D)\n\n3 * A =\n [[ 3  6]\n [ 9 12]]\n\n\nHere, each element of A is tripled.\n\nCombining both operations We can mix addition and scaling, just like with vectors.\n\n\ncombo = 2*A - B\nprint(\"2A - B =\\n\", combo)\n\n2A - B =\n [[-3 -2]\n [-1  0]]\n\n\nThis creates new matrices as linear combinations of others.\n\nZero matrix A matrix of all zeros acts like “nothing happens” for addition.\n\n\nzero = np.zeros((2,2))\nprint(\"Zero matrix:\\n\", zero)\nprint(\"A + Zero =\\n\", A + zero)\n\nZero matrix:\n [[0. 0.]\n [0. 0.]]\nA + Zero =\n [[1. 2.]\n [3. 4.]]\n\n\n\nShape mismatch (what fails) If shapes don’t match, NumPy throws an error.\n\n\nX = np.array([\n    [1,2,3],\n    [4,5,6]\n])\n\ntry:\n    print(A + X)\nexcept ValueError as e:\n    print(\"Error:\", e)\n\nError: operands could not be broadcast together with shapes (2,2) (2,3) \n\n\nThis shows why shape consistency matters.\n\n\nTry It Yourself\n\nCreate two random 3×3 matrices with np.random.randint(0,10,(3,3)) and add them.\nMultiply a 4×4 matrix by -1. What happens to its entries?\nCompute 3A + 2B with the matrices from above. Compare with doing each step manually.\n\n\n\n\n14. Matrix–Vector Product (Linear Combinations of Columns)\nThis lab introduces the matrix–vector product, one of the most important operations in linear algebra. Multiplying a matrix by a vector doesn’t just crunch numbers - it produces a new vector by combining the matrix’s columns in a weighted way.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nA simple matrix and vector\n\n\nA = np.array([\n    [1, 2],\n    [3, 4],\n    [5, 6]\n])  # 3×2 matrix\n\nx = np.array([2, -1])  # 2D vector\n\nHere, A has 2 columns, so we can multiply it by a 2D vector x.\n\nMatrix–vector multiplication in NumPy\n\n\ny = A.dot(x)\nprint(\"A·x =\", y)\n\nA·x = [0 2 4]\n\n\nResult: a 3D vector.\n\nInterpreting the result as linear combinations\n\nMatrix A has two columns:\n\ncol1 = A[:,0]   # first column\ncol2 = A[:,1]   # second column\n\nmanual = 2*col1 + (-1)*col2\nprint(\"Manual linear combination:\", manual)\n\nManual linear combination: [0 2 4]\n\n\nThis matches A·x. In words: multiply each column by the corresponding entry of x and then add them up.\n\nAnother example (geometry)\n\n\nB = np.array([\n    [2, 0],\n    [0, 1]\n])  # stretches x-axis by 2\n\nv = np.array([1, 3])\nprint(\"B·v =\", B.dot(v))\n\nB·v = [2 3]\n\n\nHere, (1,3) becomes (2,3). The x-component was doubled, while y stayed the same.\n\nVisualization of matrix action\n\n\nimport matplotlib.pyplot as plt\n\n# draw original vector\nplt.quiver(0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='r',label='v')\n\n# draw transformed vector\nv_transformed = B.dot(v)\nplt.quiver(0,0,v_transformed[0],v_transformed[1],angles='xy',scale_units='xy',scale=1,color='b',label='B·v')\n\nplt.xlim(-1,4)\nplt.ylim(-1,4)\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nRed arrow = original vector, blue arrow = transformed vector.\n\n\nTry It Yourself\n\nMultiply\n\\[\nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ -1 & 2\\end{bmatrix},\\; x = [3,1]\n\\]\nWhat’s the result?\nReplace B with [[0,-1],[1,0]]. Multiply it by (1,0) and (0,1). What geometric transformation does this represent?\nFor a 4×4 identity matrix (np.eye(4)), try multiplying by any 4D vector. What do you observe?\n\n\n\n\n15. Matrix–Matrix Product (Composition of Linear Steps)\nMatrix–matrix multiplication is how we combine two linear transformations into one. Instead of applying one transformation, then another, we can multiply their matrices and get a single matrix that does both at once.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nMatrix–matrix multiplication in NumPy\n\n\nA = np.array([\n    [1, 2],\n    [3, 4]\n])  # 2×2\n\nB = np.array([\n    [2, 0],\n    [1, 2]\n])  # 2×2\n\nC = A.dot(B)   # or A @ B\nprint(\"A·B =\\n\", C)\n\nA·B =\n [[ 4  4]\n [10  8]]\n\n\nThe result C is another 2×2 matrix.\n\nManual computation\n\nEach entry of C is computed as a row of A dotted with a column of B:\n\nc11 = A[0,:].dot(B[:,0])\nc12 = A[0,:].dot(B[:,1])\nc21 = A[1,:].dot(B[:,0])\nc22 = A[1,:].dot(B[:,1])\n\nprint(\"Manual C =\\n\", np.array([[c11,c12],[c21,c22]]))\n\nManual C =\n [[ 4  4]\n [10  8]]\n\n\nThis should match A·B.\n\nGeometric interpretation\n\nLet’s see how two transformations combine.\n\nMatrix B scales x by 2 and stretches y by 2.\nMatrix A applies another linear transformation.\n\nTogether, C = A·B does both in one step.\n\nv = np.array([1,1])\n\nprint(\"First apply B:\", B.dot(v))\nprint(\"Then apply A:\", A.dot(B.dot(v)))\nprint(\"Directly with C:\", C.dot(v))\n\nFirst apply B: [2 3]\nThen apply A: [ 8 18]\nDirectly with C: [ 8 18]\n\n\nThe result is the same: applying B then A is equivalent to applying C.\n\nNon-square matrices\n\nMatrix multiplication also works for rectangular matrices, as long as the inner dimensions match.\n\nM = np.array([\n    [1, 0, 2],\n    [0, 1, 3]\n])  # 2×3\n\nN = np.array([\n    [1, 2],\n    [0, 1],\n    [4, 0]\n])  # 3×2\n\nP = M.dot(N)  # result is 2×2\nprint(\"M·N =\\n\", P)\n\nM·N =\n [[ 9  2]\n [12  1]]\n\n\nShape rule: (2×3)·(3×2) = (2×2).\n\nAssociativity (but not commutativity)\n\nMatrix multiplication is associative: (A·B)·C = A·(B·C). But it’s not commutative: in general, A·B ≠ B·A.\n\nA = np.array([[1,2],[3,4]])\nB = np.array([[0,1],[1,0]])\n\nprint(\"A·B =\\n\", A.dot(B))\nprint(\"B·A =\\n\", B.dot(A))\n\nA·B =\n [[2 1]\n [4 3]]\nB·A =\n [[3 4]\n [1 2]]\n\n\nThe two results are different.\n\n\nTry It Yourself\n\nMultiply\n\\[\nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix},\\;\nB = \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix}\n\\]\nWhat transformation does A·B represent?\nCreate a random 3×2 matrix and a 2×4 matrix. Multiply them. What shape is the result?\nVerify with Python that (A·B)·C = A·(B·C) for some 3×3 random matrices.\n\n\n\n\n16. Identity, Inverse, and Transpose\nIn this lab, we’ll meet three special matrix operations and objects: the identity matrix, the inverse, and the transpose. These are the building blocks of matrix algebra, each with a simple meaning but deep importance.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nIdentity matrix The identity matrix is like the number 1 for matrices: multiplying by it changes nothing.\n\n\nI = np.eye(3)  # 3×3 identity matrix\nprint(\"Identity matrix:\\n\", I)\n\nA = np.array([\n    [2, 1, 0],\n    [0, 1, 3],\n    [4, 0, 1]\n])\n\nprint(\"A·I =\\n\", A.dot(I))\nprint(\"I·A =\\n\", I.dot(A))\n\nIdentity matrix:\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nA·I =\n [[2. 1. 0.]\n [0. 1. 3.]\n [4. 0. 1.]]\nI·A =\n [[2. 1. 0.]\n [0. 1. 3.]\n [4. 0. 1.]]\n\n\nBoth equal A.\n\nTranspose The transpose flips rows and columns.\n\n\nB = np.array([\n    [1, 2, 3],\n    [4, 5, 6]\n])\n\nprint(\"B:\\n\", B)\nprint(\"B.T:\\n\", B.T)\n\nB:\n [[1 2 3]\n [4 5 6]]\nB.T:\n [[1 4]\n [2 5]\n [3 6]]\n\n\n\nOriginal: 2×3\nTranspose: 3×2\n\nGeometrically, transpose swaps the axes when vectors are viewed in row/column form.\n\nInverse The inverse matrix is like dividing by a number: multiplying a matrix by its inverse gives the identity.\n\n\nC = np.array([\n    [2, 1],\n    [5, 3]\n])\n\nC_inv = np.linalg.inv(C)\nprint(\"Inverse of C:\\n\", C_inv)\n\nprint(\"C·C_inv =\\n\", C.dot(C_inv))\nprint(\"C_inv·C =\\n\", C_inv.dot(C))\n\nInverse of C:\n [[ 3. -1.]\n [-5.  2.]]\nC·C_inv =\n [[ 1.00000000e+00  2.22044605e-16]\n [-8.88178420e-16  1.00000000e+00]]\nC_inv·C =\n [[1.00000000e+00 3.33066907e-16]\n [0.00000000e+00 1.00000000e+00]]\n\n\nBoth products are (approximately) the identity.\n\nMatrices that don’t have inverses Not every matrix is invertible. If a matrix is singular (determinant = 0), it has no inverse.\n\n\nD = np.array([\n    [1, 2],\n    [2, 4]\n])\n\ntry:\n    np.linalg.inv(D)\nexcept np.linalg.LinAlgError as e:\n    print(\"Error:\", e)\n\nError: Singular matrix\n\n\nHere, the second row is a multiple of the first, so D can’t be inverted.\n\nTranspose and inverse together For invertible matrices,\n\n\\[\n(A^T)^{-1} = (A^{-1})^T\n\\]\nWe can check this numerically:\n\nA = np.array([\n    [1, 2],\n    [3, 5]\n])\n\nlhs = np.linalg.inv(A.T)\nrhs = np.linalg.inv(A).T\n\nprint(\"Do they match?\", np.allclose(lhs, rhs))\n\nDo they match? True\n\n\n\n\nTry It Yourself\n\nCreate a 4×4 identity matrix. Multiply it by any 4×1 vector. Does it change?\nTake a random 2×2 matrix with np.random.randint. Compute its inverse and check if multiplying gives identity.\nPick a rectangular 3×2 matrix. What happens when you try np.linalg.inv? Why?\nCompute (A.T).T for some matrix A. What do you notice?\n\n\n\n\n17. Symmetric, Diagonal, Triangular, and Permutation Matrices\nIn this lab, we’ll meet four important families of special matrices. They have patterns that make them easier to understand, compute with, and use in algorithms.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nSymmetric matrices A matrix is symmetric if it equals its transpose: \\(A = A^T\\).\n\n\nA = np.array([\n    [2, 3, 4],\n    [3, 5, 6],\n    [4, 6, 8]\n])\n\nprint(\"A:\\n\", A)\nprint(\"A.T:\\n\", A.T)\nprint(\"Is symmetric?\", np.allclose(A, A.T))\n\nA:\n [[2 3 4]\n [3 5 6]\n [4 6 8]]\nA.T:\n [[2 3 4]\n [3 5 6]\n [4 6 8]]\nIs symmetric? True\n\n\nSymmetric matrices appear in physics, optimization, and statistics (e.g., covariance matrices).\n\nDiagonal matrices A diagonal matrix has nonzero entries only on the main diagonal.\n\n\nD = np.diag([1, 5, 9])\nprint(\"Diagonal matrix:\\n\", D)\n\nx = np.array([2, 3, 4])\nprint(\"D·x =\", D.dot(x))  # scales each component\n\nDiagonal matrix:\n [[1 0 0]\n [0 5 0]\n [0 0 9]]\nD·x = [ 2 15 36]\n\n\nDiagonal multiplication simply scales each coordinate separately.\n\nTriangular matrices Upper triangular: all entries below the diagonal are zero. Lower triangular: all entries above the diagonal are zero.\n\n\nU = np.array([\n    [1, 2, 3],\n    [0, 4, 5],\n    [0, 0, 6]\n])\n\nL = np.array([\n    [7, 0, 0],\n    [8, 9, 0],\n    [1, 2, 3]\n])\n\nprint(\"Upper triangular U:\\n\", U)\nprint(\"Lower triangular L:\\n\", L)\n\nUpper triangular U:\n [[1 2 3]\n [0 4 5]\n [0 0 6]]\nLower triangular L:\n [[7 0 0]\n [8 9 0]\n [1 2 3]]\n\n\nThese are important in solving linear systems (e.g., Gaussian elimination).\n\nPermutation matrices A permutation matrix rearranges the order of coordinates. Each row and each column has exactly one 1, everything else is 0.\n\n\nP = np.array([\n    [0, 1, 0],\n    [0, 0, 1],\n    [1, 0, 0]\n])\n\nprint(\"Permutation matrix P:\\n\", P)\n\nv = np.array([10, 20, 30])\nprint(\"P·v =\", P.dot(v))\n\nPermutation matrix P:\n [[0 1 0]\n [0 0 1]\n [1 0 0]]\nP·v = [20 30 10]\n\n\nHere, P cycles (10,20,30) into (20,30,10).\n\nChecking properties\n\n\ndef is_symmetric(M): return np.allclose(M, M.T)\ndef is_diagonal(M): return np.count_nonzero(M - np.diag(np.diag(M))) == 0\ndef is_upper_triangular(M): return np.allclose(M, np.triu(M))\ndef is_lower_triangular(M): return np.allclose(M, np.tril(M))\n\nprint(\"A symmetric?\", is_symmetric(A))\nprint(\"D diagonal?\", is_diagonal(D))\nprint(\"U upper triangular?\", is_upper_triangular(U))\nprint(\"L lower triangular?\", is_lower_triangular(L))\n\nA symmetric? True\nD diagonal? True\nU upper triangular? True\nL lower triangular? True\n\n\n\n\nTry It Yourself\n\nCreate a random symmetric matrix by generating any matrix M and computing (M + M.T)/2.\nBuild a 4×4 diagonal matrix with diagonal entries [2,4,6,8] and multiply it by [1,1,1,1].\nMake a permutation matrix that swaps the first and last components of a 3D vector.\nCheck whether the identity matrix is diagonal, symmetric, upper triangular, and lower triangular all at once.\n\n\n\n\n18. Trace and Basic Matrix Properties\nIn this lab, we’ll introduce the trace of a matrix and a few quick properties that often appear in proofs, algorithms, and applications. The trace is simple to compute but surprisingly powerful.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nWhat is the trace? The trace of a square matrix is the sum of its diagonal entries:\n\n\\[\n\\text{tr}(A) = \\sum_i A_{ii}\n\\]\n\nA = np.array([\n    [2, 1, 3],\n    [0, 4, 5],\n    [7, 8, 6]\n])\n\ntrace_A = np.trace(A)\nprint(\"Matrix A:\\n\", A)\nprint(\"Trace of A =\", trace_A)\n\nMatrix A:\n [[2 1 3]\n [0 4 5]\n [7 8 6]]\nTrace of A = 12\n\n\nHere, trace = \\(2 + 4 + 6 = 12\\).\n\nTrace is linear For matrices A and B:\n\n\\[\n\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B)\n\\]\n\\[\n\\text{tr}(cA) = c \\cdot \\text{tr}(A)\n\\]\n\nB = np.array([\n    [1, 0, 0],\n    [0, 2, 0],\n    [0, 0, 3]\n])\n\nprint(\"tr(A+B) =\", np.trace(A+B))\nprint(\"tr(A) + tr(B) =\", np.trace(A) + np.trace(B))\n\nprint(\"tr(3A) =\", np.trace(3*A))\nprint(\"3 * tr(A) =\", 3*np.trace(A))\n\ntr(A+B) = 18\ntr(A) + tr(B) = 18\ntr(3A) = 36\n3 * tr(A) = 36\n\n\n\nTrace of a product One important property is:\n\n\\[\n\\text{tr}(AB) = \\text{tr}(BA)\n\\]\n\nC = np.array([\n    [0,1],\n    [2,3]\n])\n\nD = np.array([\n    [4,5],\n    [6,7]\n])\n\nprint(\"tr(CD) =\", np.trace(C.dot(D)))\nprint(\"tr(DC) =\", np.trace(D.dot(C)))\n\ntr(CD) = 37\ntr(DC) = 37\n\n\nBoth are equal, even though CD and DC are different matrices.\n\nTrace and eigenvalues The trace equals the sum of eigenvalues of a matrix (counting multiplicities).\n\n\nvals, vecs = np.linalg.eig(A)\nprint(\"Eigenvalues:\", vals)\nprint(\"Sum of eigenvalues =\", np.sum(vals))\nprint(\"Trace =\", np.trace(A))\n\nEigenvalues: [12.83286783  2.13019807 -2.9630659 ]\nSum of eigenvalues = 12.000000000000004\nTrace = 12\n\n\nThe results should match (within rounding error).\n\nQuick invariants\n\n\nTrace doesn’t change under transpose: tr(A) = tr(A.T)\nTrace doesn’t change under similarity transforms: tr(P^-1 A P) = tr(A)\n\n\nprint(\"tr(A) =\", np.trace(A))\nprint(\"tr(A.T) =\", np.trace(A.T))\n\ntr(A) = 12\ntr(A.T) = 12\n\n\n\n\nTry It Yourself\n\nCreate a 2×2 rotation matrix for 90°:\n\\[\nR = \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix}\n\\]\nWhat is its trace? What does that tell you about its eigenvalues?\nMake a random 3×3 matrix and compare tr(A) with the sum of eigenvalues.\nTest tr(AB) and tr(BA) with a rectangular matrix A (e.g. 2×3) and B (3×2). Do they still match?\n\n\n\n\n19. Affine Transforms and Homogeneous Coordinates\nAffine transformations let us do more than just linear operations - they include translations (shifting points), which ordinary matrices can’t handle alone. To unify rotations, scalings, reflections, and translations, we use homogeneous coordinates.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nLinear transformations vs affine transformations\n\n\nA linear transformation can rotate, scale, or shear, but always keeps the origin fixed.\nAn affine transformation allows translation as well.\n\nFor example, shifting every point by (2,3) is affine but not linear.\n\nHomogeneous coordinates idea We add an extra coordinate (usually 1) to vectors.\n\n\nA 2D point (x,y) becomes (x,y,1).\nA 3D point (x,y,z) becomes (x,y,z,1).\n\nThis trick lets us represent translations using matrix multiplication.\n\n2D translation matrix\n\n\\[\nT = \\begin{bmatrix}\n1 & 0 & t_x \\\\\n0 & 1 & t_y \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nT = np.array([\n    [1, 0, 2],\n    [0, 1, 3],\n    [0, 0, 1]\n])\n\np = np.array([1, 1, 1])  # point at (1,1)\np_translated = T.dot(p)\n\nprint(\"Original point:\", p)\nprint(\"Translated point:\", p_translated)\n\nOriginal point: [1 1 1]\nTranslated point: [3 4 1]\n\n\nThis shifts (1,1) to (3,4).\n\nCombining rotation and translation\n\nA 90° rotation around the origin in 2D:\n\nR = np.array([\n    [0, -1, 0],\n    [1,  0, 0],\n    [0,  0, 1]\n])\n\nM = T.dot(R)  # rotate then translate\nprint(\"Combined transform:\\n\", M)\n\np = np.array([1, 0, 1])\nprint(\"Rotated + translated point:\", M.dot(p))\n\nCombined transform:\n [[ 0 -1  2]\n [ 1  0  3]\n [ 0  0  1]]\nRotated + translated point: [2 4 1]\n\n\nNow we can apply rotation and translation in one step.\n\nVisualization of translation\n\n\npoints = np.array([\n    [0,0,1],\n    [1,0,1],\n    [1,1,1],\n    [0,1,1]\n])  # a unit square\n\ntransformed = points.dot(T.T)\n\nplt.scatter(points[:,0], points[:,1], color='r', label='original')\nplt.scatter(transformed[:,0], transformed[:,1], color='b', label='translated')\n\nfor i in range(len(points)):\n    plt.arrow(points[i,0], points[i,1],\n              transformed[i,0]-points[i,0],\n              transformed[i,1]-points[i,1],\n              head_width=0.05, color='gray')\n\nplt.legend()\nplt.axis('equal')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll see the red unit square moved to a blue unit square shifted by (2,3).\n\nExtending to 3D In 3D, homogeneous coordinates use 4×4 matrices. Translations, rotations, and scalings all fit the same framework.\n\n\nT3 = np.array([\n    [1,0,0,5],\n    [0,1,0,-2],\n    [0,0,1,3],\n    [0,0,0,1]\n])\n\np3 = np.array([1,2,3,1])\nprint(\"Translated 3D point:\", T3.dot(p3))\n\nTranslated 3D point: [6 0 6 1]\n\n\nThis shifts (1,2,3) to (6,0,6).\n\n\nTry It Yourself\n\nBuild a scaling matrix in homogeneous coordinates that doubles both x and y, and apply it to (1,1).\nCreate a 2D transform that rotates by 90° and then shifts by (−2,1). Apply it to (0,2).\nIn 3D, translate (0,0,0) by (10,10,10). What homogeneous matrix did you use?\n\n\n\n\n20. Computing with Matrices (Cost Counts and Simple Speedups)\nWorking with matrices is not just about theory - in practice, we care about how much computation it takes to perform operations, and how we can make them faster. This lab introduces basic cost analysis (counting operations) and demonstrates simple NumPy optimizations.\n\nSet Up Your Lab\n\nimport numpy as np\nimport time\n\n\n\nStep-by-Step Code Walkthrough\n\nCounting operations (matrix–vector multiply)\n\nIf A is an \\(m \\times n\\) matrix and x is an \\(n\\)-dimensional vector, computing A·x takes about \\(m \\times n\\) multiplications and the same number of additions.\n\nm, n = 3, 4\nA = np.random.randint(1,10,(m,n))\nx = np.random.randint(1,10,n)\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Vector x:\", x)\nprint(\"A·x =\", A.dot(x))\n\nMatrix A:\n [[8 5 3 6]\n [2 6 9 7]\n [3 3 5 1]]\nVector x: [4 2 8 4]\nA·x = [ 90 120  62]\n\n\nHere the cost is \\(3 \\times 4 = 12\\) multiplications + 12 additions.\n\nCounting operations (matrix–matrix multiply)\n\nFor an \\(m \\times n\\) times \\(n \\times p\\) multiplication, the cost is about \\(m \\times n \\times p\\).\n\nm, n, p = 3, 4, 2\nA = np.random.randint(1,10,(m,n))\nB = np.random.randint(1,10,(n,p))\n\nC = A.dot(B)\nprint(\"A·B =\\n\", C)\n\nA·B =\n [[ 54  82]\n [ 71 111]\n [ 49  81]]\n\n\nHere the cost is \\(3 \\times 4 \\times 2 = 24\\) multiplications + 24 additions.\n\nTiming with NumPy (vectorized vs loop)\n\nNumPy is optimized in C and Fortran under the hood. Let’s compare matrix multiplication with and without vectorization.\n\nn = 50\nA = np.random.randn(n,n)\nB = np.random.randn(n,n)\n\n# Vectorized\nstart = time.time()\nC1 = A.dot(B)\nend = time.time()\nprint(\"Vectorized dot:\", round(end-start,3), \"seconds\")\n\n# Manual loops\nC2 = np.zeros((n,n))\nstart = time.time()\nfor i in range(n):\n    for j in range(n):\n        for k in range(n):\n            C2[i,j] += A[i,k]*B[k,j]\nend = time.time()\nprint(\"Triple loop:\", round(end-start,3), \"seconds\")\n\nVectorized dot: 0.0 seconds\nTriple loop: 0.062 seconds\n\n\nThe vectorized version should be thousands of times faster.\n\nBroadcasting tricks\n\nNumPy lets us avoid loops by broadcasting operations across entire rows or columns.\n\nA = np.array([\n    [1,2,3],\n    [4,5,6]\n])\n\n# Add 10 to every entry\nprint(\"A+10 =\\n\", A+10)\n\n# Multiply each row by a different scalar\nscales = np.array([1,10])[:,None]\nprint(\"Row-scaled A =\\n\", A*scales)\n\nA+10 =\n [[11 12 13]\n [14 15 16]]\nRow-scaled A =\n [[ 1  2  3]\n [40 50 60]]\n\n\n\nMemory and data types\n\nFor large computations, data type matters.\n\nA = np.random.randn(1000,1000).astype(np.float32)  # 32-bit floats\nB = np.random.randn(1000,1000).astype(np.float32)\n\nstart = time.time()\nC = A.dot(B)\nprint(\"Result shape:\", C.shape, \"dtype:\", C.dtype)\nprint(\"Time:\", round(time.time()-start,3), \"seconds\")\n\nResult shape: (1000, 1000) dtype: float32\nTime: 0.013 seconds\n\n\nUsing float32 instead of float64 halves memory use and can speed up computation (at the cost of some precision).\n\n\nTry It Yourself\n\nCompute the cost of multiplying a 200×500 matrix with a 500×1000 matrix. How many multiplications are needed?\nTime matrix multiplication for sizes 100, 500, 1000 in NumPy. How does the time scale?\nExperiment with float32 vs float64 in NumPy. How do speed and memory change?\nTry broadcasting: multiply each column of a matrix by [1,2,3,...].\n\n\n\nThe Takeaway\n\nMatrix operations have predictable computational costs: A·x ~ \\(m \\times n\\), A·B ~ \\(m \\times n \\times p\\).\nVectorized NumPy operations are vastly faster than Python loops.\nBroadcasting and choosing the right data type are simple speedups every beginner should learn.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-3.-linear-systems-and-elimination",
    "href": "books/en-US/lab.html#chapter-3.-linear-systems-and-elimination",
    "title": "The LAB",
    "section": "Chapter 3. Linear Systems and Elimination",
    "text": "Chapter 3. Linear Systems and Elimination\n\n21. From Equations to Matrices (Augmenting and Encoding)\nLinear algebra often begins with solving systems of linear equations. For example:\n\\[\n\\begin{cases}  \nx + 2y = 5 \\\\  \n3x - y = 4  \n\\end{cases}\n\\]\nInstead of juggling symbols, we can encode the entire system into a matrix. This is the key idea that lets computers handle thousands or millions of equations efficiently.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nWrite a system of equations\n\nWe’ll use this small example:\n\\[\n\\begin{cases}  \n2x + y = 8 \\\\  \n-3x + 4y = -11  \n\\end{cases}\n\\]\n\nEncode coefficients and constants\n\n\nCoefficient matrix \\(A\\): numbers multiplying variables.\nVariable vector \\(x\\): unknowns [x, y].\nConstant vector \\(b\\): right-hand side.\n\n\nA = np.array([\n    [2, 1],\n    [-3, 4]\n])\n\nb = np.array([8, -11])\n\nprint(\"Coefficient matrix A:\\n\", A)\nprint(\"Constants vector b:\", b)\n\nCoefficient matrix A:\n [[ 2  1]\n [-3  4]]\nConstants vector b: [  8 -11]\n\n\nSo the system is \\(A·x = b\\).\n\nAugmented matrix\n\nWe can bundle the system into one compact matrix:\n\\[\n[A|b] = \\begin{bmatrix}2 & 1 & | & 8 \\\\ -3 & 4 & | & -11 \\end{bmatrix}\n\\]\n\naugmented = np.column_stack((A, b))\nprint(\"Augmented matrix:\\n\", augmented)\n\nAugmented matrix:\n [[  2   1   8]\n [ -3   4 -11]]\n\n\nThis format is useful for elimination algorithms.\n\nSolving directly with NumPy\n\n\nsolution = np.linalg.solve(A, b)\nprint(\"Solution (x,y):\", solution)\n\nSolution (x,y): [3.90909091 0.18181818]\n\n\nHere NumPy solves the system using efficient algorithms.\n\nChecking the solution\n\nAlways verify:\n\ncheck = A.dot(solution)\nprint(\"A·x =\", check, \"should equal b =\", b)\n\nA·x = [  8. -11.] should equal b = [  8 -11]\n\n\n\nAnother example (3 variables)\n\n\\[\n\\begin{cases}  \nx + y + z = 6 \\\\  \n2x - y + z = 3 \\\\  \n- x + 2y - z = 2  \n\\end{cases}\n\\]\n\nA = np.array([\n    [1, 1, 1],\n    [2, -1, 1],\n    [-1, 2, -1]\n])\n\nb = np.array([6, 3, 2])\n\nprint(\"Augmented matrix:\\n\", np.column_stack((A, b)))\nprint(\"Solution:\", np.linalg.solve(A, b))\n\nAugmented matrix:\n [[ 1  1  1  6]\n [ 2 -1  1  3]\n [-1  2 -1  2]]\nSolution: [2.33333333 2.66666667 1.        ]\n\n\n\n\nTry It Yourself\n\nEncode the system:\n\\[\n\\begin{cases}  \n2x - y = 1 \\\\  \nx + 3y = 7  \n\\end{cases}\n\\]\nWrite A and b, then solve.\nFor a 3×3 system, try creating a random coefficient matrix with np.random.randint(-5,5,(3,3)) and a random b. Use np.linalg.solve.\nModify the constants b slightly and see how the solution changes. This introduces the idea of sensitivity.\n\n\n\nThe Takeaway\n\nSystems of linear equations can be neatly written as \\(A·x = b\\).\nThe augmented matrix \\([A|b]\\) is a compact way to set up elimination.\nThis matrix encoding transforms algebra problems into matrix problems - the gateway to all of linear algebra.\n\n\n\n\n22. Row Operations (Legal Moves That Keep Solutions)\nWhen solving linear systems, we don’t want to change the solutions - just simplify the system into an easier form. This is where row operations come in. They are the “legal moves” we can do on an augmented matrix \\([A|b]\\) without changing the solution set.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nThree legal row operations\nSwap two rows \\((R_i \\leftrightarrow R_j)\\)\nMultiply a row by a nonzero scalar \\((R_i \\to c·R_i)\\)\nReplace a row with itself plus a multiple of another row \\((R_i \\to R_i + c·R_j)\\)\n\nThese preserve the solution set.\n\nStart with an augmented matrix\n\nSystem:\n\\[\n\\begin{cases}  \nx + 2y = 5 \\\\  \n3x + 4y = 6  \n\\end{cases}\n\\]\n\nA = np.array([\n    [1, 2, 5],\n    [3, 4, 6]\n], dtype=float)\n\nprint(\"Initial augmented matrix:\\n\", A)\n\nInitial augmented matrix:\n [[1. 2. 5.]\n [3. 4. 6.]]\n\n\n\nRow swap\n\nSwap row 0 and row 1.\n\nA[[0,1]] = A[[1,0]]\nprint(\"After swapping rows:\\n\", A)\n\nAfter swapping rows:\n [[3. 4. 6.]\n [1. 2. 5.]]\n\n\n\nMultiply a row by a scalar\n\nMake the pivot in row 0 equal to 1.\n\nA[0] = A[0] / A[0,0]\nprint(\"After scaling first row:\\n\", A)\n\nAfter scaling first row:\n [[1.         1.33333333 2.        ]\n [1.         2.         5.        ]]\n\n\n\nAdd a multiple of another row\n\nEliminate the first column of row 1.\n\nA[1] = A[1] - 3*A[0]\nprint(\"After eliminating x from second row:\\n\", A)\n\nAfter eliminating x from second row:\n [[ 1.          1.33333333  2.        ]\n [-2.         -2.         -1.        ]]\n\n\nNow the system is simpler: second row has only y.\n\nSolving from the new system\n\n\ny = A[1,2] / A[1,1]\nx = (A[0,2] - A[0,1]*y) / A[0,0]\nprint(\"Solution: x =\", x, \", y =\", y)\n\nSolution: x = 1.3333333333333335 , y = 0.5\n\n\n\nUsing NumPy step-by-step vs solver\n\n\ncoeff = np.array([[1,2],[3,4]])\nconst = np.array([5,6])\nprint(\"np.linalg.solve result:\", np.linalg.solve(coeff,const))\n\nnp.linalg.solve result: [-4.   4.5]\n\n\nBoth methods give the same solution.\n\n\nTry It Yourself\n\nTake the system:\n\\[\n\\begin{cases}  \n2x + y = 7 \\\\  \nx - y = 1  \n\\end{cases}\n\\]\nWrite its augmented matrix, then:\n\nSwap rows.\nScale the first row.\nEliminate one variable.\n\nCreate a random 3×3 system with integers between -5 and 5. Perform at least one of each row operation manually in code.\nExperiment with multiplying a row by 0. What happens, and why is this not allowed as a legal operation?\n\n\n\nThe Takeaway\n\nThe three legal row operations are row swap, row scaling, and row replacement.\nThese steps preserve the solution set while moving toward a simpler form.\nThey are the foundation of Gaussian elimination, the standard algorithm for solving linear systems.\n\n\n\n\n23. Row-Echelon and Reduced Row-Echelon Forms (Target Shapes)\nWhen solving systems, our goal is to simplify the augmented matrix into a standard shape where the solutions are easy to read. These shapes are called row-echelon form (REF) and reduced row-echelon form (RREF).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\nWe’ll use NumPy for basic work and SymPy for exact RREF (since NumPy doesn’t have it built-in).\n\n\nStep-by-Step Code Walkthrough\n\nRow-Echelon Form (REF)\n\n\nAll nonzero rows are above any zero rows.\nEach leading entry (pivot) is to the right of the pivot in the row above.\nPivots are usually scaled to 1, but not strictly required.\n\nExample system:\n\\[\n\\begin{cases}  \nx + 2y + z = 7 \\\\  \n2x + 4y + z = 12 \\\\  \n3x + 6y + 2z = 17  \n\\end{cases}\n\\]\n\nA = np.array([\n    [1, 2, 1, 7],\n    [2, 4, 1, 12],\n    [3, 6, 2, 17]\n], dtype=float)\n\nprint(\"Augmented matrix:\\n\", A)\n\nAugmented matrix:\n [[ 1.  2.  1.  7.]\n [ 2.  4.  1. 12.]\n [ 3.  6.  2. 17.]]\n\n\nPerform elimination manually:\n\n# eliminate first column entries below pivot\nA[1] = A[1] - 2*A[0]\nA[2] = A[2] - 3*A[0]\nprint(\"After eliminating first column:\\n\", A)\n\nAfter eliminating first column:\n [[ 1.  2.  1.  7.]\n [ 0.  0. -1. -2.]\n [ 0.  0. -1. -4.]]\n\n\nNow the pivots move diagonally across the matrix - this is row-echelon form.\n\nReduced Row-Echelon Form (RREF) In RREF, we go further:\n\n\nEvery pivot = 1.\nEvery pivot is the only nonzero in its column.\n\nInstead of coding manually, we’ll let SymPy handle it:\n\nM = Matrix([\n    [1, 2, 1, 7],\n    [2, 4, 1, 12],\n    [3, 6, 2, 17]\n])\n\nM_rref = M.rref()\nprint(\"RREF form:\\n\", M_rref[0])\n\nRREF form:\n Matrix([[1, 2, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n\n\nSymPy shows the final canonical form.\n\nReading solutions from RREF\n\nIf the RREF looks like:\n\\[\n\\begin{bmatrix}  \n1 & 0 & a & b \\\\  \n0 & 1 & c & d \\\\  \n0 & 0 & 0 & 0  \n\\end{bmatrix}\n\\]\nIt means:\n\nThe first two variables are leading (pivots).\nThe third variable is free.\nSolutions can be written in terms of the free variable.\n\n\nA quick example with free variables\n\nSystem:\n\\[\nx + y + z = 3 \\\\  \n2x + y - z = 0  \n\\]\n\nM2 = Matrix([\n    [1,1,1,3],\n    [2,1,-1,0]\n])\n\nM2_rref = M2.rref()\nprint(\"RREF form:\\n\", M2_rref[0])\n\nRREF form:\n Matrix([[1, 0, -2, -3], [0, 1, 3, 6]])\n\n\nHere, one column will not have a pivot → that variable is free.\n\n\nTry It Yourself\n\nTake the system:\n\\[\n2x + 3y = 6, \\quad 4x + 6y = 12\n\\]\nWrite the augmented matrix and compute its RREF. What does it tell you about solutions?\nCreate a random 3×4 matrix in NumPy. Use SymPy’s Matrix.rref() to compute its reduced form. Identify the pivot columns.\nFor the system:\n\\[\nx + 2y + 3z = 4, \\quad 2x + 4y + 6z = 8\n\\]\nCheck if the equations are independent or multiples of each other by looking at the RREF.\n\n\n\nThe Takeaway\n\nREF organizes equations into a staircase shape.\nRREF goes further, making each pivot the only nonzero in its column.\nThese canonical forms make it easy to identify pivot variables, free variables, and the solution set structure.\n\n\n\n\n24. Pivots, Free Variables, and Leading Ones (Reading Solutions)\nOnce a matrix is in row-echelon or reduced row-echelon form, the solutions to the system become visible. The key is identifying pivots, leading ones, and free variables.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nWhat are pivots?\n\n\nA pivot is the first nonzero entry in a row (after elimination).\nIn RREF, pivots are scaled to 1 and are called leading ones.\nPivot columns correspond to basic variables.\n\n\nExample system\n\n\\[\n\\begin{cases}  \nx + y + z = 6 \\\\  \n2x + 3y + z = 10  \n\\end{cases}\n\\]\n\nM = Matrix([\n    [1,1,1,6],\n    [2,3,1,10]\n])\n\nM_rref = M.rref()\nprint(\"RREF form:\\n\", M_rref[0])\n\nRREF form:\n Matrix([[1, 0, 2, 8], [0, 1, -1, -2]])\n\n\n\nInterpreting the RREF\n\nSuppose the RREF comes out as:\n\\[\n\\begin{bmatrix}  \n1 & 0 & -2 & 4 \\\\  \n0 & 1 & 1 & 2  \n\\end{bmatrix}\n\\]\nThis means:\n\nPivot columns: 1 and 2 → variables \\(x\\) and \\(y\\) are basic.\nFree variable: \\(z\\).\nEquations:\n\\[\nx - 2z = 4, \\quad y + z = 2\n\\]\nSolution in terms of \\(z\\):\n\\[\nx = 4 + 2z, \\quad y = 2 - z, \\quad z = z\n\\]\n\n\nCoding the solution extraction\n\n\nrref_matrix, pivots = M_rref\nprint(\"Pivot columns:\", pivots)\n\n# free variables are the columns not in pivots\nall_vars = set(range(rref_matrix.shape[1]-1))  # exclude last column (constants)\nfree_vars = all_vars - set(pivots)\nprint(\"Free variable indices:\", free_vars)\n\nPivot columns: (0, 1)\nFree variable indices: {2}\n\n\n\nAnother example with infinitely many solutions\n\n\\[\nx + 2y + 3z = 4, \\quad 2x + 4y + 6z = 8\n\\]\n\nM2 = Matrix([\n    [1,2,3,4],\n    [2,4,6,8]\n])\n\nM2_rref = M2.rref()\nprint(\"RREF form:\\n\", M2_rref[0])\n\nRREF form:\n Matrix([[1, 2, 3, 4], [0, 0, 0, 0]])\n\n\nThe second row becomes all zeros, showing redundancy. Pivot in column 1, free variables in columns 2 and 3.\n\nSolving underdetermined systems\n\nIf you have more variables than equations, expect free variables. Example:\n\\[\nx + y = 3\n\\]\n\nM3 = Matrix([[1,1,3]])\nprint(\"RREF form:\\n\", M3.rref()[0])\n\nRREF form:\n Matrix([[1, 1, 3]])\n\n\nHere, \\(x = 3 - y\\). Variable \\(y\\) is free.\n\n\nTry It Yourself\n\nTake the system:\n\\[\nx + y + z = 2, \\quad y + z = 1\n\\]\nCompute its RREF and identify pivot and free variables.\nCreate a random 3×4 system and compute its pivots. How many free variables do you get?\nFor the system:\n\\[\nx - y = 0, \\quad 2x - 2y = 0\n\\]\nVerify that the system has infinitely many solutions and describe them in terms of a free variable.\n\n\n\nThe Takeaway\n\nPivots / leading ones mark the basic variables.\nFree variables correspond to non-pivot columns.\nSolutions are written in terms of free variables, showing whether the system has a unique, infinite, or no solution.\n\n\n\n\n25. Solving Consistent Systems (Unique vs. Infinite Solutions)\nNow that we can spot pivots and free variables, we can classify systems of equations as having a unique solution or infinitely many solutions (assuming they’re consistent). In this lab, we’ll practice solving both types.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nUnique solution example\n\nSystem:\n\\[\nx + y = 3, \\quad 2x - y = 0\n\\]\n\nfrom sympy import Matrix\n\nM = Matrix([\n    [1, 1, 3],\n    [2, -1, 0]\n])\n\nM_rref = M.rref()\nprint(\"RREF form:\\n\", M_rref[0])\n\n# Split into coefficient matrix A and right-hand side b\nA = M[:, :2]\nb = M[:, 2]\n\nsolution = A.solve_least_squares(b)\nprint(\"Solution:\", solution)\n\nRREF form:\n Matrix([[1, 0, 1], [0, 1, 2]])\nSolution: Matrix([[1], [2]])\n\n\n\nInfinite solution example\n\nSystem:\n\\[\nx + y + z = 2, \\quad 2x + 2y + 2z = 4\n\\]\n\nM2 = Matrix([\n    [1, 1, 1, 2],\n    [2, 2, 2, 4]\n])\n\nM2_rref = M2.rref()\nprint(\"RREF form:\\n\", M2_rref[0])\n\nRREF form:\n Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])\n\n\nOnly one pivot → two free variables.\nInterpretation:\n\n\\(x = 2 - y - z\\)\n\\(y, z\\) are free\nInfinite solutions described by parameters.\n\n\nClassifying consistency\n\nA system is consistent if the RREF does not have a row like:\n\\[\n[0, 0, 0, c] \\quad (c \\neq 0)\n\\]\nExample consistent system:\n\nM3 = Matrix([\n    [1, 2, 3],\n    [0, 1, 4]\n])\nprint(\"RREF:\\n\", M3.rref()[0])\n\nRREF:\n Matrix([[1, 0, -5], [0, 1, 4]])\n\n\nExample inconsistent system (no solution):\n\nM4 = Matrix([\n    [1, 1, 2],\n    [2, 2, 5]\n])\nprint(\"RREF:\\n\", M4.rref()[0])\n\nRREF:\n Matrix([[1, 1, 0], [0, 0, 1]])\n\n\nThe second one ends with [0,0,1], meaning contradiction (0 = 1).\n\nQuick NumPy comparison\n\nFor systems with unique solutions:\n\nA = np.array([[1,1],[2,-1]], dtype=float)\nb = np.array([3,0], dtype=float)\nprint(\"Unique solution with np.linalg.solve:\", np.linalg.solve(A,b))\n\nUnique solution with np.linalg.solve: [1. 2.]\n\n\nFor systems with infinite solutions, np.linalg.solve will fail, but SymPy handles parametric solutions.\n\n\nTry It Yourself\n\nSolve:\n\\[\nx + y + z = 1, \\quad 2x + 3y + z = 2\n\\]\nIs the solution unique or infinite?\nCheck consistency of:\n\\[\nx + 2y = 3, \\quad 2x + 4y = 8\n\\]\nBuild a random 3×4 augmented matrix and compute its RREF. Identify:\n\nDoes it have a unique solution, infinitely many, or none?\n\n\n\n\nThe Takeaway\n\nUnique solution: pivot in every variable column.\nInfinite solutions: free variables remain, system is still consistent.\nNo solution: an inconsistent row appears.\n\nUnderstanding pivots and free variables gives a complete picture of the solution set.\n\n\n\n26. Detecting Inconsistency (When No Solution Exists)\nNot all systems of linear equations can be solved. Some are inconsistent, meaning the equations contradict each other. In this lab, we’ll learn how to recognize inconsistency using augmented matrices and RREF.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nAn inconsistent system\n\n\\[\nx + y = 2, \\quad 2x + 2y = 5\n\\]\nNotice the second equation looks like a multiple of the first, but the constant doesn’t match - contradiction.\n\nM = Matrix([\n    [1, 1, 2],\n    [2, 2, 5]\n])\n\nM_rref = M.rref()\nprint(\"RREF:\\n\", M_rref[0])\n\nRREF:\n Matrix([[1, 1, 0], [0, 0, 1]])\n\n\nRREF gives:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 2 \\\\  \n0 & 0 & 1  \n\\end{bmatrix}\n\\]\nThe last row means \\(0 = 1\\), so no solution exists.\n\nA consistent system (for contrast)\n\n\\[\nx + y = 2, \\quad 2x + 2y = 4\n\\]\n\nM2 = Matrix([\n    [1, 1, 2],\n    [2, 2, 4]\n])\n\nprint(\"RREF:\\n\", M2.rref()[0])\n\nRREF:\n Matrix([[1, 1, 2], [0, 0, 0]])\n\n\nThis reduces to one equation and a redundant row of zeros → infinitely many solutions.\n\nVisualizing inconsistency (2D case)\n\nSystem:\n\\[\nx + y = 2 \\quad \\text{and} \\quad x + y = 3\n\\]\nThese are parallel lines that never meet.\n\nimport matplotlib.pyplot as plt\n\nx_vals = np.linspace(-1, 3, 100)\ny1 = 2 - x_vals\ny2 = 3 - x_vals\n\nplt.plot(x_vals, y1, label=\"x+y=2\")\nplt.plot(x_vals, y2, label=\"x+y=3\")\n\nplt.legend()\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe two lines are parallel → no solution.\n\nDetecting inconsistency automatically\n\nWe can scan the RREF for a row of the form \\([0, 0, …, c]\\) with \\(c \\neq 0\\).\n\ndef is_inconsistent(M):\n    rref_matrix, _ = M.rref()\n    for row in rref_matrix.tolist():\n        if all(v == 0 for v in row[:-1]) and row[-1] != 0:\n            return True\n    return False\n\nprint(\"System 1 inconsistent?\", is_inconsistent(M))\nprint(\"System 2 inconsistent?\", is_inconsistent(M2))\n\nSystem 1 inconsistent? True\nSystem 2 inconsistent? False\n\n\n\n\nTry It Yourself\n\nTest the system:\n\\[\nx + 2y = 4, \\quad 2x + 4y = 10\n\\]\nWrite the augmented matrix and check if it’s inconsistent.\nBuild a random 2×3 augmented matrix with integer entries. Use is_inconsistent to check.\nPlot two linear equations in 2D. Adjust constants to see when they intersect (consistent) vs when they are parallel (inconsistent).\n\n\n\nThe Takeaway\n\nA system is inconsistent if RREF contains a row like \\([0,0,…,c]\\) with \\(c \\neq 0\\).\nGeometrically, this means the equations describe parallel lines (2D), parallel planes (3D), or higher-dimensional contradictions.\nRecognizing inconsistency quickly saves time and avoids chasing impossible solutions.\n\n\n\n\n27. Gaussian Elimination by Hand (A Disciplined Procedure)\nGaussian elimination is the systematic way to solve linear systems using row operations. It transforms the augmented matrix into row-echelon form (REF) and then uses back substitution to find solutions. In this lab, we’ll walk step by step through the process.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nExample system\n\n\\[\n\\begin{cases}  \nx + y + z = 6 \\\\  \n2x + 3y + z = 14 \\\\  \nx + 2y + 3z = 14  \n\\end{cases}\n\\]\n\nA = np.array([\n    [1, 1, 1, 6],\n    [2, 3, 1, 14],\n    [1, 2, 3, 14]\n], dtype=float)\n\nprint(\"Initial augmented matrix:\\n\", A)\n\nInitial augmented matrix:\n [[ 1.  1.  1.  6.]\n [ 2.  3.  1. 14.]\n [ 1.  2.  3. 14.]]\n\n\n\nStep 1: Get a pivot in the first column\n\nMake the pivot at (0,0) into 1 (it already is). Now eliminate below it.\n\nA[1] = A[1] - 2*A[0]   # Row2 → Row2 - 2*Row1\nA[2] = A[2] - A[0]     # Row3 → Row3 - Row1\nprint(\"After eliminating first column:\\n\", A)\n\nAfter eliminating first column:\n [[ 1.  1.  1.  6.]\n [ 0.  1. -1.  2.]\n [ 0.  1.  2.  8.]]\n\n\n\nStep 2: Pivot in the second column\n\nMake the pivot in row 1, col 1 equal to 1.\n\nA[1] = A[1] / A[1,1]\nprint(\"After scaling second row:\\n\", A)\n\nAfter scaling second row:\n [[ 1.  1.  1.  6.]\n [ 0.  1. -1.  2.]\n [ 0.  1.  2.  8.]]\n\n\nNow eliminate below:\n\nA[2] = A[2] - A[2,1]*A[1]\nprint(\"After eliminating second column:\\n\", A)\n\nAfter eliminating second column:\n [[ 1.  1.  1.  6.]\n [ 0.  1. -1.  2.]\n [ 0.  0.  3.  6.]]\n\n\n\nStep 3: Pivot in the third column\n\nMake the bottom-right entry into 1.\n\nA[2] = A[2] / A[2,2]\nprint(\"After scaling third row:\\n\", A)\n\nAfter scaling third row:\n [[ 1.  1.  1.  6.]\n [ 0.  1. -1.  2.]\n [ 0.  0.  1.  2.]]\n\n\nAt this point, the matrix is in row-echelon form (REF).\n\nBack substitution\n\nNow solve from the bottom up:\n\nz = A[2,3]\ny = A[1,3] - A[1,2]*z\nx = A[0,3] - A[0,1]*y - A[0,2]*z\n\nprint(f\"Solution: x={x}, y={y}, z={z}\")\n\nSolution: x=0.0, y=4.0, z=2.0\n\n\n\nVerification\n\n\ncoeff = np.array([\n    [1,1,1],\n    [2,3,1],\n    [1,2,3]\n], dtype=float)\nconst = np.array([6,14,14], dtype=float)\n\nprint(\"Check with np.linalg.solve:\", np.linalg.solve(coeff,const))\n\nCheck with np.linalg.solve: [0. 4. 2.]\n\n\nThe results match.\n\n\nTry It Yourself\n\nSolve:\n\\[\n2x + y = 5, \\quad 4x - 6y = -2\n\\]\nusing Gaussian elimination manually in code.\nCreate a random 3×4 augmented matrix and reduce it step by step, printing after each row operation.\nCompare your manual elimination to SymPy’s RREF with Matrix.rref().\n\n\n\nThe Takeaway\n\nGaussian elimination is a disciplined sequence of row operations.\nIt reduces the matrix to row-echelon form, from which back substitution is straightforward.\nThis method is the backbone of solving systems by hand and underlies many numerical algorithms.\n\n\n\n\n28. Back Substitution and Solution Sets (Finishing Cleanly)\nOnce Gaussian elimination reduces a system to row-echelon form (REF), the final step is back substitution. This means solving variables starting from the last equation and working upward. In this lab, we’ll practice both unique and infinite solution cases.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nUnique solution example\n\nSystem:\n\\[\nx + y + z = 6, \\quad 2y + 5z = -4, \\quad z = 3\n\\]\nRow-echelon form looks like:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & 6 \\\\  \n0 & 2 & 5 & -4 \\\\  \n0 & 0 & 1 & 3  \n\\end{bmatrix}\n\\]\nSolve bottom-up:\n\nz = 3\ny = (-4 - 5*z)/2\nx = 6 - y - z\nprint(f\"Solution: x={x}, y={y}, z={z}\")\n\nSolution: x=12.5, y=-9.5, z=3\n\n\n\nInfinite solution example\n\nSystem:\n\\[\nx + y + z = 2, \\quad 2x + 2y + 2z = 4\n\\]\nAfter elimination:\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 & 2 \\\\  \n0 & 0 & 0 & 0  \n\\end{bmatrix}\n\\]\nThis means:\n\nEquation: \\(x + y + z = 2\\).\nFree variables: choose \\(y\\) and \\(z\\).\n\nLet \\(y = s, z = t\\). Then:\n\\[\nx = 2 - s - t\n\\]\nSo the solution set is:\n\nfrom sympy import symbols\ns, t = symbols('s t')\nx = 2 - s - t\ny = s\nz = t\nprint(\"General solution:\")\nprint(\"x =\", x, \", y =\", y, \", z =\", z)\n\nGeneral solution:\nx = -s - t + 2 , y = s , z = t\n\n\n\nConsistency check with RREF\n\nWe can use SymPy to confirm solution sets:\n\nM = Matrix([\n    [1,1,1,2],\n    [2,2,2,4]\n])\n\nprint(\"RREF form:\\n\", M.rref()[0])\n\nRREF form:\n Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])\n\n\nThe second row disappears, showing infinite solutions.\n\nEncoding solution sets\n\nGeneral solutions are often written in parametric vector form.\nFor the infinite solution above:\n\\[\n(x,y,z) = (2,0,0) + s(-1,1,0) + t(-1,0,1)\n\\]\nThis shows the solution space is a plane in \\(\\mathbb{R}^3\\).\n\n\nTry It Yourself\n\nSolve:\n\\[\nx + 2y = 5, \\quad y = 1\n\\]\nDo back substitution by hand and check with NumPy.\nTake the system:\n\\[\nx + y + z = 1, \\quad 2x + 2y + 2z = 2\n\\]\nWrite its solution set in parametric form.\nUse Matrix.rref() on a 3×4 random augmented matrix. Identify pivot and free variables, then describe the solution set.\n\n\n\nThe Takeaway\n\nBack substitution is the cleanup step after Gaussian elimination.\nIt reveals whether the system has a unique solution or infinitely many.\nSolutions can be expressed explicitly (unique case) or parametrically (infinite case).\n\n\n\n\n29. Rank and Its First Meaning (Pivots as Information)\nThe rank of a matrix tells us how much independent information it contains. Rank is one of the most important concepts in linear algebra because it connects to pivots, independence, dimension, and the number of solutions to a system.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nRank definition The rank is the number of pivots (leading ones) in the row-echelon form of a matrix.\n\nExample:\n\nA = Matrix([\n    [1, 2, 3],\n    [2, 4, 6],\n    [1, 1, 1]\n])\n\nprint(\"RREF:\\n\", A.rref()[0])\nprint(\"Rank of A:\", A.rank())\n\nRREF:\n Matrix([[1, 0, -1], [0, 1, 2], [0, 0, 0]])\nRank of A: 2\n\n\n\nThe second row is a multiple of the first, so the rank is less than 3.\nOnly two independent rows → rank = 2.\n\n\nRank and solutions to \\(A·x = b\\)\n\nConsider:\n\\[\n\\begin{cases}  \nx + y + z = 3 \\\\  \n2x + 2y + 2z = 6 \\\\  \nx - y = 0  \n\\end{cases}\n\\]\n\nM = Matrix([\n    [1, 1, 1, 3],\n    [2, 2, 2, 6],\n    [1, -1, 0, 0]\n])\n\nprint(\"RREF:\\n\", M.rref()[0])\nprint(\"Rank of coefficient matrix:\", M[:, :-1].rank())\nprint(\"Rank of augmented matrix:\", M.rank())\n\nRREF:\n Matrix([[1, 0, 1/2, 3/2], [0, 1, 1/2, 3/2], [0, 0, 0, 0]])\nRank of coefficient matrix: 2\nRank of augmented matrix: 2\n\n\n\nIf rank(A) = rank([A|b]) = number of variables → unique solution.\nIf rank(A) = rank([A|b]) &lt; number of variables → infinite solutions.\nIf rank(A) &lt; rank([A|b]) → no solution.\n\n\nNumPy comparison\n\n\nA = np.array([\n    [1, 2, 3],\n    [2, 4, 6],\n    [1, 1, 1]\n], dtype=float)\n\nprint(\"Rank with NumPy:\", np.linalg.matrix_rank(A))\n\nRank with NumPy: 2\n\n\n\nRank as “dimension of information”\n\nThe rank equals:\n\nThe number of independent rows.\nThe number of independent columns.\nThe dimension of the column space.\n\n\nB = Matrix([\n    [1,2],\n    [2,4],\n    [3,6]\n])\n\nprint(\"Rank of B:\", B.rank())\n\nRank of B: 1\n\n\nAll columns are multiples → only one independent direction → rank = 1.\n\n\nTry It Yourself\n\nCompute the rank of:\n\\[\n\\begin{bmatrix}  \n1 & 2 & 3 \\\\  \n2 & 4 & 6 \\\\  \n3 & 6 & 9  \n\\end{bmatrix}\n\\]\nWhat do you expect?\nCreate a random 4×4 matrix with np.random.randint. Compute its rank with both SymPy and NumPy.\nTest solution consistency using rank: build a system where rank(A) ≠ rank([A|b]) and show it has no solution.\n\n\n\nThe Takeaway\n\nRank = number of pivots = dimension of independent information.\nRank reveals whether a system has no solution, one solution, or infinitely many.\nRank connects algebra (pivots) with geometry (dimension of subspaces).\n\n\n\n\n30. LU Factorization (Elimination Captured as L and U)\nGaussian elimination can be recorded in a neat factorization:\n\\[\nA = LU\n\\]\nwhere \\(L\\) is a lower triangular matrix (recording the multipliers we used) and \\(U\\) is an upper triangular matrix (the result of elimination). This is called LU factorization. It’s a powerful tool for solving systems efficiently.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom scipy.linalg import lu\n\n\n\nStep-by-Step Code Walkthrough\n\nExample matrix\n\n\nA = np.array([\n    [2, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n], dtype=float)\n\nprint(\"Matrix A:\\n\", A)\n\nMatrix A:\n [[ 2.  3.  1.]\n [ 4.  7.  7.]\n [ 6. 18. 22.]]\n\n\n\nLU decomposition with SciPy\n\n\nP, L, U = lu(A)\n\nprint(\"Permutation matrix P:\\n\", P)\nprint(\"Lower triangular L:\\n\", L)\nprint(\"Upper triangular U:\\n\", U)\n\nPermutation matrix P:\n [[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\nLower triangular L:\n [[1.         0.         0.        ]\n [0.66666667 1.         0.        ]\n [0.33333333 0.6        1.        ]]\nUpper triangular U:\n [[ 6.         18.         22.        ]\n [ 0.         -5.         -7.66666667]\n [ 0.          0.         -1.73333333]]\n\n\nHere, \\(P\\) handles row swaps (partial pivoting), \\(L\\) is lower triangular, and \\(U\\) is upper triangular.\n\nVerifying the factorization\n\n\nreconstructed = P @ L @ U\nprint(\"Does P·L·U equal A?\\n\", np.allclose(reconstructed, A))\n\nDoes P·L·U equal A?\n True\n\n\n\nSolving a system with LU\n\nSuppose we want to solve \\(Ax = b\\). Instead of working directly with \\(A\\), we solve in two steps:\n\nSolve \\(Ly = Pb\\) (forward substitution).\nSolve \\(Ux = y\\) (back substitution).\n\n\nb = np.array([1, 2, 3], dtype=float)\n\n# Step 1: Pb\nPb = P @ b\n\n# Step 2: forward substitution Ly = Pb\ny = np.linalg.solve(L, Pb)\n\n# Step 3: back substitution Ux = y\nx = np.linalg.solve(U, y)\n\nprint(\"Solution x:\", x)\n\nSolution x: [ 0.5 -0.  -0. ]\n\n\n\nEfficiency advantage\n\nIf we have to solve many systems with the same \\(A\\) but different \\(b\\), we only compute \\(LU\\) once, then reuse it. This saves a lot of computation.\n\nNumPy’s built-in rank-revealing factorization\n\nWhile NumPy doesn’t have lu directly, it works seamlessly with SciPy. For large matrices, LU decomposition is the backbone of solvers like np.linalg.solve.\n\n\nTry It Yourself\n\nCompute LU decomposition for\n\\[\nA = \\begin{bmatrix} 1 & 2 & 0 \\\\ 3 & 4 & 4 \\\\ 5 & 6 & 3 \\end{bmatrix}\n\\]\nVerify \\(P·L·U = A\\).\nSolve \\(Ax = b\\) with\n\\[\nb = [3,7,8]\n\\]\nusing LU factorization.\nCompare solving with LU factorization vs directly using np.linalg.solve(A,b). Are the answers the same?\n\n\n\nThe Takeaway\n\nLU factorization captures Gaussian elimination in matrix form: \\(A = P·L·U\\).\nIt allows fast repeated solving of systems with different right-hand sides.\nLU decomposition is a core technique in numerical linear algebra and the basis of many solvers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-4.-vector-spaces-and-subspaces",
    "href": "books/en-US/lab.html#chapter-4.-vector-spaces-and-subspaces",
    "title": "The LAB",
    "section": "Chapter 4. Vector Spaces and Subspaces",
    "text": "Chapter 4. Vector Spaces and Subspaces\n\n31. Axioms of Vector Spaces (What “Space” Really Means)\nVector spaces generalize what we’ve been doing with vectors and matrices. Instead of just \\(\\mathbb{R}^n\\), a vector space is any collection of objects (vectors) where addition and scalar multiplication follow specific axioms (rules). In this lab, we’ll explore these axioms concretely with Python.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nVector space example: \\(\\mathbb{R}^2\\)\n\nLet’s check two rules (axioms): closure under addition and scalar multiplication.\n\nu = np.array([1, 2])\nv = np.array([3, -1])\n\n# Closure under addition\nprint(\"u + v =\", u + v)\n\n# Closure under scalar multiplication\nk = 5\nprint(\"k * u =\", k * u)\n\nu + v = [4 1]\nk * u = [ 5 10]\n\n\nBoth results are still in \\(\\mathbb{R}^2\\).\n\nZero vector and additive inverses\n\nEvery vector space must contain a zero vector, and every vector must have an additive inverse.\n\nzero = np.array([0, 0])\ninverse_u = -u\nprint(\"Zero vector:\", zero)\nprint(\"u + (-u) =\", u + inverse_u)\n\nZero vector: [0 0]\nu + (-u) = [0 0]\n\n\n\nDistributive and associative properties\n\nCheck:\n\n\\(a(u+v) = au + av\\)\n\\((a+b)u = au + bu\\)\n\n\na, b = 2, 3\n\nlhs1 = a * (u + v)\nrhs1 = a*u + a*v\nprint(\"a(u+v) =\", lhs1, \", au+av =\", rhs1)\n\nlhs2 = (a+b) * u\nrhs2 = a*u + b*u\nprint(\"(a+b)u =\", lhs2, \", au+bu =\", rhs2)\n\na(u+v) = [8 2] , au+av = [8 2]\n(a+b)u = [ 5 10] , au+bu = [ 5 10]\n\n\nBoth equalities hold → distributive laws confirmed.\n\nA set that fails to be a vector space\n\nConsider only positive numbers with normal addition and scalar multiplication.\n\npositive_numbers = [1, 2, 3]\ntry:\n    print(\"Closure under negatives?\", -1 * np.array(positive_numbers))\nexcept Exception as e:\n    print(\"Error:\", e)\n\nClosure under negatives? [-1 -2 -3]\n\n\nNegative results leave the set → not a vector space.\n\nPython helper to check axioms\n\nWe can quickly check if a set of vectors is closed under addition and scalar multiplication.\n\ndef check_closure(vectors, scalars):\n    for v in vectors:\n        for u in vectors:\n            if not any(np.array_equal(v+u, w) for w in vectors):\n                return False\n        for k in scalars:\n            if not any(np.array_equal(k*v, w) for w in vectors):\n                return False\n    return True\n\nvectors = [np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])]\nscalars = [0,1,-1]\nprint(\"Closed under addition and scalar multiplication?\", check_closure(vectors, scalars))\n\nClosed under addition and scalar multiplication? False\n\n\nThis small set is closed → it forms a vector space (a subspace of \\(\\mathbb{R}^2\\)).\n\n\nTry It Yourself\n\nVerify that \\(\\mathbb{R}^3\\) satisfies the vector space axioms using random vectors.\nTest whether the set of all 2×2 matrices forms a vector space under normal addition and scalar multiplication.\nFind an example of a set that fails closure (e.g., integers under division).\n\n\n\nThe Takeaway\n\nA vector space is any set where addition and scalar multiplication satisfy 10 standard axioms.\nThese rules ensure consistent algebraic behavior.\nMany objects beyond arrows in \\(\\mathbb{R}^n\\) (like polynomials or matrices) are vector spaces too.\n\n\n\n\n32. Subspaces, Column Space, and Null Space (Where Solutions Live)\nA subspace is a smaller vector space sitting inside a bigger one. For matrices, two subspaces show up all the time:\n\nColumn space: all combinations of the matrix’s columns (possible outputs of \\(Ax\\)).\nNull space: all vectors \\(x\\) such that \\(Ax = 0\\) (inputs that vanish).\n\nThis lab explores both in Python.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nColumn space basics\n\nTake:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{bmatrix}\n\\]\n\nA = Matrix([\n    [1,2],\n    [2,4],\n    [3,6]\n])\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Column space basis:\\n\", A.columnspace())\nprint(\"Rank (dimension of column space):\", A.rank())\n\nMatrix A:\n Matrix([[1, 2], [2, 4], [3, 6]])\nColumn space basis:\n [Matrix([\n[1],\n[2],\n[3]])]\nRank (dimension of column space): 1\n\n\n\nThe second column is a multiple of the first → column space has dimension 1.\nAll outputs of \\(Ax\\) lie on a line in \\(\\mathbb{R}^3\\).\n\n\nNull space basics\n\n\nprint(\"Null space basis:\\n\", A.nullspace())\n\nNull space basis:\n [Matrix([\n[-2],\n[ 1]])]\n\n\nThe null space contains all \\(x\\) where \\(Ax=0\\). Here, the null space is 1-dimensional (vectors like \\([-2,1]\\)).\n\nA full-rank example\n\n\nB = Matrix([\n    [1,0,0],\n    [0,1,0],\n    [0,0,1]\n])\n\nprint(\"Column space basis:\\n\", B.columnspace())\nprint(\"Null space basis:\\n\", B.nullspace())\n\nColumn space basis:\n [Matrix([\n[1],\n[0],\n[0]]), Matrix([\n[0],\n[1],\n[0]]), Matrix([\n[0],\n[0],\n[1]])]\nNull space basis:\n []\n\n\n\nColumn space = all of \\(\\mathbb{R}^3\\).\nNull space = only the zero vector.\n\n\nGeometry link\n\nFor \\(A\\) (rank 1, 2 columns):\n\nColumn space: line in \\(\\mathbb{R}^3\\).\nNull space: line in \\(\\mathbb{R}^2\\).\n\nTogether they explain the system \\(Ax = b\\):\n\nIf \\(b\\) is outside the column space, no solution exists.\nIf \\(b\\) is inside, solutions differ by a vector in the null space.\n\n\nQuick NumPy version\n\nNumPy doesn’t directly give null space, but we can compute it with SVD.\n\nfrom numpy.linalg import svd\n\nA = np.array([[1,2],[2,4],[3,6]], dtype=float)\nU, S, Vt = svd(A)\n\ntol = 1e-10\nnull_mask = (S &lt;= tol)\nnull_space = Vt.T[:, null_mask]\nprint(\"Null space (via SVD):\\n\", null_space)\n\nNull space (via SVD):\n [[-0.89442719]\n [ 0.4472136 ]]\n\n\n\n\nTry It Yourself\n\nFind the column space and null space of\n\\[\n\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n\\]\nHow many dimensions does each have?\nGenerate a random 3×3 matrix. Compute its rank, column space, and null space.\nSolve \\(Ax = b\\) with\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nand describe why it has infinitely many solutions.\n\n\n\nThe Takeaway\n\nThe column space = all possible outputs of a matrix.\nThe null space = all inputs that map to zero.\nThese subspaces give the complete picture of what a matrix does.\n\n\n\n\n33. Span and Generating Sets (Coverage of a Space)\nThe span of a set of vectors is all the linear combinations you can make from them. If a set of vectors can “cover” a whole space, we call it a generating set. This lab shows how to compute and visualize spans.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nSpan in \\(\\mathbb{R}^2\\)\n\nTwo vectors that aren’t multiples span the whole plane.\n\nu = np.array([1, 0])\nv = np.array([0, 1])\n\nM = Matrix.hstack(Matrix(u), Matrix(v))\nprint(\"Rank:\", M.rank())\n\nRank: 2\n\n\nRank = 2 → the span of \\(\\{u,v\\}\\) is all of \\(\\mathbb{R}^2\\).\n\nDependent vectors (smaller span)\n\n\nu = np.array([1, 2])\nv = np.array([2, 4])\n\nM = Matrix.hstack(Matrix(u), Matrix(v))\nprint(\"Rank:\", M.rank())\n\nRank: 1\n\n\nRank = 1 → these vectors only span a line.\n\nVisualizing a span\n\nLet’s see what the span of two vectors looks like.\n\nu = np.array([1, 2])\nv = np.array([2, 1])\n\ncoeffs = np.linspace(-2, 2, 11)\npoints = []\nfor a in coeffs:\n    for b in coeffs:\n        points.append(a*u + b*v)\npoints = np.array(points)\n\nplt.scatter(points[:,0], points[:,1], s=10)\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.title(\"Span of {u,v}\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll see a filled grid - the entire plane, because the two vectors are independent.\n\nGenerating set of a space\n\nFor \\(\\mathbb{R}^3\\):\n\nbasis = [Matrix([1,0,0]), Matrix([0,1,0]), Matrix([0,0,1])]\nM = Matrix.hstack(*basis)\nprint(\"Rank:\", M.rank())\n\nRank: 3\n\n\nRank = 3 → this set spans the whole space.\n\nTesting if a vector is in the span\n\nExample: Is \\([3,5]\\) in the span of \\([1,2]\\) and \\([2,1]\\)?\n\nu = Matrix([1,2])\nv = Matrix([2,1])\ntarget = Matrix([3,5])\n\nM = Matrix.hstack(u,v)\nsolution = M.gauss_jordan_solve(target)\nprint(\"Coefficients (a,b):\", solution)\n\nCoefficients (a,b): (Matrix([\n[7/3],\n[1/3]]), Matrix(0, 1, []))\n\n\nIf a solution exists, the target is in the span.\n\n\nTry It Yourself\n\nTest if \\([4,6]\\) is in the span of \\([1,2]\\).\nVisualize the span of \\([1,0,0]\\) and \\([0,1,0]\\) in \\(\\mathbb{R}^3\\). What does it look like?\nCreate a random 3×3 matrix. Use rank() to check if its columns span \\(\\mathbb{R}^3\\).\n\n\n\nThe Takeaway\n\nSpan = all linear combinations of a set of vectors.\nIndependent vectors span bigger spaces; dependent ones collapse to smaller spaces.\nGenerating sets are the foundation of bases and coordinate systems.\n\n\n\n\n34. Linear Independence and Dependence (No Redundancy vs. Redundancy)\nA set of vectors is linearly independent if none of them can be written as a combination of the others. If at least one can, the set is dependent. This distinction tells us whether a set of vectors has redundancy.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nIndependent vectors example\n\n\nv1 = Matrix([1, 0, 0])\nv2 = Matrix([0, 1, 0])\nv3 = Matrix([0, 0, 1])\n\nM = Matrix.hstack(v1, v2, v3)\nprint(\"Rank:\", M.rank(), \" Number of vectors:\", M.shape[1])\n\nRank: 3  Number of vectors: 3\n\n\nRank = 3, number of vectors = 3 → all independent.\n\nDependent vectors example\n\n\nv1 = Matrix([1, 2, 3])\nv2 = Matrix([2, 4, 6])\nv3 = Matrix([3, 6, 9])\n\nM = Matrix.hstack(v1, v2, v3)\nprint(\"Rank:\", M.rank(), \" Number of vectors:\", M.shape[1])\n\nRank: 1  Number of vectors: 3\n\n\nRank = 1, number of vectors = 3 → they’re dependent (multiples of each other).\n\nChecking dependence automatically\n\nA quick test: if rank &lt; number of vectors → dependent.\n\ndef check_independence(vectors):\n    M = Matrix.hstack(*vectors)\n    return M.rank() == M.shape[1]\n\nprint(\"Independent?\", check_independence([Matrix([1,0]), Matrix([0,1])]))\nprint(\"Independent?\", check_independence([Matrix([1,2]), Matrix([2,4])]))\n\nIndependent? True\nIndependent? False\n\n\n\nSolving for dependence relation\n\nIf vectors are dependent, we can find coefficients \\(c_1, c_2, …\\) such that\n\\[\nc_1 v_1 + c_2 v_2 + … + c_k v_k = 0\n\\]\nwith some \\(c_i \\neq 0\\).\n\nM = Matrix.hstack(Matrix([1,2]), Matrix([2,4]))\nnull_space = M.nullspace()\nprint(\"Dependence relation (coefficients):\", null_space)\n\nDependence relation (coefficients): [Matrix([\n[-2],\n[ 1]])]\n\n\nThis shows the exact linear relation.\n\nRandom example\n\n\nnp.random.seed(0)\nR = Matrix(np.random.randint(-3, 4, (3,3)))\nprint(\"Random matrix:\\n\", R)\nprint(\"Rank:\", R.rank())\n\nRandom matrix:\n Matrix([[1, 2, -3], [0, 0, 0], [-2, 0, 2]])\nRank: 2\n\n\nDepending on the rank, the columns may be independent (rank = 3) or dependent (rank &lt; 3).\n\n\nTry It Yourself\n\nTest if \\([1,1,0], [0,1,1], [1,2,1]\\) are independent.\nGenerate 4 random vectors in \\(\\mathbb{R}^3\\). Can they ever be independent? Why or why not?\nFind the dependence relation for \\([2,4], [3,6]\\).\n\n\n\nThe Takeaway\n\nIndependent set: no redundancy, each vector adds a new direction.\nDependent set: at least one vector is unnecessary (it lies in the span of others).\nIndependence is the key to defining basis and dimension.\n\n\n\n\n35. Basis and Coordinates (Naming Every Vector Uniquely)\nA basis is a set of independent vectors that span a space. It’s like choosing a coordinate system: every vector in the space can be expressed uniquely as a combination of basis vectors. In this lab, we’ll see how to find bases and compute coordinates relative to them.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nStandard basis in \\(\\mathbb{R}^3\\)\n\n\ne1 = Matrix([1,0,0])\ne2 = Matrix([0,1,0])\ne3 = Matrix([0,0,1])\n\nM = Matrix.hstack(e1, e2, e3)\nprint(\"Rank:\", M.rank())\n\nRank: 3\n\n\nThese three independent vectors form the standard basis of \\(\\mathbb{R}^3\\). Any vector like \\([2,5,-1]\\) can be expressed as\n\\[\n2e_1 + 5e_2 - 1e_3\n\\]\n\nFinding a basis from dependent vectors\n\n\nv1 = Matrix([1,2,3])\nv2 = Matrix([2,4,6])\nv3 = Matrix([1,0,1])\n\nM = Matrix.hstack(v1,v2,v3)\nprint(\"Column space basis:\", M.columnspace())\n\nColumn space basis: [Matrix([\n[1],\n[2],\n[3]]), Matrix([\n[1],\n[0],\n[1]])]\n\n\nSymPy extracts independent columns automatically. This gives a basis for the column space.\n\nCoordinates relative to a basis\n\nSuppose basis = \\(\\{ [1,0], [1,1] \\}\\). Express vector \\([3,5]\\) in this basis.\n\nB = Matrix.hstack(Matrix([1,0]), Matrix([1,1]))\ntarget = Matrix([3,5])\n\ncoords = B.solve_least_squares(target)\nprint(\"Coordinates in basis B:\", coords)\n\nCoordinates in basis B: Matrix([[-2], [5]])\n\n\nSo \\([3,5] = 3·[1,0] + 2·[1,1]\\).\n\nBasis change\n\nIf we switch to a different basis, coordinates change but the vector stays the same.\n\nnew_basis = Matrix.hstack(Matrix([2,1]), Matrix([1,2]))\ncoords_new = new_basis.solve_least_squares(target)\nprint(\"Coordinates in new basis:\", coords_new)\n\nCoordinates in new basis: Matrix([[1/3], [7/3]])\n\n\n\nRandom example\n\nGenerate 3 random vectors in \\(\\mathbb{R}^3\\). Check if they form a basis.\n\nnp.random.seed(1)\nR = Matrix(np.random.randint(-3,4,(3,3)))\nprint(\"Random matrix:\\n\", R)\nprint(\"Rank:\", R.rank())\n\nRandom matrix:\n Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])\nRank: 3\n\n\nIf rank = 3 → basis for \\(\\mathbb{R}^3\\). Otherwise, only span a subspace.\n\n\nTry It Yourself\n\nCheck if \\([1,2], [3,4]\\) form a basis of \\(\\mathbb{R}^2\\).\nExpress vector \\([7,5]\\) in that basis.\nCreate 4 random vectors in \\(\\mathbb{R}^3\\). Find a basis for their span.\n\n\n\nThe Takeaway\n\nA basis = minimal set of vectors that span a space.\nEvery vector has a unique coordinate representation in a given basis.\nChanging bases changes the coordinates, not the vector itself.\n\n\n\n\n36. Dimension (How Many Directions)\nThe dimension of a vector space is the number of independent directions it has. Formally, it’s the number of vectors in any basis of the space. Dimension tells us the “size” of a space in terms of degrees of freedom.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nDimension of \\(\\mathbb{R}^n\\)\n\nThe dimension of \\(\\mathbb{R}^n\\) is \\(n\\).\n\nn = 4\nbasis = [Matrix.eye(n)[:,i] for i in range(n)]\nprint(\"Basis for R^4:\", basis)\nprint(\"Dimension of R^4:\", len(basis))\n\nBasis for R^4: [Matrix([\n[1],\n[0],\n[0],\n[0]]), Matrix([\n[0],\n[1],\n[0],\n[0]]), Matrix([\n[0],\n[0],\n[1],\n[0]]), Matrix([\n[0],\n[0],\n[0],\n[1]])]\nDimension of R^4: 4\n\n\nEach standard unit vector adds one independent direction → dimension = 4.\n\nDimension via rank\n\nThe rank of a matrix equals the dimension of its column space.\n\nA = Matrix([\n    [1,2,3],\n    [2,4,6],\n    [1,0,1]\n])\n\nprint(\"Rank (dimension of column space):\", A.rank())\n\nRank (dimension of column space): 2\n\n\nHere, rank = 2 → the column space is a 2D plane inside \\(\\mathbb{R}^3\\).\n\nNull space dimension\n\nThe null space dimension is given by:\n\\[\n\\text{dim(Null(A))} = \\#\\text{variables} - \\text{rank(A)}\n\\]\n\nprint(\"Null space basis:\", A.nullspace())\nprint(\"Dimension of null space:\", len(A.nullspace()))\n\nNull space basis: [Matrix([\n[-1],\n[-1],\n[ 1]])]\nDimension of null space: 1\n\n\nThis is the number of free variables in a solution.\n\nDimension in practice\n\n\nA line through the origin in \\(\\mathbb{R}^3\\) has dimension 1.\nA plane through the origin has dimension 2.\nThe whole \\(\\mathbb{R}^3\\) has dimension 3.\n\nExample:\n\nv1 = Matrix([1,2,3])\nv2 = Matrix([2,4,6])\nspan = Matrix.hstack(v1,v2)\nprint(\"Dimension of span:\", span.rank())\n\nDimension of span: 1\n\n\nResult = 1 → they only generate a line.\n\nRandom example\n\n\nnp.random.seed(2)\nR = Matrix(np.random.randint(-3,4,(4,4)))\nprint(\"Random 4x4 matrix:\\n\", R)\nprint(\"Column space dimension:\", R.rank())\n\nRandom 4x4 matrix:\n Matrix([[-3, 2, -3, 3], [0, -1, 0, -3], [-1, -2, 0, 2], [-1, 1, 1, 1]])\nColumn space dimension: 4\n\n\nRank may be 4 (full space) or smaller (collapsed).\n\n\nTry It Yourself\n\nFind the dimension of the column space of\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 \\\\  \n0 & 1 & 1 \\\\  \n0 & 0 & 0  \n\\end{bmatrix}\n\\]\nCompute the dimension of the null space of a 3×3 singular matrix.\nGenerate a 5×3 random matrix and compute its column space dimension.\n\n\n\nThe Takeaway\n\nDimension = number of independent directions.\nFound by counting basis vectors (or rank).\nDimensions describe lines (1D), planes (2D), and higher subspaces inside larger spaces.\n\n\n\n\n37. Rank–Nullity Theorem (Dimensions That Add Up)\nThe rank–nullity theorem ties together the dimension of the column space and the null space of a matrix. It says:\n\\[\n\\text{rank}(A) + \\text{nullity}(A) = \\text{number of columns of } A\n\\]\nThis is a powerful consistency check in linear algebra.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple 3×3 example\n\n\nA = Matrix([\n    [1, 2, 3],\n    [2, 4, 6],\n    [1, 0, 1]\n])\n\nrank = A.rank()\nnullity = len(A.nullspace())\nprint(\"Rank:\", rank)\nprint(\"Nullity:\", nullity)\nprint(\"Rank + Nullity =\", rank + nullity)\nprint(\"Number of columns =\", A.shape[1])\n\nRank: 2\nNullity: 1\nRank + Nullity = 3\nNumber of columns = 3\n\n\nYou should see that rank + nullity = 3, the number of columns.\n\nFull-rank case\n\n\nB = Matrix([\n    [1,0,0],\n    [0,1,0],\n    [0,0,1]\n])\n\nprint(\"Rank:\", B.rank())\nprint(\"Nullity:\", len(B.nullspace()))\n\nRank: 3\nNullity: 0\n\n\n\nRank = 3 (all independent).\nNullity = 0 (only zero solution to \\(Bx=0\\)).\nRank + Nullity = 3 columns.\n\n\nWide matrix (more columns than rows)\n\n\nC = Matrix([\n    [1,2,3,4],\n    [0,1,1,2],\n    [0,0,0,0]\n])\n\nrank = C.rank()\nnullity = len(C.nullspace())\nprint(\"Rank:\", rank, \" Nullity:\", nullity, \" Columns:\", C.shape[1])\n\nRank: 2  Nullity: 2  Columns: 4\n\n\nHere, nullity &gt; 0 because there are more variables than independent equations.\n\nVerifying with random matrices\n\n\nnp.random.seed(3)\nR = Matrix(np.random.randint(-3,4,(4,5)))\nprint(\"Random 4x5 matrix:\\n\", R)\nprint(\"Rank + Nullity =\", R.rank() + len(R.nullspace()))\nprint(\"Number of columns =\", R.shape[1])\n\nRandom 4x5 matrix:\n Matrix([[-1, -3, -2, 0, -3], [-3, -3, 2, 2, 0], [-1, 0, -2, -2, -1], [2, 3, -3, 1, 1]])\nRank + Nullity = 5\nNumber of columns = 5\n\n\nAlways consistent: rank + nullity = number of columns.\n\nGeometric interpretation\n\nFor an \\(m \\times n\\) matrix:\n\nRank(A) = dimension of outputs (column space).\nNullity(A) = dimension of hidden directions that collapse to 0.\nTogether, they use up all the “input dimensions” (n).\n\n\n\nTry It Yourself\n\nCompute rank and nullity of\n\\[\n\\begin{bmatrix}  \n1 & 1 & 1 \\\\  \n0 & 1 & 1  \n\\end{bmatrix}\n\\]\nCheck the theorem.\nCreate a 2×4 random integer matrix. Confirm that rank + nullity = 4.\nExplain why a tall full-rank \\(5 \\times 3\\) matrix must have nullity = 0.\n\n\n\nThe Takeaway\n\nRank + Nullity = number of columns (always true).\nRank measures independent outputs; nullity measures hidden freedom.\nThis theorem connects solutions of \\(Ax=0\\) with the structure of \\(A\\).\n\n\n\n\n38. Coordinates Relative to a Basis (Changing the “Ruler”)\nOnce we choose a basis, every vector can be described with coordinates relative to that basis. This is like changing the “ruler” we use to measure vectors. In this lab, we’ll practice computing coordinates in different bases.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nStandard basis coordinates\n\nVector \\(v = [4,5]\\) in \\(\\mathbb{R}^2\\):\n\nv = Matrix([4,5])\ne1 = Matrix([1,0])\ne2 = Matrix([0,1])\n\nB = Matrix.hstack(e1,e2)\ncoords = B.solve_least_squares(v)\nprint(\"Coordinates in standard basis:\", coords)\n\nCoordinates in standard basis: Matrix([[4], [5]])\n\n\nResult is just \\([4,5]\\). Easy - the standard basis matches the components directly.\n\nNon-standard basis\n\nSuppose basis = \\(\\{ [1,1], [1,-1] \\}\\). Express \\(v = [4,5]\\) in this basis.\n\nB2 = Matrix.hstack(Matrix([1,1]), Matrix([1,-1]))\ncoords2 = B2.solve_least_squares(v)\nprint(\"Coordinates in new basis:\", coords2)\n\nCoordinates in new basis: Matrix([[9/2], [-1/2]])\n\n\nNow \\(v\\) has different coordinates.\n\nChanging coordinates back\n\nTo reconstruct the vector from coordinates:\n\nreconstructed = B2 * coords2\nprint(\"Reconstructed vector:\", reconstructed)\n\nReconstructed vector: Matrix([[4], [5]])\n\n\nIt matches the original \\([4,5]\\).\n\nRandom basis in \\(\\mathbb{R}^3\\)\n\n\nbasis = Matrix.hstack(\n    Matrix([1,0,1]),\n    Matrix([0,1,1]),\n    Matrix([1,1,0])\n)\nv = Matrix([2,3,4])\n\ncoords = basis.solve_least_squares(v)\nprint(\"Coordinates of v in random basis:\", coords)\n\nCoordinates of v in random basis: Matrix([[3/2], [5/2], [1/2]])\n\n\nAny independent set of 3 vectors in \\(\\mathbb{R}^3\\) works as a basis.\n\nVisualization in 2D\n\nLet’s compare coordinates in two bases.\n\nimport matplotlib.pyplot as plt\n\nv = np.array([4,5])\nb1 = np.array([1,1])\nb2 = np.array([1,-1])\n\nplt.quiver(0,0,v[0],v[1],angles='xy',scale_units='xy',scale=1,color='blue',label='v')\nplt.quiver(0,0,b1[0],b1[1],angles='xy',scale_units='xy',scale=1,color='red',label='basis1')\nplt.quiver(0,0,b2[0],b2[1],angles='xy',scale_units='xy',scale=1,color='green',label='basis2')\n\nplt.xlim(-1,6)\nplt.ylim(-6,6)\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nEven though the basis vectors look different, they span the same space, and \\(v\\) can be expressed in terms of them.\n\n\nTry It Yourself\n\nExpress \\([7,3]\\) in the basis \\(\\{[2,0], [0,3]\\}\\).\nPick three independent random vectors in \\(\\mathbb{R}^3\\). Write down the coordinates of \\([1,2,3]\\) in that basis.\nVerify that reconstructing always gives the original vector.\n\n\n\nThe Takeaway\n\nA basis provides a coordinate system for vectors.\nCoordinates depend on the basis, but the underlying vector doesn’t change.\nChanging the basis is like changing the “ruler” you measure vectors with.\n\n\n\n\n39. Change-of-Basis Matrices (Moving Between Coordinate Systems)\nWhen we switch from one basis to another, we need a change-of-basis matrix. This matrix acts like a translator: it converts coordinates in one system to coordinates in another.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nTwo bases in \\(\\mathbb{R}^2\\)\n\nLet’s define:\n\nBasis \\(B = \\{ [1,0], [0,1] \\}\\) (standard basis).\nBasis \\(C = \\{ [1,1], [1,-1] \\}\\).\n\n\nB = Matrix.hstack(Matrix([1,0]), Matrix([0,1]))\nC = Matrix.hstack(Matrix([1,1]), Matrix([1,-1]))\n\n\nChange-of-basis matrix\n\nThe matrix that converts C-coordinates → standard coordinates is just \\(C\\).\n\nprint(\"C (basis matrix):\\n\", C)\n\nC (basis matrix):\n Matrix([[1, 1], [1, -1]])\n\n\nTo go the other way (standard → C), we compute the inverse of \\(C\\).\n\nC_inv = C.inv()\nprint(\"C inverse:\\n\", C_inv)\n\nC inverse:\n Matrix([[1/2, 1/2], [1/2, -1/2]])\n\n\n\nConverting coordinates\n\nVector \\(v = [4,5]\\).\n\nIn standard basis:\n\n\nv = Matrix([4,5])\ncoords_in_standard = v\nprint(\"Coordinates in standard basis:\", coords_in_standard)\n\nCoordinates in standard basis: Matrix([[4], [5]])\n\n\n\nIn basis \\(C\\):\n\n\ncoords_in_C = C_inv * v\nprint(\"Coordinates in C basis:\", coords_in_C)\n\nCoordinates in C basis: Matrix([[9/2], [-1/2]])\n\n\n\nConvert back:\n\n\nreconstructed = C * coords_in_C\nprint(\"Reconstructed vector:\", reconstructed)\n\nReconstructed vector: Matrix([[4], [5]])\n\n\nThe reconstruction matches the original vector.\n\nGeneral formula\n\nIf \\(P\\) is the change-of-basis matrix from basis \\(B\\) to basis \\(C\\):\n\\[\n[v]_C = P^{-1}[v]_B\n\\]\n\\[\n[v]_B = P[v]_C\n\\]\nHere, \\(P\\) is the matrix of new basis vectors written in terms of the old basis.\n\nRandom 3D example\n\n\nB = Matrix.eye(3)  # standard basis\nC = Matrix.hstack(\n    Matrix([1,0,1]),\n    Matrix([0,1,1]),\n    Matrix([1,1,0])\n)\n\nv = Matrix([2,3,4])\n\nC_inv = C.inv()\ncoords_in_C = C_inv * v\nprint(\"Coordinates in new basis C:\", coords_in_C)\n\nprint(\"Back to standard:\", C * coords_in_C)\n\nCoordinates in new basis C: Matrix([[3/2], [5/2], [1/2]])\nBack to standard: Matrix([[2], [3], [4]])\n\n\n\n\nTry It Yourself\n\nConvert \\([7,3]\\) from the standard basis to the basis \\(\\{[2,0], [0,3]\\}\\).\nPick a random invertible 3×3 matrix as a basis. Write a vector in that basis, then convert it back to the standard basis.\nProve that converting back and forth always returns the same vector.\n\n\n\nThe Takeaway\n\nA change-of-basis matrix converts coordinates between bases.\nGoing from new basis → old basis uses the basis matrix.\nGoing from old basis → new basis requires its inverse.\nThe vector itself never changes - only the description of it does.\n\n\n\n\n40. Affine Subspaces (Lines and Planes Not Through the Origin)\nSo far, subspaces always passed through the origin. But many familiar objects - like lines offset from the origin or planes floating in space - are affine subspaces. They look like subspaces, just shifted away from zero.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nLine through the origin (a subspace)\n\n\\[\nL = \\{ t \\cdot [1,2] : t \\in \\mathbb{R} \\}\n\\]\n\nt = np.linspace(-3,3,20)\nline_origin = np.array([t, 2*t]).T\nplt.plot(line_origin[:,0], line_origin[:,1], label=\"Through origin\")\n\n\n\n\n\n\n\n\n\nLine not through the origin (affine subspace)\n\n\\[\nL' = \\{ [3,1] + t \\cdot [1,2] : t \\in \\mathbb{R} \\}\n\\]\n\npoint = np.array([3,1])\ndirection = np.array([1,2])\nline_shifted = np.array([point + k*direction for k in t])\nplt.plot(line_shifted[:,0], line_shifted[:,1], label=\"Shifted line\")\n\n\n\n\n\n\n\n\n\nVisualizing together\n\n\nplt.scatter(*point, color=\"red\", label=\"Shift point\")\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nOne line passes through the origin, the other is parallel but shifted.\n\nPlane example\n\nA plane in \\(\\mathbb{R}^3\\):\n\\[\nP = \\{ [1,2,3] + s[1,0,0] + t[0,1,0] : s,t \\in \\mathbb{R} \\}\n\\]\nThis is an affine plane parallel to the \\(xy\\)-plane, but shifted.\n\ns_vals = np.linspace(-2,2,10)\nt_vals = np.linspace(-2,2,10)\n\npoints = []\nfor s in s_vals:\n    for t in t_vals:\n        points.append([1,2,3] + s*np.array([1,0,0]) + t*np.array([0,1,0]))\n\npoints = np.array(points)\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(points[:,0], points[:,1], points[:,2])\nax.set_title(\"Affine plane in R^3\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAlgebraic difference\n\n\nA subspace must satisfy closure under addition and scalar multiplication, and must include 0.\nAn affine subspace is just a subspace plus a fixed shift vector.\n\n\n\nTry It Yourself\n\nDefine a line in \\(\\mathbb{R}^2\\):\n\\[\n(x,y) = (2,3) + t(1,-1)\n\\]\nPlot it and compare with the subspace spanned by \\((1,-1)\\).\nConstruct an affine plane in \\(\\mathbb{R}^3\\) shifted by vector \\((5,5,5)\\).\nShow algebraically that subtracting the shift point turns an affine subspace back into a regular subspace.\n\n\n\nThe Takeaway\n\nSubspaces go through the origin.\nAffine subspaces are shifted copies of subspaces.\nThey’re essential in geometry, computer graphics, and optimization (e.g., feasible regions in linear programming).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-5.-linear-transformation-and-structure",
    "href": "books/en-US/lab.html#chapter-5.-linear-transformation-and-structure",
    "title": "The LAB",
    "section": "Chapter 5. Linear Transformation and Structure",
    "text": "Chapter 5. Linear Transformation and Structure\n\n41. Linear Transformations (Preserving Lines and Sums)\nA linear transformation is a function between vector spaces that preserves two key properties:\n\nAdditivity: \\(T(u+v) = T(u) + T(v)\\)\nHomogeneity: \\(T(cu) = cT(u)\\)\n\nIn practice, every linear transformation can be represented by a matrix. This lab will help you understand and experiment with linear transformations in Python.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple linear transformation (scaling)\n\nLet’s scale vectors by 2 in the x-direction and by 0.5 in the y-direction.\n\nA = np.array([\n    [2, 0],\n    [0, 0.5]\n])\n\nv = np.array([1, 2])\nTv = A @ v\nprint(\"Original v:\", v)\nprint(\"Transformed Tv:\", Tv)\n\nOriginal v: [1 2]\nTransformed Tv: [2. 1.]\n\n\n\nVisualizing multiple vectors\n\n\nvectors = [np.array([1,1]), np.array([2,0]), np.array([-1,2])]\n\nfor v in vectors:\n    Tv = A @ v\n    plt.arrow(0,0,v[0],v[1],head_width=0.1,color='blue',length_includes_head=True)\n    plt.arrow(0,0,Tv[0],Tv[1],head_width=0.1,color='red',length_includes_head=True)\n\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.xlim(-3,5)\nplt.ylim(-1,5)\nplt.grid()\nplt.title(\"Blue = original, Red = transformed\")\nplt.show()\n\n\n\n\n\n\n\n\nBlue arrows are the original vectors; red arrows are the transformed ones. Notice how the transformation stretches and compresses consistently.\n\nRotation as a linear transformation\n\nRotating vectors by \\(\\theta = 90^\\circ\\):\n\ntheta = np.pi/2\nR = np.array([\n    [np.cos(theta), -np.sin(theta)],\n    [np.sin(theta),  np.cos(theta)]\n])\n\nv = np.array([1,0])\nprint(\"Rotate [1,0] by 90°:\", R @ v)\n\nRotate [1,0] by 90°: [6.123234e-17 1.000000e+00]\n\n\nThe result is \\([0,1]\\), a perfect rotation.\n\nChecking linearity\n\n\nu = np.array([1,2])\nv = np.array([3,4])\nc = 5\n\nlhs = A @ (u+v)\nrhs = A@u + A@v\nprint(\"Additivity holds?\", np.allclose(lhs,rhs))\n\nlhs = A @ (c*u)\nrhs = c*(A@u)\nprint(\"Homogeneity holds?\", np.allclose(lhs,rhs))\n\nAdditivity holds? True\nHomogeneity holds? True\n\n\nBoth checks return True, proving \\(T\\) is linear.\n\nNon-linear example (for contrast)\n\nA transformation like \\(T(x,y) = (x^2, y)\\) is not linear.\n\ndef nonlinear(v):\n    return np.array([v[0]**2, v[1]])\n\nprint(\"T([2,3]) =\", nonlinear(np.array([2,3])))\nprint(\"Check additivity:\", nonlinear(np.array([1,2])+np.array([3,4])) == (nonlinear([1,2])+nonlinear([3,4])))\n\nT([2,3]) = [4 3]\nCheck additivity: [False  True]\n\n\nThis fails the additivity test, so it’s not linear.\n\n\nTry It Yourself\n\nDefine a shear matrix\n\\[\nS = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\n\\]\nApply it to vectors and plot before/after.\nVerify linearity for rotation by 45°.\nTest whether \\(T(x,y) = (x+y, y)\\) is linear.\n\n\n\nThe Takeaway\n\nA linear transformation preserves vector addition and scalar multiplication.\nEvery linear transformation can be represented by a matrix.\nVisualizing with arrows helps build geometric intuition: stretching, rotating, and shearing are all linear.\n\n\n\n\n42. Matrix Representation of a Linear Map (Choosing a Basis)\nEvery linear transformation can be written as a matrix, but the exact matrix depends on the basis you choose. This lab shows how to build and interpret matrix representations.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nFrom transformation to matrix\n\nSuppose \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) is defined by:\n\\[\nT(x,y) = (2x + y, \\; x - y)\n\\]\nTo find its matrix in the standard basis, apply \\(T\\) to each basis vector:\n\ne1 = Matrix([1,0])\ne2 = Matrix([0,1])\n\ndef T(v):\n    x, y = v\n    return Matrix([2*x + y, x - y])\n\nprint(\"T(e1):\", T(e1))\nprint(\"T(e2):\", T(e2))\n\nT(e1): Matrix([[2], [1]])\nT(e2): Matrix([[1], [-1]])\n\n\nStacking results as columns gives the matrix:\n\nA = Matrix.hstack(T(e1), T(e2))\nprint(\"Matrix representation in standard basis:\\n\", A)\n\nMatrix representation in standard basis:\n Matrix([[2, 1], [1, -1]])\n\n\n\nUsing the matrix for computations\n\n\nv = Matrix([3,4])\nprint(\"T(v) via definition:\", T(v))\nprint(\"T(v) via matrix:\", A*v)\n\nT(v) via definition: Matrix([[10], [-1]])\nT(v) via matrix: Matrix([[10], [-1]])\n\n\nBoth methods match.\n\nMatrix in a different basis\n\nNow suppose we use basis\n\\[\nB = \\{ [1,1], [1,-1] \\}\n\\]\nTo represent \\(T\\) in this basis:\n\nBuild the change-of-basis matrix \\(P\\).\nCompute \\(A_B = P^{-1}AP\\).\n\n\nB = Matrix.hstack(Matrix([1,1]), Matrix([1,-1]))\nP = B\nA_B = P.inv() * A * P\nprint(\"Matrix representation in new basis:\\n\", A_B)\n\nMatrix representation in new basis:\n Matrix([[3/2, 3/2], [3/2, -1/2]])\n\n\n\nInterpretation\n\n\nIn standard basis, \\(A\\) tells us how \\(T\\) acts on unit vectors.\nIn basis \\(B\\), \\(A_B\\) shows how \\(T\\) looks when described using different coordinates.\n\n\nRandom linear map in \\(\\mathbb{R}^3\\)\n\n\nnp.random.seed(1)\nA3 = Matrix(np.random.randint(-3,4,(3,3)))\nprint(\"Random transformation matrix:\\n\", A3)\n\nB3 = Matrix.hstack(Matrix([1,0,1]), Matrix([0,1,1]), Matrix([1,1,0]))\nA3_B = B3.inv() * A3 * B3\nprint(\"Representation in new basis:\\n\", A3_B)\n\nRandom transformation matrix:\n Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])\nRepresentation in new basis:\n Matrix([[5/2, -3/2, 3], [-7/2, -9/2, -4], [1/2, 5/2, -1]])\n\n\n\n\nTry It Yourself\n\nDefine \\(T(x,y) = (x+2y, 3x+y)\\). Find its matrix in the standard basis.\nUse a new basis \\(\\{[2,0],[0,3]\\}\\). Compute the representation \\(A_B\\).\nVerify that applying \\(T\\) directly to a vector matches computing via \\(A_B\\) and change-of-basis.\n\n\n\nThe Takeaway\n\nA linear transformation becomes a matrix representation once a basis is chosen.\nColumns of the matrix = images of basis vectors.\nChanging the basis changes the matrix, but the transformation itself stays the same.\n\n\n\n\n43. Kernel and Image (Inputs That Vanish; Outputs We Can Reach)\nTwo fundamental subspaces describe any linear transformation \\(T(x) = Ax\\):\n\nKernel (null space): all vectors \\(x\\) such that \\(Ax = 0\\).\nImage (column space): all possible outputs \\(Ax\\).\n\nThe kernel tells us what inputs collapse to zero, while the image tells us what outputs are achievable.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nKernel of a matrix\n\nConsider\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\\]\n\nA = Matrix([\n    [1,2,3],\n    [2,4,6]\n])\n\nprint(\"Null space (kernel):\", A.nullspace())\n\nNull space (kernel): [Matrix([\n[-2],\n[ 1],\n[ 0]]), Matrix([\n[-3],\n[ 0],\n[ 1]])]\n\n\nThe null space basis shows dependencies among columns. Here, the kernel is 2-dimensional because columns are dependent.\n\nImage (column space)\n\n\nprint(\"Column space (image):\", A.columnspace())\nprint(\"Rank (dimension of image):\", A.rank())\n\nColumn space (image): [Matrix([\n[1],\n[2]])]\nRank (dimension of image): 1\n\n\nThe image is spanned by \\([1,2]^T\\). So all outputs of \\(A\\) are multiples of this vector.\n\nInterpretation\n\n\nKernel vectors → directions that map to zero.\nImage vectors → directions we can actually reach in the output space.\n\nIf \\(x \\in \\ker(A)\\), then \\(Ax = 0\\). If \\(b\\) is not in the image, the system \\(Ax = b\\) has no solution.\n\nExample with full rank\n\n\nB = Matrix([\n    [1,0,0],\n    [0,1,0],\n    [0,0,1]\n])\n\nprint(\"Kernel of B:\", B.nullspace())\nprint(\"Image of B:\", B.columnspace())\n\nKernel of B: []\nImage of B: [Matrix([\n[1],\n[0],\n[0]]), Matrix([\n[0],\n[1],\n[0]]), Matrix([\n[0],\n[0],\n[1]])]\n\n\n\nKernel = only zero vector.\nImage = all of \\(\\mathbb{R}^3\\).\n\n\nNumPy version (image via column space)\n\n\nA = np.array([[1,2,3],[2,4,6]], dtype=float)\nrank = np.linalg.matrix_rank(A)\nprint(\"Rank with NumPy:\", rank)\n\nRank with NumPy: 1\n\n\nNumPy doesn’t compute null spaces directly, but we can use SVD for that if needed.\n\n\nTry It Yourself\n\nCompute kernel and image for\n\\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n\\]\nWhat do they look like?\nTake a random 3×4 matrix and find its kernel and image dimensions.\nSolve \\(Ax = b\\) for a matrix \\(A\\). Try two different \\(b\\): one inside the image, one outside. Observe the difference.\n\n\n\nThe Takeaway\n\nKernel = inputs that vanish under \\(A\\).\nImage = outputs that can be reached by \\(A\\).\nTogether, they fully describe what a linear map does: what it “kills” and what it “produces.”\n\n\n\n\n44. Invertibility and Isomorphisms (Perfectly Reversible Maps)\nA matrix (or linear map) is invertible if it has an inverse \\(A^{-1}\\) such that\n\\[\nA^{-1}A = I \\quad \\text{and} \\quad AA^{-1} = I\n\\]\nAn invertible map is also called an isomorphism, because it preserves all information - every input has exactly one output, and every output comes from exactly one input.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nChecking invertibility\n\n\nA = Matrix([\n    [2,1],\n    [5,3]\n])\n\nprint(\"Determinant:\", A.det())\nprint(\"Is invertible?\", A.det() != 0)\n\nDeterminant: 1\nIs invertible? True\n\n\nIf determinant ≠ 0 → invertible.\n\nComputing the inverse\n\n\nA_inv = A.inv()\nprint(\"Inverse matrix:\\n\", A_inv)\n\nprint(\"Check A*A_inv = I:\\n\", A * A_inv)\n\nInverse matrix:\n Matrix([[3, -1], [-5, 2]])\nCheck A*A_inv = I:\n Matrix([[1, 0], [0, 1]])\n\n\n\nSolving systems with inverses\n\nFor \\(Ax = b\\), if \\(A\\) is invertible:\n\nb = Matrix([1,2])\nx = A_inv * b\nprint(\"Solution x:\", x)\n\nSolution x: Matrix([[1], [-1]])\n\n\nThis is equivalent to A.solve(b) in SymPy or np.linalg.solve in NumPy.\n\nNon-invertible (singular) example\n\n\nB = Matrix([\n    [1,2],\n    [2,4]\n])\n\nprint(\"Determinant:\", B.det())\nprint(\"Is invertible?\", B.det() != 0)\n\nDeterminant: 0\nIs invertible? False\n\n\nDeterminant = 0 → no inverse. The matrix collapses space onto a line, losing information.\n\nNumPy version\n\n\nA = np.array([[2,1],[5,3]], dtype=float)\nprint(\"Determinant:\", np.linalg.det(A))\nprint(\"Inverse:\\n\", np.linalg.inv(A))\n\nDeterminant: 1.0000000000000009\nInverse:\n [[ 3. -1.]\n [-5.  2.]]\n\n\n\nGeometric intuition\n\n\nInvertible transformation = reversible (like rotating, scaling by nonzero).\nNon-invertible transformation = squashing space into a lower dimension (like flattening a plane onto a line).\n\n\n\nTry It Yourself\n\nTest whether\n\\[\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\]\nis invertible and find its inverse.\nCompute the determinant of a 3×3 random integer matrix. If it’s nonzero, find its inverse.\nCreate a singular 3×3 matrix (make one row a multiple of another). Confirm it has no inverse.\n\n\n\nThe Takeaway\n\nInvertible matrix ↔︎ isomorphism: perfectly reversible, no information lost.\nDeterminant ≠ 0 → invertible; determinant = 0 → singular.\nInverses are useful conceptually, but in computation we usually solve systems directly instead of calculating \\(A^{-1}\\).\n\n\n\n\n45. Composition, Powers, and Iteration (Doing It Again and Again)\nLinear transformations can be chained together. Applying one after another is called composition, and in matrix form this becomes multiplication. Repeated application of the same transformation leads to powers of a matrix.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nComposition of transformations\n\nSuppose we have two linear maps:\n\n\\(T_1\\): rotate by 90°\n\\(T_2\\): scale x by 2\n\n\ntheta = np.pi/2\nR = np.array([\n    [np.cos(theta), -np.sin(theta)],\n    [np.sin(theta),  np.cos(theta)]\n])\nS = np.array([\n    [2,0],\n    [0,1]\n])\n\n# Compose: apply R then S\nC = S @ R\nprint(\"Composite matrix:\\n\", C)\n\nComposite matrix:\n [[ 1.2246468e-16 -2.0000000e+00]\n [ 1.0000000e+00  6.1232340e-17]]\n\n\nApplying the composite matrix is equivalent to applying both maps in sequence.\n\nVerifying with a vector\n\n\nv = np.array([1,1])\nstep1 = R @ v\nstep2 = S @ step1\ncomposite = C @ v\n\nprint(\"Step-by-step:\", step2)\nprint(\"Composite:\", composite)\n\nStep-by-step: [-2.  1.]\nComposite: [-2.  1.]\n\n\nBoth results are the same → composition = matrix multiplication.\n\nPowers of a matrix\n\nRepeatedly applying a transformation corresponds to matrix powers.\nExample: scaling by 2.\n\nA = np.array([[2,0],[0,2]])\nv = np.array([1,1])\n\nprint(\"A @ v =\", A @ v)\nprint(\"A^2 @ v =\", np.linalg.matrix_power(A,2) @ v)\nprint(\"A^5 @ v =\", np.linalg.matrix_power(A,5) @ v)\n\nA @ v = [2 2]\nA^2 @ v = [4 4]\nA^5 @ v = [32 32]\n\n\nEach step doubles the scaling effect.\n\nIteration dynamics\n\nLet’s iterate a transformation many times and see what happens.\nExample:\n\\[\nA = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}\n\\]\n\nA = np.array([[0.5,0],[0,0.5]])\nv = np.array([4,4])\n\nfor i in range(5):\n    v = A @ v\n    print(f\"Step {i+1}:\", v)\n\nStep 1: [2. 2.]\nStep 2: [1. 1.]\nStep 3: [0.5 0.5]\nStep 4: [0.25 0.25]\nStep 5: [0.125 0.125]\n\n\nEach step shrinks the vector → iteration can reveal stability.\n\nRandom example\n\n\nnp.random.seed(0)\nM = np.random.randint(-2,3,(2,2))\nprint(\"Random matrix:\\n\", M)\n\nprint(\"M^2:\\n\", np.linalg.matrix_power(M,2))\nprint(\"M^3:\\n\", np.linalg.matrix_power(M,3))\n\nRandom matrix:\n [[ 2 -2]\n [ 1  1]]\nM^2:\n [[ 2 -6]\n [ 3 -1]]\nM^3:\n [[ -2 -10]\n [  5  -7]]\n\n\n\n\nTry It Yourself\n\nCreate two transformations: reflection across x-axis and scaling by 3. Compose them.\nTake a shear matrix and compute \\(A^5\\). What happens to a vector after repeated application?\nExperiment with a rotation matrix raised to higher powers. What cycle do you see?\n\n\n\nThe Takeaway\n\nComposition of linear maps = matrix multiplication.\nPowers of a matrix represent repeated application.\nIteration reveals long-term dynamics: shrinking, growing, or oscillating behavior.\n\n\n\n\n46. Similarity and Conjugation (Same Action, Different Basis)\nTwo matrices \\(A\\) and \\(B\\) are called similar if there exists an invertible matrix \\(P\\) such that\n\\[\nB = P^{-1} A P\n\\]\nThis means \\(A\\) and \\(B\\) represent the same linear transformation, but in different bases. This lab explores similarity and why it matters.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nExample with a change of basis\n\n\nA = Matrix([\n    [2,1],\n    [0,2]\n])\n\nP = Matrix([\n    [1,1],\n    [0,1]\n])\n\nB = P.inv() * A * P\nprint(\"Original A:\\n\", A)\nprint(\"Similar matrix B:\\n\", B)\n\nOriginal A:\n Matrix([[2, 1], [0, 2]])\nSimilar matrix B:\n Matrix([[2, 1], [0, 2]])\n\n\nHere, \\(A\\) and \\(B\\) are similar: they describe the same transformation in different coordinates.\n\nEigenvalues stay the same\n\nSimilarity preserves eigenvalues.\n\nprint(\"Eigenvalues of A:\", A.eigenvals())\nprint(\"Eigenvalues of B:\", B.eigenvals())\n\nEigenvalues of A: {2: 2}\nEigenvalues of B: {2: 2}\n\n\nBoth matrices have the same eigenvalues, even though their entries differ.\n\nSimilarity and diagonalization\n\nIf a matrix is diagonalizable, there exists \\(P\\) such that\n\\[\nD = P^{-1} A P\n\\]\nwhere \\(D\\) is diagonal.\n\nC = Matrix([\n    [4,1],\n    [0,2]\n])\n\nP, D = C.diagonalize()\nprint(\"Diagonal form D:\\n\", D)\nprint(\"Check similarity (P^-1 C P = D):\\n\", P.inv()*C*P)\n\nDiagonal form D:\n Matrix([[2, 0], [0, 4]])\nCheck similarity (P^-1 C P = D):\n Matrix([[2, 0], [0, 4]])\n\n\nDiagonalization is a special case of similarity, where the new matrix is as simple as possible.\n\nNumPy version\n\n\nA = np.array([[2,1],[0,2]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A)\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors (basis P):\\n\", eigvecs)\n\nEigenvalues: [2. 2.]\nEigenvectors (basis P):\n [[ 1.0000000e+00 -1.0000000e+00]\n [ 0.0000000e+00  4.4408921e-16]]\n\n\nHere, eigenvectors form the change-of-basis matrix \\(P\\).\n\nGeometric interpretation\n\n\nSimilar matrices = same transformation, different “ruler” (basis).\nDiagonalization = finding a ruler that makes the transformation look like pure stretching along axes.\n\n\n\nTry It Yourself\n\nTake\n\\[\nA = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\n\\]\nand find a matrix \\(P\\) that gives a similar \\(B\\).\nShow that two similar matrices have the same determinant and trace.\nFor a random 3×3 matrix, check if it is diagonalizable using SymPy’s .diagonalize() method.\n\n\n\nThe Takeaway\n\nSimilarity = same linear map, different basis.\nSimilar matrices share eigenvalues, determinant, and trace.\nDiagonalization is the simplest similarity form, making repeated computations (like powers) much easier.\n\n\n\n\n47. Projections and Reflections (Idempotent and Involutive Maps)\nTwo very common geometric linear maps are projections and reflections. They show up in graphics, physics, and optimization.\n\nA projection squashes vectors onto a subspace (like dropping a shadow).\nA reflection flips vectors across a line or plane (like a mirror).\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nProjection onto a line\n\nIf we want to project onto the line spanned by \\(u\\), the projection matrix is:\n\\[\nP = \\frac{uu^T}{u^T u}\n\\]\n\nu = np.array([2,1], dtype=float)\nu = u / np.linalg.norm(u)   # normalize\nP = np.outer(u,u)\n\nprint(\"Projection matrix:\\n\", P)\n\nProjection matrix:\n [[0.8 0.4]\n [0.4 0.2]]\n\n\nApply projection:\n\nv = np.array([3,4], dtype=float)\nproj_v = P @ v\nprint(\"Original v:\", v)\nprint(\"Projection of v onto u:\", proj_v)\n\nOriginal v: [3. 4.]\nProjection of v onto u: [4. 2.]\n\n\n\nVisualization of projection\n\n\nplt.arrow(0,0,v[0],v[1],head_width=0.1,color=\"blue\",length_includes_head=True)\nplt.arrow(0,0,proj_v[0],proj_v[1],head_width=0.1,color=\"red\",length_includes_head=True)\nplt.arrow(proj_v[0],proj_v[1],v[0]-proj_v[0],v[1]-proj_v[1],head_width=0.1,color=\"gray\",linestyle=\"dashed\")\n\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.title(\"Blue = original, Red = projection, Gray = error vector\")\nplt.show()\n\n\n\n\n\n\n\n\nThe projection is the closest point on the line to the original vector.\n\nReflection across a line\n\nThe reflection matrix across the line spanned by \\(u\\) is:\n\\[\nR = 2P - I\n\\]\n\nI = np.eye(2)\nR = 2*P - I\n\nreflect_v = R @ v\nprint(\"Reflection of v across line u:\", reflect_v)\n\nReflection of v across line u: [ 5.00000000e+00 -5.55111512e-16]\n\n\n\nChecking algebraic properties\n\n\nProjection: \\(P^2 = P\\) (idempotent).\nReflection: \\(R^2 = I\\) (involutive).\n\n\nprint(\"P^2 =\\n\", P @ P)\nprint(\"R^2 =\\n\", R @ R)\n\nP^2 =\n [[0.8 0.4]\n [0.4 0.2]]\nR^2 =\n [[ 1.00000000e+00 -1.59872116e-16]\n [-1.59872116e-16  1.00000000e+00]]\n\n\n\nProjection in higher dimensions\n\nProject onto the plane spanned by two vectors in \\(\\mathbb{R}^3\\).\n\nu1 = np.array([1,0,0], dtype=float)\nu2 = np.array([0,1,0], dtype=float)\n\nU = np.column_stack((u1,u2))   # basis for plane\nP_plane = U @ np.linalg.inv(U.T @ U) @ U.T\n\nv = np.array([1,2,3], dtype=float)\nproj_plane = P_plane @ v\nprint(\"Projection onto xy-plane:\", proj_plane)\n\nProjection onto xy-plane: [1. 2. 0.]\n\n\n\n\nTry It Yourself\n\nProject \\([4,5]\\) onto the x-axis and verify the result.\nReflect \\([1,2]\\) across the line \\(y=x\\).\nCreate a random 3D vector and project it onto the plane spanned by \\([1,1,0]\\) and \\([0,1,1]\\).\n\n\n\nThe Takeaway\n\nProjection: idempotent (\\(P^2 = P\\)), finds the closest vector in a subspace.\nReflection: involutive (\\(R^2 = I\\)), flips across a line/plane but preserves lengths.\nBoth are simple but powerful examples of linear transformations with clear geometry.\n\n\n\n\n48. Rotations and Shear (Geometric Intuition)\nTwo transformations often used in geometry, graphics, and physics are rotations and shears. Both are linear maps, but they behave differently:\n\nRotation preserves lengths and angles.\nShear preserves area (in 2D) but distorts shapes, turning squares into parallelograms.\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nRotation in 2D\n\nThe rotation matrix by angle \\(\\theta\\) is:\n\\[\nR(\\theta) = \\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix}\n\\]\n\ndef rotation_matrix(theta):\n    return np.array([\n        [np.cos(theta), -np.sin(theta)],\n        [np.sin(theta),  np.cos(theta)]\n    ])\n\ntheta = np.pi/4   # 45 degrees\nR = rotation_matrix(theta)\n\nv = np.array([2,1])\nrotated_v = R @ v\nprint(\"Original v:\", v)\nprint(\"Rotated v (45°):\", rotated_v)\n\nOriginal v: [2 1]\nRotated v (45°): [0.70710678 2.12132034]\n\n\n\nVisualizing rotation\n\n\nplt.arrow(0,0,v[0],v[1],head_width=0.1,color=\"blue\",length_includes_head=True)\nplt.arrow(0,0,rotated_v[0],rotated_v[1],head_width=0.1,color=\"red\",length_includes_head=True)\n\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.title(\"Blue = original, Red = rotated (45°)\")\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\nThe vector rotates counterclockwise by 45°.\n\nShear in 2D\n\nA shear along the x-axis by factor \\(k\\):\n\\[\nS = \\begin{bmatrix}\n1 & k \\\\\n0 & 1\n\\end{bmatrix}\n\\]\n\nk = 1.0\nS = np.array([\n    [1,k],\n    [0,1]\n])\n\nsheared_v = S @ v\nprint(\"Sheared v:\", sheared_v)\n\nSheared v: [3. 1.]\n\n\n\nVisualizing shear\n\n\nplt.arrow(0,0,v[0],v[1],head_width=0.1,color=\"blue\",length_includes_head=True)\nplt.arrow(0,0,sheared_v[0],sheared_v[1],head_width=0.1,color=\"green\",length_includes_head=True)\n\nplt.axhline(0,color='black',linewidth=0.5)\nplt.axvline(0,color='black',linewidth=0.5)\nplt.grid()\nplt.title(\"Blue = original, Green = sheared\")\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\nThe shear moves the vector sideways, distorting its angle.\n\nProperties check\n\n\nRotation preserves length:\n\n\nprint(\"||v|| =\", np.linalg.norm(v))\nprint(\"||R v|| =\", np.linalg.norm(rotated_v))\n\n||v|| = 2.23606797749979\n||R v|| = 2.2360679774997894\n\n\n\nShear preserves area (determinant = 1):\n\n\nprint(\"det(S) =\", np.linalg.det(S))\n\ndet(S) = 1.0\n\n\n\n\nTry It Yourself\n\nRotate \\([1,0]\\) by 90° and check it becomes \\([0,1]\\).\nApply shear with \\(k=2\\) to a square (points \\((0,0),(1,0),(1,1),(0,1)\\)) and plot before/after.\nCombine rotation and shear: apply shear first, then rotation. What happens?\n\n\n\nThe Takeaway\n\nRotation: length- and angle-preserving, determinant = 1.\nShear: shape-distorting but area-preserving, determinant = 1.\nBoth are linear maps that provide geometric intuition and real-world modeling tools.\n\n\n\n\n49. Rank and Operator Viewpoint (Rank Beyond Elimination)\nThe rank of a matrix tells us how much “information” a linear map carries. Algebraically, it is the dimension of the image (column space). Geometrically, it measures how many independent directions survive the transformation.\nFrom the operator viewpoint:\n\nA matrix \\(A\\) is not just a table of numbers - it is a linear operator that maps vectors to other vectors.\nThe rank is the dimension of the output space that \\(A\\) actually reaches.\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nRank via elimination (SymPy)\n\n\nA = Matrix([\n    [1,2,3],\n    [2,4,6],\n    [1,1,1]\n])\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Rank of A:\", A.rank())\n\nMatrix A:\n Matrix([[1, 2, 3], [2, 4, 6], [1, 1, 1]])\nRank of A: 2\n\n\nHere, the second row is a multiple of the first → less independence → rank &lt; 3.\n\nRank via NumPy\n\n\nA_np = np.array([[1,2,3],[2,4,6],[1,1,1]], dtype=float)\nprint(\"Rank (NumPy):\", np.linalg.matrix_rank(A_np))\n\nRank (NumPy): 2\n\n\n\nOperator viewpoint\n\nLet’s apply \\(A\\) to random vectors:\n\nfor v in [np.array([1,0,0]), np.array([0,1,0]), np.array([0,0,1])]:\n    print(\"A @\", v, \"=\", A_np @ v)\n\nA @ [1 0 0] = [1. 2. 1.]\nA @ [0 1 0] = [2. 4. 1.]\nA @ [0 0 1] = [3. 6. 1.]\n\n\nEven though we started in 3D, all outputs lie in a plane in \\(\\mathbb{R}^3\\). That’s why rank = 2.\n\nFull rank vs reduced rank\n\n\nFull rank: the transformation preserves dimension (no collapse).\nReduced rank: the transformation collapses onto a lower-dimensional subspace.\n\nExample full-rank:\n\nB = Matrix([\n    [1,0,0],\n    [0,1,0],\n    [0,0,1]\n])\n\nprint(\"Rank of B:\", B.rank())\n\nRank of B: 3\n\n\n\nConnection to nullity\n\nThe rank-nullity theorem:\n\\[\n\\text{rank}(A) + \\text{nullity}(A) = \\text{number of columns of } A\n\\]\nCheck with SymPy:\n\nprint(\"Null space (basis):\", A.nullspace())\nprint(\"Nullity:\", len(A.nullspace()))\nprint(\"Rank + Nullity =\", A.rank() + len(A.nullspace()))\n\nNull space (basis): [Matrix([\n[ 1],\n[-2],\n[ 1]])]\nNullity: 1\nRank + Nullity = 3\n\n\n\n\nTry It Yourself\n\nTake\n\\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n\\]\nand compute its rank. Why is it 1?\nFor a random 4×4 matrix, use np.linalg.matrix_rank to check if it’s invertible.\nVerify rank-nullity theorem for a 3×5 random integer matrix.\n\n\n\nThe Takeaway\n\nRank = dimension of the image (how many independent outputs a transformation has).\nOperator viewpoint: rank shows how much of the input space survives after transformation.\nRank-nullity links the image and kernel - together they fully describe a linear operator.\n\n\n\n\n50. Block Matrices and Block Maps (Divide and Conquer Structure)\nSometimes matrices can be arranged in blocks (submatrices). Treating a big matrix as smaller pieces helps simplify calculations, especially in systems with structure (networks, coupled equations, or partitioned variables).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nConstructing block matrices\n\nWe can build a block matrix from smaller pieces:\n\nA11 = Matrix([[1,2],[3,4]])\nA12 = Matrix([[5,6],[7,8]])\nA21 = Matrix([[9,10]])\nA22 = Matrix([[11,12]])\n\n# Combine into a block matrix\nA = Matrix.vstack(\n    Matrix.hstack(A11, A12),\n    Matrix.hstack(A21, A22)\n)\nprint(\"Block matrix A:\\n\", A)\n\nBlock matrix A:\n Matrix([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 11, 12]])\n\n\n\nBlock multiplication\n\nIf a matrix is partitioned into blocks, multiplication follows block rules:\n\\[\n\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n= \\begin{bmatrix} Ax + By \\\\ Cx + Dy \\end{bmatrix}\n\\]\nExample:\n\nA = Matrix([\n    [1,2,5,6],\n    [3,4,7,8],\n    [9,10,11,12]\n])\n\nx = Matrix([1,1,2,2])\nprint(\"A * x =\", A*x)\n\nA * x = Matrix([[25], [37], [65]])\n\n\nHere the vector is split into blocks \\([x,y]\\).\n\nBlock diagonal matrices\n\nBlock diagonal = independent subproblems:\n\nB1 = Matrix([[2,0],[0,2]])\nB2 = Matrix([[3,1],[0,3]])\n\nBlockDiag = Matrix([\n    [2,0,0,0],\n    [0,2,0,0],\n    [0,0,3,1],\n    [0,0,0,3]\n])\n\nprint(\"Block diagonal matrix:\\n\", BlockDiag)\n\nBlock diagonal matrix:\n Matrix([[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 1], [0, 0, 0, 3]])\n\n\nApplying this matrix acts separately on each block - like running two smaller transformations in parallel.\n\nInverse of block diagonal\n\nThe inverse of a block diagonal is just the block diagonal of inverses:\n\nB1_inv = B1.inv()\nB2_inv = B2.inv()\nBlockDiagInv = Matrix([\n    [B1_inv[0,0],0,0,0],\n    [0,B1_inv[1,1],0,0],\n    [0,0,B2_inv[0,0],B2_inv[0,1]],\n    [0,0,B2_inv[1,0],B2_inv[1,1]]\n])\nprint(\"Inverse block diag:\\n\", BlockDiagInv)\n\nInverse block diag:\n Matrix([[1/2, 0, 0, 0], [0, 1/2, 0, 0], [0, 0, 1/3, -1/9], [0, 0, 0, 1/3]])\n\n\n\nPractical example - coupled equations\n\nSuppose we have two independent systems:\n\nSystem 1: \\(Ax = b\\)\nSystem 2: \\(Cy = d\\)\n\nWe can represent both together:\n\\[\n\\begin{bmatrix} A & 0 \\\\ 0 & C \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n= \\begin{bmatrix} b \\\\ d \\end{bmatrix}\n\\]\nThis shows how block matrices organize multiple systems in one big equation.\n\n\nTry It Yourself\n\nBuild a block diagonal matrix with three 2×2 blocks. Apply it to a vector.\nVerify block multiplication rule by manually computing \\(Ax + By\\) and \\(Cx + Dy\\).\nWrite two small systems of equations and combine them into one block system.\n\n\n\nThe Takeaway\n\nBlock matrices let us break down big systems into smaller parts.\nBlock diagonal matrices = independent subsystems.\nThinking in blocks simplifies algebra, programming, and numerical computation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-6.-determinants-and-volume",
    "href": "books/en-US/lab.html#chapter-6.-determinants-and-volume",
    "title": "The LAB",
    "section": "Chapter 6. Determinants and volume",
    "text": "Chapter 6. Determinants and volume\n\n51. Areas, Volumes, and Signed Scale Factors (Geometric Entry Point)\nThe determinant of a matrix has a deep geometric meaning: it tells us how a linear transformation scales area (in 2D), volume (in 3D), or higher-dimensional content. It can also flip orientation (sign).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nDeterminant in 2D (area scaling)\n\nLet’s take a matrix that stretches and shears:\n\nA = Matrix([\n    [2,1],\n    [1,1]\n])\n\nprint(\"Determinant:\", A.det())\n\nDeterminant: 1\n\n\nThe determinant = 1 → areas are preserved, even though the shape is distorted.\n\nUnit square under transformation\n\nTransform the square with corners \\((0,0),(1,0),(1,1),(0,1)\\):\n\nsquare = Matrix([\n    [0,0],\n    [1,0],\n    [1,1],\n    [0,1]\n])\n\ntransformed = (A * square.T).T\nprint(\"Original square:\\n\", square)\nprint(\"Transformed square:\\n\", transformed)\n\nOriginal square:\n Matrix([[0, 0], [1, 0], [1, 1], [0, 1]])\nTransformed square:\n Matrix([[0, 0], [2, 1], [3, 2], [1, 1]])\n\n\nThe area of the transformed shape equals \\(|\\det(A)|\\).\n\nDeterminant in 3D (volume scaling)\n\n\nB = Matrix([\n    [1,2,0],\n    [0,1,0],\n    [0,0,3]\n])\n\nprint(\"Determinant:\", B.det())\n\nDeterminant: 3\n\n\n\\(\\det(B)=3\\) means that volumes are scaled by 3.\n\nNegative determinant = orientation flip\n\n\nC = Matrix([\n    [0,1],\n    [1,0]\n])\n\nprint(\"Determinant:\", C.det())\n\nDeterminant: -1\n\n\nThe determinant = -1 → area preserved but orientation flipped (like a mirror reflection).\n\nNumPy version\n\n\nA = np.array([[2,1],[1,1]], dtype=float)\nprint(\"Det (NumPy):\", np.linalg.det(A))\n\nDet (NumPy): 1.0\n\n\n\n\nTry It Yourself\n\nTake\n\\[\n\\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nand compute the determinant. Verify it scales areas by 6.\nBuild a 3×3 shear matrix and check how it affects volume.\nTest a reflection matrix and confirm that the determinant is negative.\n\n\n\nThe Takeaway\n\nDeterminant measures how a linear map scales area, volume, or hypervolume.\nPositive determinant = preserves orientation; negative = flips it.\nMagnitude of determinant = scaling factor of geometric content.\n\n\n\n\n52. Determinant via Linear Rules (Multilinearity, Sign, Normalization)\nThe determinant isn’t just a formula; it’s defined by three elegant rules that make it unique. These rules capture its geometric meaning as a volume-scaling factor.\n\nMultilinearity: Linear in each row (or column).\nSign Change: Swapping two rows flips the sign.\nNormalization: The determinant of the identity matrix is 1.\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nMultilinearity\n\nIf one row is scaled, the determinant scales the same way.\n\nA = Matrix([[1,2],[3,4]])\nprint(\"det(A):\", A.det())\n\nB = Matrix([[2,4],[3,4]])  # first row doubled\nprint(\"det(B):\", B.det())\n\ndet(A): -2\ndet(B): -4\n\n\nYou’ll see det(B) = 2 * det(A).\n\nSign change by row swap\n\n\nC = Matrix([[1,2],[3,4]])\nC_swapped = Matrix([[3,4],[1,2]])\n\nprint(\"det(C):\", C.det())\nprint(\"det(C_swapped):\", C_swapped.det())\n\ndet(C): -2\ndet(C_swapped): 2\n\n\nSwapping rows flips the sign of the determinant.\n\nNormalization rule\n\n\nI = Matrix.eye(3)\nprint(\"det(I):\", I.det())\n\ndet(I): 1\n\n\nThe determinant of the identity is always 1 - this fixes the scaling baseline.\n\nCombining rules (example in 3×3)\n\n\nM = Matrix([[1,2,3],[4,5,6],[7,8,9]])\nprint(\"det(M):\", M.det())\n\ndet(M): 0\n\n\nHere, rows are linearly dependent, so the determinant is 0 - consistent with multilinearity (since one row can be written as a combo of others).\n\nNumPy check\n\n\nA = np.array([[1,2],[3,4]], dtype=float)\nprint(\"det(A) NumPy:\", np.linalg.det(A))\n\ndet(A) NumPy: -2.0000000000000004\n\n\nBoth SymPy and NumPy confirm the same result.\n\n\nTry It Yourself\n\nScale a row of a 3×3 matrix by 3. Confirm the determinant scales by 3.\nSwap two rows twice in a row - does the determinant return to its original value?\nCompute determinant of a triangular matrix. What pattern do you see?\n\n\n\nThe Takeaway\n\nDeterminant is defined by multilinearity, sign change, and normalization.\nThese rules uniquely pin down the determinant’s behavior.\nEvery formula (cofactor expansion, row-reduction method, etc.) comes from these core principles.\n\n\n\n\n53. Determinant and Row Operations (How Each Move Changes det)\nRow operations are at the heart of Gaussian elimination, and the determinant has simple, predictable reactions to them. Understanding these reactions gives both computational shortcuts and geometric intuition.\n\nThe Three Key Rules\n\nRow swap: Swapping two rows flips the sign of the determinant.\nRow scaling: Multiplying a row by a scalar \\(c\\) multiplies the determinant by \\(c\\).\nRow replacement: Adding a multiple of one row to another leaves the determinant unchanged.\n\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nRow swap\n\n\nA = Matrix([[1,2],[3,4]])\nprint(\"det(A):\", A.det())\n\nA_swapped = Matrix([[3,4],[1,2]])\nprint(\"det(after swap):\", A_swapped.det())\n\ndet(A): -2\ndet(after swap): 2\n\n\nThe result flips sign.\n\nRow scaling\n\n\nB = Matrix([[1,2],[3,4]])\nB_scaled = Matrix([[2,4],[3,4]])  # first row × 2\n\nprint(\"det(B):\", B.det())\nprint(\"det(after scaling row 1 by 2):\", B_scaled.det())\n\ndet(B): -2\ndet(after scaling row 1 by 2): -4\n\n\nDeterminant is multiplied by 2.\n\nRow replacement (no change)\n\n\nC = Matrix([[1,2],[3,4]])\nC_replaced = Matrix([[1,2],[3-2*1, 4-2*2]])  # row2 → row2 - 2*row1\n\nprint(\"det(C):\", C.det())\nprint(\"det(after row replacement):\", C_replaced.det())\n\ndet(C): -2\ndet(after row replacement): -2\n\n\nDeterminant stays the same.\n\nTriangular form shortcut\n\nSince elimination only uses row replacement (which doesn’t change the determinant) and row swaps/scales (which we can track), the determinant of a triangular matrix is just the product of its diagonal entries.\n\nD = Matrix([[2,1,3],[0,4,5],[0,0,6]])\nprint(\"det(D):\", D.det())\nprint(\"Product of diagonals:\", 2*4*6)\n\ndet(D): 48\nProduct of diagonals: 48\n\n\n\nNumPy confirmation\n\n\nA = np.array([[1,2,3],[0,4,5],[1,0,6]], dtype=float)\nprint(\"det(A):\", np.linalg.det(A))\n\ndet(A): 22.000000000000004\n\n\n\n\nTry It Yourself\n\nTake\n\\[\n\\begin{bmatrix} 2 & 3 \\\\ 4 & 6 \\end{bmatrix}\n\\]\nand scale the second row by \\(\\tfrac{1}{2}\\). Compare determinants before and after.\nDo Gaussian elimination on a 3×3 matrix, and track how each row operation changes the determinant.\nCompute determinant by reducing to triangular form and compare with SymPy’s .det().\n\n\n\nThe Takeaway\n\nDeterminant reacts predictably to row operations.\nRow replacement is “safe” (no change), scaling multiplies by the factor, and swapping flips the sign.\nThis makes elimination not just a solving tool, but also a method to compute determinants efficiently.\n\n\n\n\n54. Triangular Matrices and Product of Diagonals (Fast Wins)\nFor triangular matrices (upper or lower), the determinant is simply the product of the diagonal entries. This rule is one of the biggest shortcuts in linear algebra - no expansion or elimination needed.\n\nWhy It Works\n\nTriangular matrices already look like the end result of Gaussian elimination.\nSince row replacement operations don’t change the determinant, what’s left is just the product of the diagonal.\n\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nUpper triangular example\n\n\nA = Matrix([\n    [2,1,3],\n    [0,4,5],\n    [0,0,6]\n])\n\nprint(\"det(A):\", A.det())\nprint(\"Product of diagonals:\", 2*4*6)\n\ndet(A): 48\nProduct of diagonals: 48\n\n\nBoth values match exactly.\n\nLower triangular example\n\n\nB = Matrix([\n    [7,0,0],\n    [2,5,0],\n    [3,4,9]\n])\n\nprint(\"det(B):\", B.det())\nprint(\"Product of diagonals:\", 7*5*9)\n\ndet(B): 315\nProduct of diagonals: 315\n\n\n\nDiagonal matrix (special case)\n\nFor diagonal matrices, determinant = product of diagonal entries directly.\n\nC = Matrix.diag(3,5,7)\nprint(\"det(C):\", C.det())\nprint(\"Product of diagonals:\", 3*5*7)\n\ndet(C): 105\nProduct of diagonals: 105\n\n\n\nNumPy version\n\n\nA = np.array([[2,1,3],[0,4,5],[0,0,6]], dtype=float)\nprint(\"det(A):\", np.linalg.det(A))\nprint(\"Product of diagonals:\", np.prod(np.diag(A)))\n\ndet(A): 47.999999999999986\nProduct of diagonals: 48.0\n\n\n\nQuick elimination to triangular form\n\nEven for non-triangular matrices, elimination reduces them to triangular form, where this rule applies.\n\nD = Matrix([[1,2,3],[4,5,6],[7,8,10]])\nprint(\"det(D) via SymPy:\", D.det())\nprint(\"det(D) via LU decomposition:\", D.LUdecomposition()[0].det() * D.LUdecomposition()[1].det())\n\ndet(D) via SymPy: -3\ndet(D) via LU decomposition: -3\n\n\n\n\nTry It Yourself\n\nCompute the determinant of a 4×4 diagonal matrix quickly.\nVerify that triangular matrices with a zero on the diagonal always have determinant 0.\nUse SymPy to check that elimination to triangular form preserves determinant (except for swaps/scales).\n\n\n\nThe Takeaway\n\nFor triangular (and diagonal) matrices:\n\\[\n\\det(A) = \\prod_{i} a_{ii}\n\\]\nThis shortcut makes determinant computation trivial.\nGaussian elimination leverages this fact: once reduced to triangular form, the determinant is just the product of pivots (with sign adjustments for swaps).\n\n\n\n\n55. det(AB) = det(A)det(B) (Multiplicative Magic)\nOne of the most elegant properties of determinants is multiplicativity:\n\\[\n\\det(AB) = \\det(A)\\,\\det(B)\n\\]\nThis rule is powerful because it connects algebra (matrix multiplication) with geometry (volume scaling).\n\nGeometric Intuition\n\nIf \\(A\\) scales volumes by factor \\(\\det(A)\\), and \\(B\\) scales them by \\(\\det(B)\\), then applying \\(B\\) followed by \\(A\\) scales volumes by \\(\\det(A)\\det(B)\\).\nThis property works in all dimensions.\n\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\n2×2 example\n\n\nA = Matrix([[2,1],[0,3]])\nB = Matrix([[1,4],[2,5]])\n\ndetA = A.det()\ndetB = B.det()\ndetAB = (A*B).det()\n\nprint(\"det(A):\", detA)\nprint(\"det(B):\", detB)\nprint(\"det(AB):\", detAB)\nprint(\"det(A)*det(B):\", detA*detB)\n\ndet(A): 6\ndet(B): -3\ndet(AB): -18\ndet(A)*det(B): -18\n\n\nThe two results match.\n\n3×3 random matrix check\n\n\nnp.random.seed(1)\nA = Matrix(np.random.randint(-3,4,(3,3)))\nB = Matrix(np.random.randint(-3,4,(3,3)))\n\nprint(\"det(A):\", A.det())\nprint(\"det(B):\", B.det())\nprint(\"det(AB):\", (A*B).det())\nprint(\"det(A)*det(B):\", A.det()*B.det())\n\ndet(A): 25\ndet(B): -15\ndet(AB): -375\ndet(A)*det(B): -375\n\n\n\nSpecial cases\n\n\nIf \\(\\det(A)=0\\), then \\(\\det(AB)=0\\).\nIf \\(\\det(A)=\\pm1\\), it acts like a “volume-preserving” transformation (rotation/reflection).\n\n\nA = Matrix([[1,0],[0,0]])  # singular\nB = Matrix([[2,3],[4,5]])\n\nprint(\"det(A):\", A.det())\nprint(\"det(AB):\", (A*B).det())\n\ndet(A): 0\ndet(AB): 0\n\n\nBoth are 0.\n\nNumPy version\n\n\nA = np.array([[2,1],[0,3]], dtype=float)\nB = np.array([[1,4],[2,5]], dtype=float)\n\nlhs = np.linalg.det(A @ B)\nrhs = np.linalg.det(A) * np.linalg.det(B)\n\nprint(\"det(AB) =\", lhs)\nprint(\"det(A)*det(B) =\", rhs)\n\ndet(AB) = -17.999999999999996\ndet(A)*det(B) = -17.999999999999996\n\n\n\n\nTry It Yourself\n\nConstruct two triangular matrices and verify multiplicativity (diagonal products multiply too).\nTest the property with an orthogonal matrix \\(Q\\) (\\(\\det(Q)=\\pm 1\\)). What happens?\nTry with one matrix singular - confirm the product is always singular.\n\n\n\nThe Takeaway\n\nDeterminant is multiplicative, not additive.\n\\(\\det(AB) = \\det(A)\\det(B)\\) is a cornerstone identity in linear algebra.\nThis property connects geometry (volume scaling) with algebra (matrix multiplication).\n\n\n\n\n56. Invertibility and Zero Determinant (Flat vs. Full Volume)\nThe determinant gives a quick test for invertibility:\n\nIf \\(\\det(A) \\neq 0\\), the matrix is invertible.\nIf \\(\\det(A) = 0\\), the matrix is singular (non-invertible).\n\nGeometrically:\n\nNonzero determinant → transformation keeps full dimension (no collapse).\nZero determinant → transformation flattens space into a lower dimension (volume = 0).\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\nfrom sympy.matrices.common import NonInvertibleMatrixError\n\n\n\nStep-by-Step Code Walkthrough\n\nInvertible example\n\n\nA = Matrix([[2,1],[5,3]])\nprint(\"det(A):\", A.det())\nprint(\"Inverse exists?\", A.det() != 0)\nprint(\"A inverse:\\n\", A.inv())\n\ndet(A): 1\nInverse exists? True\nA inverse:\n Matrix([[3, -1], [-5, 2]])\n\n\nThe determinant is nonzero → invertible.\n\nSingular example (zero determinant)\n\n\nB = Matrix([[1,2],[2,4]])\nprint(\"det(B):\", B.det())\nprint(\"Inverse exists?\", B.det() != 0)\n\ndet(B): 0\nInverse exists? False\n\n\nSince the second row is a multiple of the first, determinant = 0 → no inverse.\n\nSolving systems with determinant check\n\nIf \\(\\det(A)=0\\), the system \\(Ax=b\\) may have no solutions or infinitely many.\n\n# 3. Solving systems with determinant check\nb = Matrix([1,2])\ntry:\n    print(\"Solve Ax=b with singular B:\", B.solve(b))\nexcept NonInvertibleMatrixError as e:\n    print(\"Error when solving Ax=b:\", e)\n\nError when solving Ax=b: Matrix det == 0; not invertible.\n\n\nSymPy indicates inconsistency or multiple solutions.\n\nHigher-dimensional example\n\n\nC = Matrix([\n    [1,0,0],\n    [0,2,0],\n    [0,0,3]\n])\nprint(\"det(C):\", C.det())\nprint(\"Invertible?\", C.det() != 0)\n\ndet(C): 6\nInvertible? True\n\n\nDiagonal entries all nonzero → invertible.\n\nNumPy version\n\n\nA = np.array([[2,1],[5,3]], dtype=float)\nprint(\"det(A):\", np.linalg.det(A))\nprint(\"Inverse:\\n\", np.linalg.inv(A))\n\nB = np.array([[1,2],[2,4]], dtype=float)\nprint(\"det(B):\", np.linalg.det(B))\n# np.linalg.inv(B) would fail because det=0\n\ndet(A): 1.0000000000000009\nInverse:\n [[ 3. -1.]\n [-5.  2.]]\ndet(B): 0.0\n\n\n\n\nTry It Yourself\n\nBuild a 3×3 matrix with determinant 0 by making one row a multiple of another. Confirm singularity.\nGenerate a random 4×4 matrix and check whether it’s invertible using .det().\nTest if two different 2×2 matrices are invertible, then multiply them together - is the product invertible too?\n\n\n\nThe Takeaway\n\n\\(\\det(A) \\neq 0 \\implies\\) invertible (full volume).\n\\(\\det(A) = 0 \\implies\\) singular (space collapsed).\nDeterminant gives both algebraic and geometric insight into when a matrix is reversible.\n\n\n\n\n57. Cofactor Expansion (Laplace’s Method)\nThe cofactor expansion is a systematic way to compute determinants using minors. It’s not efficient for large matrices, but it reveals the recursive structure of determinants.\n\nDefinition\nFor an \\(n \\times n\\) matrix \\(A\\),\n\\[\n\\det(A) = \\sum_{j=1}^{n} (-1)^{i+j} a_{ij} \\det(M_{ij})\n\\]\nwhere:\n\n\\(i\\) = chosen row (or column),\n\\(M_{ij}\\) = minor matrix after removing row \\(i\\), column \\(j\\).\n\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix, symbols\n\n\n\nStep-by-Step Code Walkthrough\n\n2×2 case (base rule)\n\n\n# declare symbols\na, b, c, d = symbols('a b c d')\n\n# build the matrix\nA = Matrix([[a, b],[c, d]])\n\n# compute determinant\ndetA = A.det()\nprint(\"Determinant 2x2:\", detA)\n\nDeterminant 2x2: a*d - b*c\n\n\nFormula: \\(\\det(A) = ad - bc\\).\n\n3×3 example using cofactor expansion\n\n\nA = Matrix([\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n])\n\ndetA = A.det()\nprint(\"Determinant via SymPy:\", detA)\n\nDeterminant via SymPy: 0\n\n\nLet’s compute manually along the first row:\n\ncofactor_expansion = (\n    1 * Matrix([[5,6],[8,9]]).det()\n    - 2 * Matrix([[4,6],[7,9]]).det()\n    + 3 * Matrix([[4,5],[7,8]]).det()\n)\nprint(\"Cofactor expansion result:\", cofactor_expansion)\n\nCofactor expansion result: 0\n\n\nBoth match (here = 0 because rows are dependent).\n\nExpansion along different rows/columns\n\nThe result is the same no matter which row/column you expand along.\n\ncofactor_col1 = (\n    1 * Matrix([[2,3],[8,9]]).det()\n    - 4 * Matrix([[2,3],[5,6]]).det()\n    + 7 * Matrix([[2,3],[5,6]]).det()\n)\nprint(\"Expansion along col1:\", cofactor_col1)\n\nExpansion along col1: -15\n\n\n\nLarger example (4×4)\n\n\nB = Matrix([\n    [2,0,1,3],\n    [1,2,0,4],\n    [0,1,1,0],\n    [3,0,2,1]\n])\n\nprint(\"Determinant 4x4:\", B.det())\n\nDeterminant 4x4: -15\n\n\nSymPy handles it directly, but conceptually it’s still the same recursive expansion.\n\nNumPy vs SymPy\n\n\nB_np = np.array([[2,0,1,3],[1,2,0,4],[0,1,1,0],[3,0,2,1]], dtype=float)\nprint(\"NumPy determinant:\", np.linalg.det(B_np))\n\nNumPy determinant: -15.0\n\n\n\n\nTry It Yourself\n\nCompute a 3×3 determinant manually using cofactor expansion and confirm with .det().\nExpand along a different row and check that the result is unchanged.\nBuild a 4×4 diagonal matrix and expand it - what simplification do you notice?\n\n\n\nThe Takeaway\n\nCofactor expansion defines determinant recursively.\nWorks on any row or column, with consistent results.\nImportant for proofs and theory, though not practical for computation on large matrices.\n\n\n\n\n58. Permutations and Sign (The Combinatorial Core)\nThe determinant can also be defined using permutations of indices. This looks abstract, but it’s the most fundamental definition:\n\\[\n\\det(A) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{i,\\sigma(i)}\n\\]\n\n\\(S_n\\) = set of all permutations of \\(\\{1,\\dots,n\\}\\)\n\\(\\text{sgn}(\\sigma)\\) = +1 if the permutation is even, -1 if odd\nEach term = one product of entries, one from each row and column\n\nThis formula explains why determinants mix signs, why row swaps flip the determinant, and why dependence kills it.\n\nSet Up Your Lab\n\nimport itertools\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nDeterminant by permutation expansion (3×3)\n\n\ndef determinant_permutation(A):\n    n = A.shape[0]\n    total = 0\n    for perm in itertools.permutations(range(n)):\n        sign = (-1)**(sum(1 for i in range(n) for j in range(i) if perm[j] &gt; perm[i]))\n        product = 1\n        for i in range(n):\n            product *= A[i, perm[i]]\n        total += sign * product\n    return total\n\nA = np.array([[1,2,3],\n              [4,5,6],\n              [7,8,9]])\n\nprint(\"Permutation formula det:\", determinant_permutation(A))\nprint(\"NumPy det:\", np.linalg.det(A))\n\nPermutation formula det: 0\nNumPy det: 0.0\n\n\nBoth results ≈ 0, since rows are dependent.\n\nCount permutations\n\nFor \\(n=3\\), there are \\(3! = 6\\) terms:\n\\[\n\\det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}  \n- a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}\n\\]\nYou can see the alternating signs explicitly.\n\nVerification with SymPy\n\n\nM = Matrix([[2,1,0],\n            [1,3,4],\n            [0,2,5]])\nprint(\"SymPy det:\", M.det())\n\nSymPy det: 9\n\n\nMatches the permutation expansion.\n\nGrowth of terms\n\n\n2×2 → 2 terms\n3×3 → 6 terms\n4×4 → 24 terms\n\\(n\\) → \\(n!\\) terms (factorial growth!)\n\nThis is why cofactor or LU is preferred computationally.\n\n\nTry It Yourself\n\nWrite out the 2×2 permutation formula explicitly and check it equals \\(ad - bc\\).\nExpand a 3×3 determinant by hand using the six terms.\nModify the code to count how many multiplications are required for a 5×5 matrix using the permutation definition.\n\n\n\nThe Takeaway\n\nDeterminant = signed sum over all permutations.\nSigns come from permutation parity (even/odd swaps).\nThis definition is the combinatorial foundation that unifies all determinant properties.\n\n\n\n\n59. Cramer’s Rule (Solving with Determinants, and When Not to Use It)\nCramer’s Rule gives an explicit formula for solving a system of linear equations \\(Ax = b\\) using determinants. It is elegant but inefficient for large systems.\nFor \\(A \\in \\mathbb{R}^{n \\times n}\\) with \\(\\det(A) \\neq 0\\):\n\\[\nx_i = \\frac{\\det(A_i)}{\\det(A)}\n\\]\nwhere \\(A_i\\) is \\(A\\) with its \\(i\\)-th column replaced by \\(b\\).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple 2×2 example\n\nSolve:\n\\[\n\\begin{cases}\n2x + y = 5 \\\\\nx - y = 1\n\\end{cases}\n\\]\n\nA = Matrix([[2,1],[1,-1]])\nb = Matrix([5,1])\n\ndetA = A.det()\nprint(\"det(A):\", detA)\n\n# Replace columns\nA1 = A.copy()\nA1[:,0] = b\nA2 = A.copy()\nA2[:,1] = b\n\nx1 = A1.det() / detA\nx2 = A2.det() / detA\nprint(\"Solution via Cramer's Rule:\", [x1, x2])\n\n# Check with built-in solver\nprint(\"SymPy solve:\", A.LUsolve(b))\n\ndet(A): -3\nSolution via Cramer's Rule: [2, 1]\nSymPy solve: Matrix([[2], [1]])\n\n\nBoth give the same solution.\n\n3×3 example\n\n\nA = Matrix([\n    [1,2,3],\n    [0,1,4],\n    [5,6,0]\n])\nb = Matrix([7,8,9])\n\ndetA = A.det()\nprint(\"det(A):\", detA)\n\nsolutions = []\nfor i in range(A.shape[1]):\n    Ai = A.copy()\n    Ai[:,i] = b\n    solutions.append(Ai.det()/detA)\n\nprint(\"Solution via Cramer's Rule:\", solutions)\nprint(\"SymPy solve:\", A.LUsolve(b))\n\ndet(A): 1\nSolution via Cramer's Rule: [21, -16, 6]\nSymPy solve: Matrix([[21], [-16], [6]])\n\n\n\nNumPy version (inefficient but illustrative)\n\n\nA = np.array([[2,1],[1,-1]], dtype=float)\nb = np.array([5,1], dtype=float)\n\ndetA = np.linalg.det(A)\n\nsolutions = []\nfor i in range(A.shape[1]):\n    Ai = A.copy()\n    Ai[:,i] = b\n    solutions.append(np.linalg.det(Ai)/detA)\n\nprint(\"Solution:\", solutions)\n\nSolution: [np.float64(2.0000000000000004), np.float64(1.0)]\n\n\n\nWhy not use it in practice?\n\n\nRequires computing \\(n+1\\) determinants.\nDeterminant computation via cofactor expansion is factorial-time.\nGaussian elimination or LU is far more efficient.\n\n\n\nTry It Yourself\n\nSolve a 3×3 system using Cramer’s Rule and confirm with A.solve(b).\nTry Cramer’s Rule when \\(\\det(A)=0\\). What happens?\nCompare runtime of Cramer’s Rule vs LU for a random 5×5 matrix.\n\n\n\nThe Takeaway\n\nCramer’s Rule gives explicit formulas for solutions using determinants.\nBeautiful for theory, useful for small cases, but not computationally practical.\nIt highlights the deep connection between determinants and solving linear systems.\n\n\n\n\n60. Computing Determinants in Practice (Use LU, Mind Stability)\nWhile definitions like cofactor expansion and permutations are beautiful, they are too slow for large matrices. In practice, determinants are computed using row reduction or LU decomposition, with careful attention to numerical stability.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nCofactor expansion is too slow\n\n\nA = Matrix([\n    [1,2,3],\n    [4,5,6],\n    [7,8,10]\n])\nprint(\"det via cofactor expansion:\", A.det())\n\ndet via cofactor expansion: -3\n\n\nThis works for 3×3, but complexity grows factorially.\n\nDeterminant via triangular form (LU decomposition)\n\nLU decomposition factorizes \\(A = LU\\), where \\(L\\) is lower triangular and \\(U\\) is upper triangular. Determinant = product of diagonals of \\(U\\), up to sign corrections for row swaps.\n\nL, U, perm = A.LUdecomposition()\ndetA = A.det()\nprint(\"L:\\n\", L)\nprint(\"U:\\n\", U)\nprint(\"Permutation matrix:\\n\", perm)\nprint(\"det via LU product:\", detA)\n\nL:\n Matrix([[1, 0, 0], [4, 1, 0], [7, 2, 1]])\nU:\n Matrix([[1, 2, 3], [0, -3, -6], [0, 0, 1]])\nPermutation matrix:\n []\ndet via LU product: -3\n\n\n\nNumPy efficient method\n\n\nA_np = np.array([[1,2,3],[4,5,6],[7,8,10]], dtype=float)\nprint(\"NumPy det:\", np.linalg.det(A_np))\n\nNumPy det: -2.9999999999999996\n\n\nNumPy uses optimized routines (LAPACK under the hood).\n\nLarge random matrix\n\n\nnp.random.seed(0)\nB = np.random.rand(5,5)\nprint(\"NumPy det (5x5):\", np.linalg.det(B))\n\nNumPy det (5x5): 0.00965822550588513\n\n\nComputes quickly even for larger matrices.\n\nStability issues\n\nDeterminants of large or ill-conditioned matrices can suffer from floating-point errors. For example, if rows are nearly dependent:\n\nC = np.array([[1,2,3],[2,4.0000001,6],[3,6,9]], dtype=float)\nprint(\"det(C):\", np.linalg.det(C))\n\ndet(C): 0.0\n\n\nThe result may not be exactly 0 due to floating-point approximations.\n\n\nTry It Yourself\n\nCompute the determinant of a random 10×10 matrix with np.linalg.det.\nCompare results between SymPy (exact rational arithmetic) and NumPy (floating-point).\nTest determinant of a nearly singular matrix - notice numerical instability.\n\n\n\nThe Takeaway\n\nDeterminants in practice are computed with LU decomposition or equivalent.\nAlways be mindful of numerical stability - small errors matter when determinant ≈ 0.\nFor exact answers (small cases), use symbolic tools like SymPy; for speed, use NumPy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-7.-eigenvalues-eigenvectors-and-dynamics",
    "href": "books/en-US/lab.html#chapter-7.-eigenvalues-eigenvectors-and-dynamics",
    "title": "The LAB",
    "section": "Chapter 7. Eigenvalues, Eigenvectors, and Dynamics",
    "text": "Chapter 7. Eigenvalues, Eigenvectors, and Dynamics\n\n61. Eigenvalues and Eigenvectors (Directions That Stay Put)\nAn eigenvector of a matrix \\(A\\) is a special vector that doesn’t change direction when multiplied by \\(A\\). Instead, it only gets stretched or shrunk by a scalar called the eigenvalue.\nFormally:\n\\[\nA v = \\lambda v\n\\]\nwhere \\(v\\) is an eigenvector and \\(\\lambda\\) is the eigenvalue.\nGeometrically: eigenvectors are “preferred directions” of a linear transformation.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nA simple 2×2 example\n\n\nA = Matrix([\n    [2,1],\n    [1,2]\n])\n\neigs = A.eigenvects()\nprint(\"Eigenvalues and eigenvectors:\", eigs)\n\nEigenvalues and eigenvectors: [(1, 1, [Matrix([\n[-1],\n[ 1]])]), (3, 1, [Matrix([\n[1],\n[1]])])]\n\n\nThis outputs eigenvalues and their associated eigenvectors.\n\nVerify the eigen equation\n\nPick one eigenpair \\((\\lambda, v)\\):\n\nlam = eigs[0][0]\nv = eigs[0][2][0]\nprint(\"Check Av = λv:\", A*v, lam*v)\n\nCheck Av = λv: Matrix([[-1], [1]]) Matrix([[-1], [1]])\n\n\nBoth sides match → confirming the eigenpair.\n\nNumPy version\n\n\nA_np = np.array([[2,1],[1,2]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A_np)\n\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\\n\", eigvecs)\n\nEigenvalues: [3. 1.]\nEigenvectors:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n\n\nColumns of eigvecs are eigenvectors.\n\nGeometric interpretation (plot)\n\n\nimport matplotlib.pyplot as plt\n\nv1 = np.array(eigvecs[:,0])\nv2 = np.array(eigvecs[:,1])\n\nplt.arrow(0,0,v1[0],v1[1],head_width=0.1,color=\"blue\",length_includes_head=True)\nplt.arrow(0,0,v2[0],v2[1],head_width=0.1,color=\"red\",length_includes_head=True)\n\nplt.axhline(0,color=\"black\",linewidth=0.5)\nplt.axvline(0,color=\"black\",linewidth=0.5)\nplt.axis(\"equal\")\nplt.grid()\nplt.title(\"Eigenvectors: directions that stay put\")\nplt.show()\n\n\n\n\n\n\n\n\nBoth eigenvectors define directions where the transformation acts by scaling only.\n\nRandom 3×3 matrix example\n\n\nnp.random.seed(1)\nB = Matrix(np.random.randint(-2,3,(3,3)))\nprint(\"Matrix B:\\n\", B)\nprint(\"Eigenvalues/vectors:\", B.eigenvects())\n\nMatrix B:\n Matrix([[1, 2, -2], [-1, 1, -2], [-2, -1, 2]])\nEigenvalues/vectors: [(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)), 1, [Matrix([\n[ -16/27 - 91/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 - 7*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],\n[50/27 + 5*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 2*(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 + 65/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],\n[                                                                                                                                                                                                                                                              1]])]), (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([\n[ -16/27 - 7*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 - 91/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],\n[50/27 + 65/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) - 2*(4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 5*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],\n[                                                                                                                                                                                                                                                              1]])]), (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([\n[  -7*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 16/27 - 91/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9],\n[-2*(13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 65/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + 5*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + 50/27],\n[                                                                                                                                                                           1]])])]\n\n\n\n\nTry It Yourself\n\nCompute eigenvalues and eigenvectors of\n\\[\n\\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nand verify that they match the diagonal entries.\nUse NumPy to find eigenvectors of a rotation matrix by 90°. What do you notice?\nFor a singular matrix, check if 0 is an eigenvalue.\n\n\n\nThe Takeaway\n\nEigenvalues = scale factors; eigenvectors = directions that stay put.\nThe eigen equation \\(Av=\\lambda v\\) captures the essence of a matrix’s action.\nThey form the foundation for deeper topics like diagonalization, stability, and dynamics.\n\n\n\n\n62. Characteristic Polynomial (Where Eigenvalues Come From)\nEigenvalues don’t appear out of thin air - they come from the characteristic polynomial of a matrix. For a square matrix \\(A\\),\n\\[\np(\\lambda) = \\det(A - \\lambda I)\n\\]\nThe roots of this polynomial are the eigenvalues of \\(A\\).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix, symbols\n\n\n\nStep-by-Step Code Walkthrough\n\n2×2 example\n\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\]\n\nλ = symbols('λ')\nA = Matrix([[2,1],[1,2]])\nchar_poly = A.charpoly(λ)\nprint(\"Characteristic polynomial:\", char_poly.as_expr())\nprint(\"Eigenvalues (roots):\", char_poly.all_roots())\n\nCharacteristic polynomial: λ**2 - 4*λ + 3\nEigenvalues (roots): [1, 3]\n\n\nPolynomial: \\(\\lambda^2 - 4\\lambda + 3\\). Roots: \\(\\lambda = 3, 1\\).\n\nVerify with eigen computation\n\n\nprint(\"Eigenvalues directly:\", A.eigenvals())\n\nEigenvalues directly: {3: 1, 1: 1}\n\n\nMatches the roots of the polynomial.\n\n3×3 example\n\n\nB = Matrix([\n    [1,2,3],\n    [0,1,4],\n    [5,6,0]\n])\n\nchar_poly_B = B.charpoly(λ)\nprint(\"Characteristic polynomial of B:\", char_poly_B.as_expr())\nprint(\"Eigenvalues of B:\", char_poly_B.all_roots())\n\nCharacteristic polynomial of B: λ**3 - 2*λ**2 - 38*λ - 1\nEigenvalues of B: [CRootOf(x**3 - 2*x**2 - 38*x - 1, 0), CRootOf(x**3 - 2*x**2 - 38*x - 1, 1), CRootOf(x**3 - 2*x**2 - 38*x - 1, 2)]\n\n\n\nNumPy version\n\nNumPy doesn’t give the polynomial directly, but eigenvalues can be checked:\n\nB_np = np.array([[1,2,3],[0,1,4],[5,6,0]], dtype=float)\neigvals = np.linalg.eigvals(B_np)\nprint(\"NumPy eigenvalues:\", eigvals)\n\nNumPy eigenvalues: [-5.2296696  -0.02635282  7.25602242]\n\n\n\nRelation to trace and determinant\n\nFor a 2×2 matrix\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n\\]\nthe characteristic polynomial is\n\\[\n\\lambda^2 - (a+d)\\lambda + (ad - bc).\n\\]\n\nCoefficient of \\(\\lambda\\): \\(-\\text{trace}(A)\\).\nConstant term: \\(\\det(A)\\).\n\n\nprint(\"Trace:\", A.trace())\nprint(\"Determinant:\", A.det())\n\nTrace: 4\nDeterminant: 3\n\n\n\n\nTry It Yourself\n\nCompute the characteristic polynomial of\n\\[\n\\begin{bmatrix} 4 & 0 \\\\ 0 & 5 \\end{bmatrix}\n\\]\nand confirm eigenvalues are 4 and 5.\nCheck the relationship between polynomial coefficients, trace, and determinant for a 3×3 case.\nVerify with NumPy that the roots of the polynomial equal the eigenvalues.\n\n\n\nThe Takeaway\n\nThe characteristic polynomial encodes eigenvalues as its roots.\nCoefficients are tied to invariants: trace and determinant.\nThis polynomial viewpoint is the bridge from algebraic formulas to geometric eigen-behavior.\n\n\n\n\n63. Algebraic vs. Geometric Multiplicity (How Many and How Independent)\nEigenvalues can repeat, and when they do, two notions of multiplicity arise:\n\nAlgebraic multiplicity: how many times the eigenvalue appears as a root of the characteristic polynomial.\nGeometric multiplicity: the dimension of the eigenspace (number of independent eigenvectors).\n\nAlways:\n\\[\n1 \\leq \\text{geometric multiplicity} \\leq \\text{algebraic multiplicity}\n\\]\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nMatrix with repeated eigenvalue\n\n\nA = Matrix([\n    [2,1],\n    [0,2]\n])\n\nprint(\"Eigenvalues and algebraic multiplicity:\", A.eigenvals())\nprint(\"Eigenvectors:\", A.eigenvects())\n\nEigenvalues and algebraic multiplicity: {2: 2}\nEigenvectors: [(2, 2, [Matrix([\n[1],\n[0]])])]\n\n\n\nEigenvalue 2 has algebraic multiplicity = 2.\nBut only 1 independent eigenvector → geometric multiplicity = 1.\n\n\nDiagonal matrix with repetition\n\n\nB = Matrix([\n    [3,0,0],\n    [0,3,0],\n    [0,0,3]\n])\n\nprint(\"Eigenvalues:\", B.eigenvals())\nprint(\"Eigenvectors:\", B.eigenvects())\n\nEigenvalues: {3: 3}\nEigenvectors: [(3, 3, [Matrix([\n[1],\n[0],\n[0]]), Matrix([\n[0],\n[1],\n[0]]), Matrix([\n[0],\n[0],\n[1]])])]\n\n\nHere, eigenvalue 3 has algebraic multiplicity = 3, and geometric multiplicity = 3.\n\nNumPy check\n\n\nA_np = np.array([[2,1],[0,2]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A_np)\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\\n\", eigvecs)\n\nEigenvalues: [2. 2.]\nEigenvectors:\n [[ 1.0000000e+00 -1.0000000e+00]\n [ 0.0000000e+00  4.4408921e-16]]\n\n\nNumPy won’t show multiplicities directly, but you can see repeated eigenvalues.\n\nComparing two cases\n\n\nDefective matrix: Algebraic &gt; geometric (like the upper triangular \\(A\\)).\nDiagonalizable matrix: Algebraic = geometric (like \\(B\\)).\n\nThis distinction determines whether a matrix can be diagonalized.\n\n\nTry It Yourself\n\nCompute algebraic and geometric multiplicities of\n\\[\n\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\n\\]\n(hint: only one eigenvector).\nTake a diagonal matrix with repeated entries - what happens to multiplicities?\nTest a random 3×3 singular matrix. Does 0 have algebraic multiplicity &gt; 1?\n\n\n\nThe Takeaway\n\nAlgebraic multiplicity = count of root in characteristic polynomial.\nGeometric multiplicity = dimension of eigenspace.\nIf they match for all eigenvalues → matrix is diagonalizable.\n\n\n\n\n64. Diagonalization (When a Matrix Becomes Simple)\nA matrix \\(A\\) is diagonalizable if it can be written as\n\\[\nA = P D P^{-1}\n\\]\n\n\\(D\\) is diagonal (containing eigenvalues).\nColumns of \\(P\\) are the eigenvectors.\n\nThis means \\(A\\) acts like simple scaling in a “better” coordinate system.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nA diagonalizable 2×2 matrix\n\n\nA = Matrix([\n    [4,1],\n    [2,3]\n])\n\nP, D = A.diagonalize()\nprint(\"P (eigenvectors):\")\nprint(P)\nprint(\"D (eigenvalues on diagonal):\")\nprint(D)\n\n# Verify A = P D P^-1\nprint(\"Check:\", P*D*P.inv())\n\nP (eigenvectors):\nMatrix([[-1, 1], [2, 1]])\nD (eigenvalues on diagonal):\nMatrix([[2, 0], [0, 5]])\nCheck: Matrix([[4, 1], [2, 3]])\n\n\n\nA non-diagonalizable matrix\n\n\nB = Matrix([\n    [2,1],\n    [0,2]\n])\n\ntry:\n    P, D = B.diagonalize()\n    print(\"Diagonalization successful\")\nexcept Exception as e:\n    print(\"Not diagonalizable:\", e)\n\nNot diagonalizable: Matrix is not diagonalizable\n\n\nThis fails because eigenvalue 2 has algebraic multiplicity 2 but geometric multiplicity 1.\n\nDiagonalization with NumPy\n\nNumPy doesn’t diagonalize explicitly, but we can build \\(P\\) and \\(D\\) ourselves:\n\nA_np = np.array([[4,1],[2,3]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A_np)\n\nP = eigvecs\nD = np.diag(eigvals)\nPinv = np.linalg.inv(P)\n\nprint(\"Check A = PDP^-1:\\n\", P @ D @ Pinv)\n\nCheck A = PDP^-1:\n [[4. 1.]\n [2. 3.]]\n\n\n\nPowers of a diagonalizable matrix\n\nOne reason diagonalization is powerful:\n\\[\nA^k = P D^k P^{-1}\n\\]\nSince \\(D^k\\) is trivial (just raise each diagonal entry to power \\(k\\)).\n\nk = 5\n\nA_power = np.linalg.matrix_power(A, k)\nD_power = np.linalg.matrix_power(D, k)\nA_via_diag = P @ D_power @ np.linalg.inv(P)\n\nprint(\"A^5 via diagonalization:\\n\", A_via_diag)\nprint(\"Direct A^5:\\n\", A_power)\n\nA^5 via diagonalization:\n [[2094. 1031.]\n [2062. 1063.]]\nDirect A^5:\n [[2094 1031]\n [2062 1063]]\n\n\nBoth match.\n\n\nTry It Yourself\n\nCheck whether\n\\[\n\\begin{bmatrix} 5 & 0 \\\\ 0 & 5 \\end{bmatrix}\n\\]\nis diagonalizable.\nTry diagonalizing a rotation matrix by 90°. Do you get complex eigenvalues?\nVerify the formula \\(A^k = P D^k P^{-1}\\) for a 3×3 diagonalizable matrix.\n\n\n\nThe Takeaway\n\nDiagonalization rewrites a matrix in its simplest form.\nWorks if there are enough independent eigenvectors.\nIt makes powers of \\(A\\) easy, and is the gateway to analyzing dynamics.\n\n\n\n\n65. Powers of a Matrix (Long-Term Behavior via Eigenvalues)\nOne of the most useful applications of eigenvalues and diagonalization is computing powers of a matrix:\n\\[\nA^k = P D^k P^{-1}\n\\]\nwhere \\(D\\) is diagonal with eigenvalues of \\(A\\). Each eigenvalue \\(\\lambda\\) raised to \\(k\\) dictates how its eigenvector direction grows, decays, or oscillates over time.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple diagonal matrix\n\nIf \\(D = \\text{diag}(2,3)\\):\n\nD = Matrix([[2,0],[0,3]])\nprint(\"D^5 =\")\nprint(D**5)\n\nD^5 =\nMatrix([[32, 0], [0, 243]])\n\n\nEigenvalues are 2 and 3. Raising to the 5th power just raises each eigenvalue to the 5th: \\(2^5, 3^5\\).\n\nNon-diagonal matrix\n\n\nA = Matrix([\n    [4,1],\n    [2,3]\n])\n\nP, D = A.diagonalize()\nprint(\"D (eigenvalues):\")\nprint(D)\n\n# Compute A^10 via diagonalization\nA10 = P * (D**10) * P.inv()\nprint(\"A^10 =\")\nprint(A10)\n\nD (eigenvalues):\nMatrix([[2, 0], [0, 5]])\nA^10 =\nMatrix([[6510758, 3254867], [6509734, 3255891]])\n\n\nMuch easier than multiplying \\(A\\) ten times!\n\nNumPy version\n\n\nA_np = np.array([[4,1],[2,3]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A_np)\n\nk = 10\nD_power = np.diag(eigvals**k)\nA10_np = eigvecs @ D_power @ np.linalg.inv(eigvecs)\n\nprint(\"A^10 via eigen-decomposition:\\n\", A10_np)\n\nA^10 via eigen-decomposition:\n [[6510758. 3254867.]\n [6509734. 3255891.]]\n\n\n\nLong-term behavior\n\nEigenvalues tell us what happens as \\(k \\to \\infty\\):\n\nIf \\(|\\lambda| &lt; 1\\) → decay to 0.\nIf \\(|\\lambda| &gt; 1\\) → grows unbounded.\nIf \\(|\\lambda| = 1\\) → oscillates or stabilizes.\n\n\nB = Matrix([\n    [0.5,0],\n    [0,1.2]\n])\n\nP, D = B.diagonalize()\nprint(\"Eigenvalues:\", D)\nprint(\"B^20:\", P*(D**20)*P.inv())\n\nEigenvalues: Matrix([[0.500000000000000, 0], [0, 1.20000000000000]])\nB^20: Matrix([[9.53674316406250e-7, 0], [0, 38.3375999244747]])\n\n\nHere, the component along eigenvalue 0.5 decays, while eigenvalue 1.2 grows.\n\n\nTry It Yourself\n\nCompute \\(A^{50}\\) for a diagonal matrix with eigenvalues 0.9 and 1.1. Which component dominates?\nTake a stochastic (Markov) matrix and compute powers. Do the rows stabilize?\nExperiment with complex eigenvalues (like a rotation) and check if the powers oscillate.\n\n\n\nThe Takeaway\n\nMatrix powers are simple when using eigenvalues.\nLong-term dynamics are controlled by eigenvalue magnitudes.\nThis insight is critical in Markov chains, stability analysis, and dynamical systems.\n\n\n\n\n66. Real vs. Complex Spectra (Rotations and Oscillations)\nNot all eigenvalues are real. Some matrices, especially those involving rotations, have complex eigenvalues. Complex eigenvalues often describe oscillations or rotations in systems.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nRotation matrix in 2D\n\nA 90° rotation matrix:\n\\[\nR = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\n\\]\n\nR = Matrix([[0, -1],\n            [1,  0]])\n\nprint(\"Characteristic polynomial:\", R.charpoly())\nprint(\"Eigenvalues:\", R.eigenvals())\n\nCharacteristic polynomial: PurePoly(lambda**2 + 1, lambda, domain='ZZ')\nEigenvalues: {-I: 1, I: 1}\n\n\nResult: eigenvalues are \\(i\\) and \\(-i\\) (purely imaginary).\n\nVerify eigen-equation with complex numbers\n\n\neigs = R.eigenvects()\nfor eig in eigs:\n    lam = eig[0]\n    v = eig[2][0]\n    print(f\"λ = {lam}, Av = {R*v}, λv = {lam*v}\")\n\nλ = -I, Av = Matrix([[-1], [-I]]), λv = Matrix([[-1], [-I]])\nλ = I, Av = Matrix([[-1], [I]]), λv = Matrix([[-1], [I]])\n\n\n\nNumPy version\n\n\nR_np = np.array([[0,-1],[1,0]], dtype=float)\neigvals, eigvecs = np.linalg.eig(R_np)\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\\n\", eigvecs)\n\nEigenvalues: [0.+1.j 0.-1.j]\nEigenvectors:\n [[0.70710678+0.j         0.70710678-0.j        ]\n [0.        -0.70710678j 0.        +0.70710678j]]\n\n\nNumPy shows complex eigenvalues with j (Python’s imaginary unit).\n\nRotation by arbitrary angle\n\nGeneral 2D rotation:\n\\[\nR(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\n\\]\nEigenvalues:\n\\[\n\\lambda = e^{\\pm i\\theta} = \\cos\\theta \\pm i\\sin\\theta\n\\]\n\ntheta = np.pi/4  # 45 degrees\nR_theta = np.array([[np.cos(theta), -np.sin(theta)],\n                    [np.sin(theta),  np.cos(theta)]])\n\neigvals, eigvecs = np.linalg.eig(R_theta)\nprint(\"Eigenvalues (rotation 45°):\", eigvals)\n\nEigenvalues (rotation 45°): [0.70710678+0.70710678j 0.70710678-0.70710678j]\n\n\n\nOscillation insight\n\n\nComplex eigenvalues with \\(|\\lambda|=1\\) → pure oscillation (no growth).\nIf \\(|\\lambda|&lt;1\\) → decaying spiral.\nIf \\(|\\lambda|&gt;1\\) → growing spiral.\n\nExample:\n\nA = np.array([[0.8, -0.6],\n              [0.6,  0.8]])\n\neigvals, _ = np.linalg.eig(A)\nprint(\"Eigenvalues:\", eigvals)\n\nEigenvalues: [0.8+0.6j 0.8-0.6j]\n\n\nThese eigenvalues lie inside the unit circle → spiral decay.\n\n\nTry It Yourself\n\nCompute eigenvalues of a 180° rotation. What happens?\nModify the rotation matrix to include scaling (e.g., multiply by 1.1). Do the eigenvalues lie outside the unit circle?\nPlot the trajectory of repeatedly applying a rotation matrix to a vector.\n\n\n\nThe Takeaway\n\nComplex eigenvalues naturally appear in rotations and oscillatory systems.\nTheir magnitude controls growth or decay; their angle controls oscillation.\nThis is a key link between linear algebra and dynamics in physics and engineering.\n\n\n\n\n67. Defective Matrices and a Peek at Jordan Form (When Diagonalization Fails)\nNot every matrix has enough independent eigenvectors to be diagonalized. Such matrices are called defective. To handle them, mathematicians use the Jordan normal form, which extends diagonalization with extra structure.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nA defective example\n\n\\[\nA = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\n\nA = Matrix([[2,1],\n            [0,2]])\n\nprint(\"Eigenvalues:\", A.eigenvals())\nprint(\"Eigenvectors:\", A.eigenvects())\n\nEigenvalues: {2: 2}\nEigenvectors: [(2, 2, [Matrix([\n[1],\n[0]])])]\n\n\n\nEigenvalue 2 has algebraic multiplicity = 2.\nOnly 1 eigenvector exists → geometric multiplicity = 1.\n\nThus \\(A\\) is defective, not diagonalizable.\n\nAttempt diagonalization\n\n\ntry:\n    P, D = A.diagonalize()\n    print(\"Diagonal form:\", D)\nexcept Exception as e:\n    print(\"Diagonalization failed:\", e)\n\nDiagonalization failed: Matrix is not diagonalizable\n\n\nYou’ll see an error - confirming \\(A\\) is not diagonalizable.\n\nJordan form in SymPy\n\n\nJ, P = A.jordan_form()\nprint(\"Jordan form J:\")\nprint(J)\nprint(\"P (generalized eigenvectors):\")\nprint(P)\n\nJordan form J:\nMatrix([[1, 0], [0, 1]])\nP (generalized eigenvectors):\nMatrix([[2, 1], [0, 2]])\n\n\nThe Jordan form shows a Jordan block:\n\\[\nJ = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nThis block structure represents the failure of diagonalization.\n\nNumPy perspective\n\nNumPy doesn’t compute Jordan form, but you can see repeated eigenvalues and lack of eigenvectors:\n\nA_np = np.array([[2,1],[0,2]], dtype=float)\neigvals, eigvecs = np.linalg.eig(A_np)\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\\n\", eigvecs)\n\nEigenvalues: [2. 2.]\nEigenvectors:\n [[ 1.0000000e+00 -1.0000000e+00]\n [ 0.0000000e+00  4.4408921e-16]]\n\n\nThe eigenvectors matrix has fewer independent columns than expected.\n\nGeneralized eigenvectors\n\nJordan form introduces generalized eigenvectors, which satisfy:\n\\[\n(A - \\lambda I)^k v = 0 \\quad \\text{for some } k&gt;1\n\\]\nThey “fill the gap” when ordinary eigenvectors are insufficient.\n\n\nTry It Yourself\n\nTest diagonalizability of\n\\[\n\\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}\n\\]\nand compare with its Jordan form.\nTry a 3×3 defective matrix with one Jordan block of size 3.\nVerify that Jordan blocks still capture the correct eigenvalues.\n\n\n\nThe Takeaway\n\nDefective matrices lack enough eigenvectors for diagonalization.\nJordan form replaces diagonalization with blocks, keeping eigenvalues on the diagonal.\nUnderstanding Jordan blocks is essential for advanced linear algebra and differential equations.\n\n\n\n\n68. Stability and Spectral Radius (Grow, Decay, or Oscillate)\nThe spectral radius of a matrix \\(A\\) is defined as\n\\[\n\\rho(A) = \\max_i |\\lambda_i|\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues. It tells us the long-term behavior of repeated applications of \\(A\\):\n\nIf \\(\\rho(A) &lt; 1\\) → powers of \\(A\\) tend to 0 (stable/decay).\nIf \\(\\rho(A) = 1\\) → powers neither blow up nor vanish (neutral, may oscillate).\nIf \\(\\rho(A) &gt; 1\\) → powers diverge (unstable/growth).\n\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nStable matrix (\\(\\rho &lt; 1\\))\n\n\nA = np.array([[0.5, 0],\n              [0, 0.3]])\n\neigvals = np.linalg.eigvals(A)\nspectral_radius = max(abs(eigvals))\n\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Spectral radius:\", spectral_radius)\n\nprint(\"A^10:\\n\", np.linalg.matrix_power(A, 10))\n\nEigenvalues: [0.5 0.3]\nSpectral radius: 0.5\nA^10:\n [[9.765625e-04 0.000000e+00]\n [0.000000e+00 5.904900e-06]]\n\n\nAll entries shrink toward zero.\n\nUnstable matrix (\\(\\rho &gt; 1\\))\n\n\nB = np.array([[1.2, 0],\n              [0, 0.9]])\n\neigvals = np.linalg.eigvals(B)\nprint(\"Eigenvalues:\", eigvals, \"Spectral radius:\", max(abs(eigvals)))\nprint(\"B^10:\\n\", np.linalg.matrix_power(B, 10))\n\nEigenvalues: [1.2 0.9] Spectral radius: 1.2\nB^10:\n [[6.19173642 0.        ]\n [0.         0.34867844]]\n\n\nThe component along eigenvalue 1.2 grows quickly.\n\nNeutral/oscillatory case (\\(\\rho = 1\\))\n\n90° rotation matrix:\n\nR = np.array([[0, -1],\n              [1,  0]])\n\neigvals = np.linalg.eigvals(R)\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Spectral radius:\", max(abs(eigvals)))\nprint(\"R^4:\\n\", np.linalg.matrix_power(R, 4))\n\nEigenvalues: [0.+1.j 0.-1.j]\nSpectral radius: 1.0\nR^4:\n [[1 0]\n [0 1]]\n\n\nEigenvalues are ±i, with modulus 1 → pure oscillation.\n\nSpectral radius with SymPy\n\n\nM = Matrix([[2,1],[1,2]])\neigs = M.eigenvals()\nprint(\"Eigenvalues:\", eigs)\nprint(\"Spectral radius:\", max(abs(ev) for ev in eigs))\n\nEigenvalues: {3: 1, 1: 1}\nSpectral radius: 3\n\n\n\n\nTry It Yourself\n\nBuild a diagonal matrix with entries 0.8, 1.0, and 1.1. Predict which direction dominates as powers grow.\nApply a random matrix repeatedly to a vector. Does it shrink, grow, or oscillate?\nCheck if a Markov chain transition matrix always has spectral radius 1.\n\n\n\nThe Takeaway\n\nThe spectral radius is the key number that predicts growth, decay, or oscillation.\nLong-term stability in dynamical systems is governed entirely by eigenvalue magnitudes.\nThis connects linear algebra directly to control theory, Markov chains, and differential equations.\n\n\n\n\n69. Markov Chains and Steady States (Probabilities as Linear Algebra)\nA Markov chain is a process that moves between states according to probabilities. The transitions are encoded in a stochastic matrix \\(P\\):\n\nEach entry \\(p_{ij} \\geq 0\\)\nEach row sums to 1\n\nIf we start with a probability vector \\(v_0\\), then after \\(k\\) steps:\n\\[\nv_k = v_0 P^k\n\\]\nA steady state is a probability vector \\(v\\) such that \\(vP = v\\). It corresponds to eigenvalue \\(\\lambda = 1\\).\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple two-state chain\n\n\nP = np.array([\n    [0.9, 0.1],\n    [0.5, 0.5]\n])\n\nv0 = np.array([1.0, 0.0])  # start in state 1\nfor k in [1, 2, 5, 10, 50]:\n    vk = v0 @ np.linalg.matrix_power(P, k)\n    print(f\"Step {k}: {vk}\")\n\nStep 1: [0.9 0.1]\nStep 2: [0.86 0.14]\nStep 5: [0.83504 0.16496]\nStep 10: [0.83335081 0.16664919]\nStep 50: [0.83333333 0.16666667]\n\n\nThe distribution stabilizes as \\(k\\) increases.\n\nSteady state via eigenvector\n\nFind eigenvector for eigenvalue 1:\n\neigvals, eigvecs = np.linalg.eig(P.T)\nsteady_state = eigvecs[:, np.isclose(eigvals, 1)]\nsteady_state = steady_state / steady_state.sum()\nprint(\"Steady state:\", steady_state.real.flatten())\n\nSteady state: [0.83333333 0.16666667]\n\n\n\nSymPy exact check\n\n\nP_sym = Matrix([[0.9,0.1],[0.5,0.5]])\nsteady = P_sym.eigenvects()\nprint(\"Eigen info:\", steady)\n\nEigen info: [(1.00000000000000, 1, [Matrix([\n[0.707106781186548],\n[0.707106781186547]])]), (0.400000000000000, 1, [Matrix([\n[-0.235702260395516],\n[  1.17851130197758]])])]\n\n\n\nA 3-state example\n\n\nQ = np.array([\n    [0.3, 0.7, 0.0],\n    [0.2, 0.5, 0.3],\n    [0.1, 0.2, 0.7]\n])\n\neigvals, eigvecs = np.linalg.eig(Q.T)\nsteady = eigvecs[:, np.isclose(eigvals, 1)]\nsteady = steady / steady.sum()\nprint(\"Steady state for Q:\", steady.real.flatten())\n\nSteady state for Q: [0.17647059 0.41176471 0.41176471]\n\n\n\n\nTry It Yourself\n\nCreate a transition matrix where one state is absorbing (e.g., row = [0,0,1]). What happens to the steady state?\nSimulate a random walk on 3 states. Does the steady state distribute evenly?\nCompare long-run simulation with eigenvector computation.\n\n\n\nThe Takeaway\n\nMarkov chains evolve by repeated multiplication with a stochastic matrix.\nSteady states are eigenvectors with eigenvalue 1.\nThis framework powers real applications like PageRank, weather models, and queuing systems.\n\n\n\n\n70. Linear Differential Systems (Solutions via Eigen-Decomposition)\nLinear differential equations often reduce to systems of the form:\n\\[\n\\frac{d}{dt}x(t) = A x(t)\n\\]\nwhere \\(A\\) is a matrix and \\(x(t)\\) is a vector of functions. The solution is given by the matrix exponential:\n\\[\nx(t) = e^{At} x(0)\n\\]\nIf \\(A\\) is diagonalizable, this becomes simple using eigenvalues and eigenvectors.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom sympy import Matrix, exp, symbols\nfrom scipy.linalg import expm\n\n\n\nStep-by-Step Code Walkthrough\n\nSimple system with diagonal matrix\n\n\\[\nA = \\begin{bmatrix} -1 & 0 \\\\ 0 & 2 \\end{bmatrix}\n\\]\n\nA = Matrix([[-1,0],\n            [0, 2]])\nt = symbols('t')\nexpAt = (A*t).exp()\nprint(\"e^{At} =\")\nprint(expAt)\n\ne^{At} =\nMatrix([[exp(-t), 0], [0, exp(2*t)]])\n\n\nSolution:\n\\[\nx(t) = \\begin{bmatrix} e^{-t} & 0 \\\\ 0 & e^{2t} \\end{bmatrix} x(0)\n\\]\nOne component decays, the other grows.\n\nNon-diagonal example\n\n\nB = Matrix([[0,1],\n            [-2,-3]])\nexpBt = (B*t).exp()\nprint(\"e^{Bt} =\")\nprint(expBt)\n\ne^{Bt} =\nMatrix([[2*exp(-t) - exp(-2*t), exp(-t) - exp(-2*t)], [-2*exp(-t) + 2*exp(-2*t), -exp(-t) + 2*exp(-2*t)]])\n\n\nHere the solution involves exponentials and possibly sines/cosines (oscillatory behavior).\n\nNumeric computation with SciPy\n\n\nimport numpy as np\nfrom scipy.linalg import expm\n\nA = np.array([[-1,0],[0,2]], dtype=float)\nt = 1.0\nprint(\"Matrix exponential e^{At} at t=1:\\n\", expm(A*t))\n\nMatrix exponential e^{At} at t=1:\n [[0.36787944 0.        ]\n [0.         7.3890561 ]]\n\n\nThis computes \\(e^{At}\\) numerically.\n\nSimulation of a trajectory\n\n\nx0 = np.array([1.0, 1.0])\nfor t in [0, 0.5, 1, 2]:\n    xt = expm(A*t) @ x0\n    print(f\"x({t}) = {xt}\")\n\nx(0) = [1. 1.]\nx(0.5) = [0.60653066 2.71828183]\nx(1) = [0.36787944 7.3890561 ]\nx(2) = [ 0.13533528 54.59815003]\n\n\nOne coordinate decays, the other explodes with time.\n\n\nTry It Yourself\n\nSolve the system \\(\\dot{x} = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}x\\). What kind of motion do you see?\nUse SciPy to simulate a system with eigenvalues less than 0. Does it always decay?\nTry an unstable system with eigenvalues &gt; 0 and watch how trajectories diverge.\n\n\n\nThe Takeaway\n\nLinear systems \\(\\dot{x} = Ax\\) are solved via the matrix exponential.\nEigenvalues determine stability: negative real parts = stable, positive = unstable, imaginary = oscillations.\nThis ties linear algebra directly to differential equations and dynamical systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-8.-orthogonality-least-squars-and-qr",
    "href": "books/en-US/lab.html#chapter-8.-orthogonality-least-squars-and-qr",
    "title": "The LAB",
    "section": "Chapter 8. Orthogonality, least squars, and QR",
    "text": "Chapter 8. Orthogonality, least squars, and QR\n\n71. Inner Products Beyond Dot Product (Custom Notions of Angle)\nThe dot product is the standard inner product in \\(\\mathbb{R}^n\\), but linear algebra allows us to define more general inner products that measure length and angle in different ways.\nAn inner product on a vector space is a function \\(\\langle u, v \\rangle\\) that satisfies:\n\nLinearity in the first argument.\nSymmetry: \\(\\langle u, v \\rangle = \\langle v, u \\rangle\\).\nPositive definiteness: \\(\\langle v, v \\rangle \\geq 0\\) and equals 0 only if \\(v=0\\).\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nStandard dot product\n\n\nu = np.array([1,2,3])\nv = np.array([4,5,6])\n\nprint(\"Dot product:\", np.dot(u,v))\n\nDot product: 32\n\n\nThis is the familiar formula: \\(1·4 + 2·5 + 3·6 = 32\\).\n\nWeighted inner product\n\nWe can define:\n\\[\n\\langle u, v \\rangle_W = u^T W v\n\\]\nwhere \\(W\\) is a positive definite matrix.\n\nW = np.array([[2,0,0],\n              [0,1,0],\n              [0,0,3]])\n\ndef weighted_inner(u,v,W):\n    return u.T @ W @ v\n\nprint(\"Weighted inner product:\", weighted_inner(u,v,W))\n\nWeighted inner product: 72\n\n\nHere, some coordinates “count more” than others.\n\nCheck symmetry and positivity\n\n\nprint(\"⟨u,v⟩ == ⟨v,u⟩ ?\", weighted_inner(u,v,W) == weighted_inner(v,u,W))\nprint(\"⟨u,u⟩ (should be &gt;0):\", weighted_inner(u,u,W))\n\n⟨u,v⟩ == ⟨v,u⟩ ? True\n⟨u,u⟩ (should be &gt;0): 33\n\n\n\nAngle with weighted inner product\n\n\\[\n\\cos\\theta = \\frac{\\langle u,v \\rangle_W}{\\|u\\|_W \\, \\|v\\|_W}\n\\]\n\ndef weighted_norm(u,W):\n    return np.sqrt(weighted_inner(u,u,W))\n\ncos_theta = weighted_inner(u,v,W) / (weighted_norm(u,W) * weighted_norm(v,W))\nprint(\"Cosine of angle (weighted):\", cos_theta)\n\nCosine of angle (weighted): 0.97573875381809\n\n\n\nCustom example: correlation inner product\n\nFor statistics, an inner product can be defined as covariance or correlation. Example with mean-centered vectors:\n\nx = np.array([2,4,6])\ny = np.array([1,3,5])\n\nx_centered = x - x.mean()\ny_centered = y - y.mean()\n\ncorr_inner = np.dot(x_centered,y_centered)\nprint(\"Correlation-style inner product:\", corr_inner)\n\nCorrelation-style inner product: 8.0\n\n\n\n\nTry It Yourself\n\nDefine a custom inner product with \\(W = \\text{diag}(1,10,100)\\). How does it change angles between vectors?\nVerify positivity: compute \\(\\langle v, v \\rangle_W\\) for a random vector \\(v\\).\nCompare dot product vs weighted inner product on the same pair of vectors.\n\n\n\nThe Takeaway\n\nInner products generalize the dot product to new “geometries.”\nBy changing the weight matrix \\(W\\), you change how lengths and angles are measured.\nThis flexibility is essential in statistics, optimization, and machine learning.\n\n\n\n\n72. Orthogonality and Orthonormal Bases (Perpendicular Power)\nTwo vectors are orthogonal if their inner product is zero:\n\\[\n\\langle u, v \\rangle = 0\n\\]\nIf, in addition, each vector has length 1, the set is orthonormal. Orthonormal bases are extremely useful because they simplify computations: projections, decompositions, and coordinate changes all become clean.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nCheck orthogonality\n\n\nu = np.array([1, -1])\nv = np.array([1, 1])\n\nprint(\"Dot product:\", np.dot(u,v))\n\nDot product: 0\n\n\nSince the dot product is 0, they’re orthogonal.\n\nNormalizing vectors\n\n\\[\n\\hat{u} = \\frac{u}{\\|u\\|}\n\\]\n\ndef normalize(vec):\n    return vec / np.linalg.norm(vec)\n\nu_norm = normalize(u)\nv_norm = normalize(v)\n\nprint(\"Normalized u:\", u_norm)\nprint(\"Normalized v:\", v_norm)\n\nNormalized u: [ 0.70710678 -0.70710678]\nNormalized v: [0.70710678 0.70710678]\n\n\nNow both have length 1.\n\nForm an orthonormal basis\n\n\nbasis = np.column_stack((u_norm, v_norm))\nprint(\"Orthonormal basis:\\n\", basis)\n\nprint(\"Check inner products:\\n\", basis.T @ basis)\n\nOrthonormal basis:\n [[ 0.70710678  0.70710678]\n [-0.70710678  0.70710678]]\nCheck inner products:\n [[ 1.00000000e+00 -2.23711432e-17]\n [-2.23711432e-17  1.00000000e+00]]\n\n\nThe result is the identity matrix → perfectly orthonormal.\n\nApply to coordinates\n\nIf \\(x = [2,3]\\), coordinates in the orthonormal basis are:\n\nx = np.array([2,3])\ncoords = basis.T @ x\nprint(\"Coordinates in new basis:\", coords)\nprint(\"Reconstruction:\", basis @ coords)\n\nCoordinates in new basis: [-0.70710678  3.53553391]\nReconstruction: [2. 3.]\n\n\nIt reconstructs exactly.\n\nRandom example with QR\n\nAny set of linearly independent vectors can be orthonormalized (Gram–Schmidt, or QR decomposition):\n\nM = np.random.rand(3,3)\nQ, R = np.linalg.qr(M)\nprint(\"Q (orthonormal basis):\\n\", Q)\nprint(\"Check Q^T Q = I:\\n\", Q.T @ Q)\n\nQ (orthonormal basis):\n [[-0.37617518  0.91975919 -0.111961  ]\n [-0.82070726 -0.38684608 -0.42046368]\n [-0.430037   -0.06628079  0.90037494]]\nCheck Q^T Q = I:\n [[ 1.00000000e+00 -1.29639194e-16 -1.91943696e-17]\n [-1.29639194e-16  1.00000000e+00  5.38253498e-17]\n [-1.91943696e-17  5.38253498e-17  1.00000000e+00]]\n\n\n\n\nTry It Yourself\n\nCreate two 3D vectors and check if they’re orthogonal.\nNormalize them to form an orthonormal set.\nUse np.linalg.qr on a 4×3 random matrix and verify that the columns of \\(Q\\) are orthonormal.\n\n\n\nThe Takeaway\n\nOrthogonality means perpendicularity; orthonormality adds unit length.\nOrthonormal bases simplify coordinate systems, making inner products and projections easy.\nQR decomposition is the practical tool to generate orthonormal bases in higher dimensions.\n\n\n\n\n73. Gram–Schmidt Process (Constructing Orthonormal Bases)\nThe Gram–Schmidt process takes a set of linearly independent vectors and turns them into an orthonormal basis. This is crucial for working with subspaces, projections, and numerical stability.\nGiven vectors \\(v_1, v_2, \\dots, v_n\\):\n\nSet \\(u_1 = v_1\\).\nSubtract projections to make each new vector orthogonal to the earlier ones:\n\\[\nu_k = v_k - \\sum_{j=1}^{k-1} \\frac{\\langle v_k, u_j \\rangle}{\\langle u_j, u_j \\rangle} u_j\n\\]\nNormalize:\n\\[\ne_k = \\frac{u_k}{\\|u_k\\|}\n\\]\n\nThe set \\(\\{e_1, e_2, \\dots, e_n\\}\\) is orthonormal.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nDefine vectors\n\n\nv1 = np.array([1.0, 1.0, 0.0])\nv2 = np.array([1.0, 0.0, 1.0])\nv3 = np.array([0.0, 1.0, 1.0])\nV = [v1, v2, v3]\n\n\nImplement Gram–Schmidt\n\n\ndef gram_schmidt(V):\n    U = []\n    for v in V:\n        u = v.copy()\n        for uj in U:\n            u -= np.dot(v, uj) / np.dot(uj, uj) * uj\n        U.append(u)\n    # Normalize\n    E = [u/np.linalg.norm(u) for u in U]\n    return np.array(E)\n\nE = gram_schmidt(V)\nprint(\"Orthonormal basis:\\n\", E)\nprint(\"Check orthonormality:\\n\", np.round(E @ E.T, 6))\n\nOrthonormal basis:\n [[ 0.70710678  0.70710678  0.        ]\n [ 0.40824829 -0.40824829  0.81649658]\n [-0.57735027  0.57735027  0.57735027]]\nCheck orthonormality:\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\nCompare with NumPy QR\n\n\nQ, R = np.linalg.qr(np.column_stack(V))\nprint(\"QR-based orthonormal basis:\\n\", Q)\nprint(\"Check Q^T Q = I:\\n\", np.round(Q.T @ Q, 6))\n\nQR-based orthonormal basis:\n [[-0.70710678  0.40824829 -0.57735027]\n [-0.70710678 -0.40824829  0.57735027]\n [-0.          0.81649658  0.57735027]]\nCheck Q^T Q = I:\n [[ 1.  0. -0.]\n [ 0.  1. -0.]\n [-0. -0.  1.]]\n\n\nBoth methods give orthonormal bases.\n\nApplication: projection\n\nTo project a vector \\(x\\) onto the span of \\(V\\):\n\nx = np.array([2.0, 2.0, 2.0])\nproj = sum((x @ e) * e for e in E)\nprint(\"Projection of x onto span(V):\", proj)\n\nProjection of x onto span(V): [2. 2. 2.]\n\n\n\n\nTry It Yourself\n\nRun Gram–Schmidt on two vectors in 2D. Compare with just normalizing and checking orthogonality.\nReplace one vector with a linear combination of others. What happens?\nUse QR decomposition on a 4×3 random matrix and compare with Gram–Schmidt.\n\n\n\nThe Takeaway\n\nGram–Schmidt converts arbitrary independent vectors into an orthonormal basis.\nOrthonormal bases simplify projections, decompositions, and computations.\nIn practice, QR decomposition is often used as a numerically stable implementation.\n\n\n\n\n74. Orthogonal Projections onto Subspaces (Closest Point Principle)\nGiven a subspace spanned by vectors, the orthogonal projection of a vector \\(x\\) onto the subspace is the point in the subspace that is closest to \\(x\\). This is a cornerstone idea in least squares, data fitting, and signal processing.\n\nFormula Recap\nIf \\(Q\\) is a matrix with orthonormal columns spanning the subspace, the projection of \\(x\\) is:\n\\[\n\\text{proj}(x) = Q Q^T x\n\\]\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nProjection onto a line (1D subspace)\n\nSuppose the subspace is spanned by \\(u = [1,2]\\).\n\nu = np.array([1.0,2.0])\nx = np.array([3.0,1.0])\n\nu_norm = u / np.linalg.norm(u)\nproj = np.dot(x, u_norm) * u_norm\nprint(\"Projection of x onto span(u):\", proj)\n\nProjection of x onto span(u): [1. 2.]\n\n\nThis gives the closest point to \\(x\\) along the line spanned by \\(u\\).\n\nProjection onto a plane (2D subspace in 3D)\n\n\nu1 = np.array([1.0,0.0,0.0])\nu2 = np.array([0.0,1.0,0.0])\nQ = np.column_stack([u1,u2])   # Orthonormal basis for xy-plane\n\nx = np.array([2.0,3.0,5.0])\nproj = Q @ Q.T @ x\nprint(\"Projection of x onto xy-plane:\", proj)\n\nProjection of x onto xy-plane: [2. 3. 0.]\n\n\nResult drops the z-component → projection onto the plane.\n\nGeneral projection using QR\n\n\nA = np.array([[1,1,0],\n              [0,1,1],\n              [1,0,1]], dtype=float)\n\nQ, R = np.linalg.qr(A)\nQ = Q[:, :2]   # take first 2 independent columns\nx = np.array([2,2,2], dtype=float)\n\nproj = Q @ Q.T @ x\nprint(\"Projection of x onto span(A):\", proj)\n\nProjection of x onto span(A): [2.66666667 1.33333333 1.33333333]\n\n\n\nVisualization (2D case)\n\n\nimport matplotlib.pyplot as plt\n\nplt.quiver(0,0,x[0],x[1],angles='xy',scale_units='xy',scale=1,color='red',label=\"x\")\nplt.quiver(0,0,proj[0],proj[1],angles='xy',scale_units='xy',scale=1,color='blue',label=\"Projection\")\nplt.quiver(0,0,u[0],u[1],angles='xy',scale_units='xy',scale=1,color='green',label=\"Subspace\")\nplt.axis('equal'); plt.grid(); plt.legend(); plt.show()\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nProject a vector onto the line spanned by \\([2,1]\\).\nProject \\([1,2,3]\\) onto the plane spanned by \\([1,0,1]\\) and \\([0,1,1]\\).\nCompare projection via formula \\(Q Q^T x\\) with manually solving least squares.\n\n\n\nThe Takeaway\n\nOrthogonal projection finds the closest point in a subspace.\nFormula \\(Q Q^T x\\) works perfectly when \\(Q\\) has orthonormal columns.\nProjections are the foundation of least squares, PCA, and many geometric algorithms.\n\n\n\n\n75. Least-Squares Problems (Fit When Exact Solve Is Impossible)\nSometimes a system of equations \\(Ax = b\\) has no exact solution - usually because it’s overdetermined (more equations than unknowns). In this case, we look for an approximate solution \\(x^*\\) that minimizes the error:\n\\[\nx^* = \\arg\\min_x \\|Ax - b\\|^2\n\\]\nThis is the least-squares solution, which geometrically is the projection of \\(b\\) onto the column space of \\(A\\).\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nOverdetermined system\n\n3 equations, 2 unknowns:\n\nA = np.array([[1,1],\n              [1,2],\n              [1,3]], dtype=float)\nb = np.array([6, 0, 0], dtype=float)\n\n\nSolve least squares with NumPy\n\n\nx_star, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\nprint(\"Least squares solution:\", x_star)\nprint(\"Residual norm squared:\", residuals)\n\nLeast squares solution: [ 8. -3.]\nResidual norm squared: [6.]\n\n\n\nCompare with normal equations\n\n\\[\nA^T A x = A^T b\n\\]\n\nx_normal = np.linalg.inv(A.T @ A) @ (A.T @ b)\nprint(\"Solution via normal equations:\", x_normal)\n\nSolution via normal equations: [ 8. -3.]\n\n\n\nGeometric picture\n\nThe least-squares solution projects \\(b\\) onto the column space of \\(A\\):\n\nproj = A @ x_star\nprint(\"Projection of b onto Col(A):\", proj)\nprint(\"Original b:\", b)\nprint(\"Error vector (b - proj):\", b - proj)\n\nProjection of b onto Col(A): [ 5.  2. -1.]\nOriginal b: [6. 0. 0.]\nError vector (b - proj): [ 1. -2.  1.]\n\n\nThe error vector is orthogonal to the column space.\n\nVerify orthogonality condition\n\n\\[\nA^T (b - Ax^*) = 0\n\\]\n\nprint(\"Check orthogonality:\", A.T @ (b - A @ x_star))\n\nCheck orthogonality: [0. 0.]\n\n\nThe result should be (close to) zero.\n\n\nTry It Yourself\n\nCreate a taller \\(A\\) (say 5×2) with random numbers and solve least squares for a random \\(b\\).\nCompare the residual from np.linalg.lstsq with geometric intuition (projection).\nModify \\(b\\) so that the system has an exact solution. Check if least squares gives it exactly.\n\n\n\nThe Takeaway\n\nLeast-squares finds the best-fit solution when no exact solution exists.\nIt works by projecting \\(b\\) onto the column space of \\(A\\).\nThis principle underlies regression, curve fitting, and countless applications in data science.\n\n\n\n\n76. Normal Equations and Geometry of Residuals (Why It Works)\nThe least-squares solution can be found by solving the normal equations:\n\\[\nA^T A x = A^T b\n\\]\nThis comes from the condition that the residual vector\n\\[\nr = b - Ax\n\\]\nis orthogonal to the column space of \\(A\\).\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild an overdetermined system\n\n\nA = np.array([[1,1],\n              [1,2],\n              [1,3]], dtype=float)\nb = np.array([6, 0, 0], dtype=float)\n\n\nSolve least squares via normal equations\n\n\nATA = A.T @ A\nATb = A.T @ b\nx_star = np.linalg.solve(ATA, ATb)\n\nprint(\"Least-squares solution x*:\", x_star)\n\nLeast-squares solution x*: [ 8. -3.]\n\n\n\nCompute residual and check orthogonality\n\n\nresidual = b - A @ x_star\nprint(\"Residual vector:\", residual)\nprint(\"Check A^T r ≈ 0:\", A.T @ residual)\n\nResidual vector: [ 1. -2.  1.]\nCheck A^T r ≈ 0: [0. 0.]\n\n\nThis verifies the residual is perpendicular to the column space of \\(A\\).\n\nCompare with NumPy’s least squares solver\n\n\nx_lstsq, *_ = np.linalg.lstsq(A, b, rcond=None)\nprint(\"NumPy lstsq solution:\", x_lstsq)\n\nNumPy lstsq solution: [ 8. -3.]\n\n\nThe solutions should match (within numerical precision).\n\nGeometric picture\n\n\n\\(b\\) is a point in \\(\\mathbb{R}^3\\).\n\\(Ax\\) is restricted to lie in the 2D column space of \\(A\\).\nThe least-squares solution picks the \\(Ax\\) closest to \\(b\\).\nThe error vector \\(r = b - Ax^*\\) is orthogonal to the subspace.\n\n\nproj = A @ x_star\nprint(\"Projection of b onto Col(A):\", proj)\n\nProjection of b onto Col(A): [ 5.  2. -1.]\n\n\n\n\nTry It Yourself\n\nChange \\(b\\) to \\([1,1,1]\\). Solve again and check the residual.\nUse a random tall \\(A\\) (say 6×2) and verify that the residual is always orthogonal.\nCompute \\(\\|r\\|\\) and see how it changes when you change \\(b\\).\n\n\n\nThe Takeaway\n\nLeast squares works by making the residual orthogonal to the column space.\nNormal equations are the algebraic way to encode this condition.\nThis orthogonality principle is the geometric heart of least-squares fitting.\n\n\n\n\n77. QR Factorization (Stable Least Squares via Orthogonality)\nWhile normal equations solve least squares, they can be numerically unstable if \\(A^T A\\) is ill-conditioned. A more stable method uses QR factorization:\n\\[\nA = Q R\n\\]\n\n\\(Q\\): matrix with orthonormal columns\n\\(R\\): upper triangular matrix\n\nThen the least-squares problem reduces to solving:\n\\[\nRx = Q^T b\n\\]\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nOverdetermined system\n\n\nA = np.array([[1,1],\n              [1,2],\n              [1,3]], dtype=float)\nb = np.array([6, 0, 0], dtype=float)\n\n\nQR factorization\n\n\nQ, R = np.linalg.qr(A)\nprint(\"Q (orthonormal basis):\\n\", Q)\nprint(\"R (upper triangular):\\n\", R)\n\nQ (orthonormal basis):\n [[-5.77350269e-01  7.07106781e-01]\n [-5.77350269e-01  5.55111512e-17]\n [-5.77350269e-01 -7.07106781e-01]]\nR (upper triangular):\n [[-1.73205081 -3.46410162]\n [ 0.         -1.41421356]]\n\n\n\nSolve least squares using QR\n\n\ny = Q.T @ b\nx_star = np.linalg.solve(R[:2,:], y[:2])  # only top rows matter\nprint(\"Least squares solution via QR:\", x_star)\n\nLeast squares solution via QR: [ 8. -3.]\n\n\n\nCompare with NumPy’s lstsq\n\n\nx_lstsq, *_ = np.linalg.lstsq(A, b, rcond=None)\nprint(\"NumPy lstsq:\", x_lstsq)\n\nNumPy lstsq: [ 8. -3.]\n\n\nThe answers should match closely.\n\nResidual check\n\n\nresidual = b - A @ x_star\nprint(\"Residual vector:\", residual)\nprint(\"Check orthogonality (Q^T r):\", Q.T @ residual)\n\nResidual vector: [ 1. -2.  1.]\nCheck orthogonality (Q^T r): [-1.44328993e-15 -1.22124533e-15]\n\n\nResidual is orthogonal to the column space, confirming correctness.\n\n\nTry It Yourself\n\nSolve least squares for a 5×2 random matrix using both normal equations and QR. Compare results.\nCheck stability by making columns of \\(A\\) nearly dependent - see if QR behaves better than normal equations.\nCompute projection of \\(b\\) using \\(Q Q^T b\\) and confirm it equals \\(A x^*\\).\n\n\n\nThe Takeaway\n\nQR factorization provides a numerically stable way to solve least squares.\nIt avoids the instability of normal equations.\nIn practice, modern solvers (like NumPy’s lstsq) rely on QR or SVD under the hood.\n\n\n\n\n78. Orthogonal Matrices (Length-Preserving Transforms)\nAn orthogonal matrix \\(Q\\) is a square matrix whose columns (and rows) are orthonormal vectors. Formally:\n\\[\nQ^T Q = Q Q^T = I\n\\]\nKey properties:\n\nPreserves lengths: \\(\\|Qx\\| = \\|x\\|\\)\nPreserves dot products: \\(\\langle Qx, Qy \\rangle = \\langle x, y \\rangle\\)\nDeterminant is either \\(+1\\) (rotation) or \\(-1\\) (reflection)\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nConstruct a simple orthogonal matrix\n\n90° rotation in 2D:\n\nQ = np.array([[0, -1],\n              [1,  0]])\n\nprint(\"Q^T Q =\\n\", Q.T @ Q)\n\nQ^T Q =\n [[1 0]\n [0 1]]\n\n\nResult = identity → confirms orthogonality.\n\nCheck length preservation\n\n\nx = np.array([3,4])\nprint(\"Original length:\", np.linalg.norm(x))\nprint(\"Transformed length:\", np.linalg.norm(Q @ x))\n\nOriginal length: 5.0\nTransformed length: 5.0\n\n\nBoth lengths match.\n\nCheck dot product preservation\n\n\nu = np.array([1,0])\nv = np.array([0,1])\n\nprint(\"Dot(u,v):\", np.dot(u,v))\nprint(\"Dot(Q u, Q v):\", np.dot(Q @ u, Q @ v))\n\nDot(u,v): 0\nDot(Q u, Q v): 0\n\n\nDot product is preserved.\n\nReflection matrix\n\nReflection about the x-axis:\n\nR = np.array([[1,0],\n              [0,-1]])\n\nprint(\"R^T R =\\n\", R.T @ R)\nprint(\"Determinant of R:\", np.linalg.det(R))\n\nR^T R =\n [[1 0]\n [0 1]]\nDeterminant of R: -1.0\n\n\nDeterminant = -1 → reflection.\n\nRandom orthogonal matrix via QR\n\n\nM = np.random.rand(3,3)\nQ, _ = np.linalg.qr(M)\nprint(\"Q (random orthogonal):\\n\", Q)\nprint(\"Check Q^T Q ≈ I:\\n\", np.round(Q.T @ Q, 6))\n\nQ (random orthogonal):\n [[-0.59472353  0.03725157 -0.80306677]\n [-0.61109913 -0.67000966  0.42147943]\n [-0.52236172  0.74141714  0.42123492]]\nCheck Q^T Q ≈ I:\n [[ 1.  0. -0.]\n [ 0.  1.  0.]\n [-0.  0.  1.]]\n\n\n\n\nTry It Yourself\n\nBuild a 2D rotation matrix for 45°. Verify it’s orthogonal.\nCheck whether scaling matrices (e.g., \\(\\text{diag}(2,1)\\)) are orthogonal. Why or why not?\nGenerate a random orthogonal matrix with np.linalg.qr and test its determinant.\n\n\n\nThe Takeaway\n\nOrthogonal matrices are rigid motions: they rotate or reflect without distorting lengths or angles.\nThey play a key role in numerical stability, geometry, and physics.\nEvery orthonormal basis corresponds to an orthogonal matrix.\n\n\n\n\n79. Fourier Viewpoint (Expanding in Orthogonal Waves)\nThe Fourier viewpoint treats functions or signals as combinations of orthogonal waves (sines and cosines). This is just linear algebra: sine and cosine functions form an orthogonal basis, and any signal can be expressed as a linear combination of them.\n\nFormula Recap\nFor a discrete signal \\(x\\), the Discrete Fourier Transform (DFT) is:\n\\[\nX_k = \\sum_{n=0}^{N-1} x_n e^{-2\\pi i kn / N}, \\quad k=0,\\dots,N-1\n\\]\nThe inverse DFT reconstructs the signal. Orthogonality of complex exponentials makes this work.\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild a simple signal\n\n\nt = np.linspace(0, 1, 100, endpoint=False)\nsignal = np.sin(2*np.pi*3*t) + 0.5*np.sin(2*np.pi*5*t)\nplt.plot(t, signal)\nplt.title(\"Signal = sin(3Hz) + 0.5 sin(5Hz)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCompute Fourier transform (DFT)\n\n\nX = np.fft.fft(signal)\nfreqs = np.fft.fftfreq(len(t), d=1/100)  # sampling rate = 100Hz\n\nplt.stem(freqs[:50], np.abs(X[:50]), basefmt=\" \")\nplt.title(\"Fourier spectrum\")\nplt.xlabel(\"Frequency (Hz)\")\nplt.ylabel(\"Magnitude\")\nplt.show()\n\n\n\n\n\n\n\n\nPeaks appear at 3Hz and 5Hz → the frequencies of the original signal.\n\nReconstruct signal using inverse FFT\n\n\nsignal_reconstructed = np.fft.ifft(X).real\nprint(\"Reconstruction error:\", np.linalg.norm(signal - signal_reconstructed))\n\nReconstruction error: 1.786526604658442e-15\n\n\nError is near zero → perfect reconstruction.\n\nOrthogonality check of sinusoids\n\n\nu = np.sin(2*np.pi*3*t)\nv = np.sin(2*np.pi*5*t)\n\ninner = np.dot(u, v)\nprint(\"Inner product of 3Hz and 5Hz sinusoids:\", inner)\n\nInner product of 3Hz and 5Hz sinusoids: 1.2982670494210424e-14\n\n\nThe result is ≈ 0 → confirms orthogonality.\n\n\nTry It Yourself\n\nChange the frequencies to 7Hz and 9Hz. Do the Fourier peaks move accordingly?\nMix in some noise and check how the spectrum looks.\nTry cosine signals instead of sine. Do you still see orthogonality?\n\n\n\nThe Takeaway\n\nFourier analysis = linear algebra with orthogonal sinusoidal basis functions.\nAny signal can be decomposed into orthogonal waves.\nThis orthogonal viewpoint powers audio, image compression, and signal processing.\n\n\n\n\n80. Polynomial and Multifeature Least Squares (Fitting More Flexibly)\nLeast squares isn’t limited to straight lines. By adding polynomial or multiple features, we can fit curves and capture more complex relationships. This is the foundation of regression models in data science.\n\nFormula Recap\nGiven data \\((x_i, y_i)\\), we build a design matrix \\(A\\):\n\nFor polynomial fit of degree \\(d\\):\n\n\\[\nA = \\begin{bmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{bmatrix}\n\\]\nThen solve least squares:\n\\[\n\\hat{c} = \\arg\\min_c \\|Ac - y\\|^2\n\\]\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nGenerate noisy quadratic data\n\n\nnp.random.seed(0)\nx = np.linspace(-3, 3, 30)\ny_true = 1 - 2*x + 0.5*x**2\ny_noisy = y_true + np.random.normal(scale=2.0, size=x.shape)\n\nplt.scatter(x, y_noisy, label=\"Noisy data\")\nplt.plot(x, y_true, \"g--\", label=\"True curve\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBuild polynomial design matrix (degree 2)\n\n\nA = np.column_stack([np.ones_like(x), x, x**2])\ncoeffs, *_ = np.linalg.lstsq(A, y_noisy, rcond=None)\nprint(\"Fitted coefficients:\", coeffs)\n\nFitted coefficients: [ 1.15666306 -2.25753954  0.72733812]\n\n\n\nPlot fitted polynomial\n\n\ny_fit = A @ coeffs\nplt.scatter(x, y_noisy, label=\"Noisy data\")\nplt.plot(x, y_fit, \"r-\", label=\"Fitted quadratic\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nHigher-degree fit (overfitting demonstration)\n\n\nA_high = np.column_stack([x**i for i in range(6)])  # degree 5\ncoeffs_high, *_ = np.linalg.lstsq(A_high, y_noisy, rcond=None)\n\ny_fit_high = A_high @ coeffs_high\nplt.scatter(x, y_noisy, label=\"Noisy data\")\nplt.plot(x, y_fit_high, \"r-\", label=\"Degree 5 polynomial\")\nplt.plot(x, y_true, \"g--\", label=\"True curve\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMultifeature regression example\n\nSuppose we predict \\(y\\) from features \\([x, x^2, \\sin(x)]\\):\n\nA_multi = np.column_stack([np.ones_like(x), x, x**2, np.sin(x)])\ncoeffs_multi, *_ = np.linalg.lstsq(A_multi, y_noisy, rcond=None)\nprint(\"Multi-feature coefficients:\", coeffs_multi)\n\nMulti-feature coefficients: [ 1.15666306 -2.0492999   0.72733812 -0.65902274]\n\n\n\n\nTry It Yourself\n\nFit degree 3, 4, 5 polynomials to the same data. Watch how the curve changes.\nAdd features like \\(\\cos(x)\\) or \\(\\exp(x)\\) - does the fit improve?\nCompare training error (fit to noisy data) vs error on new test points.\n\n\n\nThe Takeaway\n\nLeast squares can fit polynomials and arbitrary feature combinations.\nThe design matrix encodes how input variables transform into features.\nThis is the basis of regression, curve fitting, and many machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-9.-svd-pca-and-conditioning",
    "href": "books/en-US/lab.html#chapter-9.-svd-pca-and-conditioning",
    "title": "The LAB",
    "section": "Chapter 9. SVD, PCA, and Conditioning",
    "text": "Chapter 9. SVD, PCA, and Conditioning\n\n81. Singular Values and SVD (Universal Factorization)\nThe Singular Value Decomposition (SVD) is one of the most powerful results in linear algebra. It says any \\(m \\times n\\) matrix \\(A\\) can be factored as:\n\\[\nA = U \\Sigma V^T\n\\]\n\n\\(U\\): orthogonal \\(m \\times m\\) matrix (left singular vectors)\n\\(\\Sigma\\): diagonal \\(m \\times n\\) matrix with nonnegative numbers (singular values)\n\\(V\\): orthogonal \\(n \\times n\\) matrix (right singular vectors)\n\nSingular values are always nonnegative and sorted \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots\\).\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nCompute SVD of a matrix\n\n\nA = np.array([[3,1,1],\n              [-1,3,1]])\n\nU, S, Vt = np.linalg.svd(A, full_matrices=True)\n\nprint(\"U:\\n\", U)\nprint(\"Singular values:\", S)\nprint(\"V^T:\\n\", Vt)\n\nU:\n [[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nSingular values: [3.46410162 3.16227766]\nV^T:\n [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]\n [-8.94427191e-01  4.47213595e-01  5.26260748e-16]\n [-1.82574186e-01 -3.65148372e-01  9.12870929e-01]]\n\n\n\nU: orthogonal basis in input space.\nS: singular values (as a 1D array).\nV^T: orthogonal basis in output space.\n\n\nReconstruct \\(A\\) from decomposition\n\n\nSigma = np.zeros((U.shape[1], Vt.shape[0]))\nSigma[:len(S), :len(S)] = np.diag(S)\n\nA_reconstructed = U @ Sigma @ Vt\nprint(\"Reconstruction error:\", np.linalg.norm(A - A_reconstructed))\n\nReconstruction error: 1.709166621382058e-15\n\n\nThe error should be near zero.\n\nRank from SVD\n\nNumber of nonzero singular values = rank of \\(A\\).\n\nrank = np.sum(S &gt; 1e-10)\nprint(\"Rank of A:\", rank)\n\nRank of A: 2\n\n\n\nGeometry: effect of \\(A\\)\n\nSVD says:\n\n\\(V\\) rotates input space.\n\\(\\Sigma\\) scales along orthogonal directions (by singular values).\n\\(U\\) rotates to output space.\n\nThis explains why SVD works for any matrix (not just square ones).\n\nLow-rank approximation preview\n\nKeep only the top singular value(s) → best approximation of \\(A\\).\n\nk = 1\nA_approx = np.outer(U[:,0], Vt[0]) * S[0]\nprint(\"Rank-1 approximation:\\n\", A_approx)\n\nRank-1 approximation:\n [[1. 2. 1.]\n [1. 2. 1.]]\n\n\n\n\nTry It Yourself\n\nCompute SVD for a random 5×3 matrix. Check if \\(U\\) and \\(V\\) are orthogonal.\nCompare singular values of a diagonal matrix vs a rotation matrix.\nZero out small singular values and see how much of \\(A\\) is preserved.\n\n\n\nThe Takeaway\n\nSVD factorizes any matrix into rotations and scalings.\nSingular values reveal rank and strength of directions.\nIt’s the universal tool of numerical linear algebra: the backbone of PCA, compression, and stability analysis.\n\n\n\n\n82. Geometry of SVD (Rotations + Stretching)\nThe Singular Value Decomposition (SVD) has a beautiful geometric interpretation: every matrix is just a combination of two rotations (or reflections) and a stretching.\nFor \\(A = U \\Sigma V^T\\):\n\n\\(V^T\\): rotates (or reflects) the input space.\n\\(\\Sigma\\): stretches space along orthogonal axes by singular values \\(\\sigma_i\\).\n\\(U\\): rotates (or reflects) the result into the output space.\n\nThis turns any linear transformation into a rotation → stretching → rotation pipeline.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nMake a 2D matrix\n\n\nA = np.array([[2, 1],\n              [1, 3]])\n\n\nApply SVD\n\n\nU, S, Vt = np.linalg.svd(A)\n\nprint(\"U:\\n\", U)\nprint(\"Singular values:\", S)\nprint(\"V^T:\\n\", Vt)\n\nU:\n [[-0.52573111 -0.85065081]\n [-0.85065081  0.52573111]]\nSingular values: [3.61803399 1.38196601]\nV^T:\n [[-0.52573111 -0.85065081]\n [-0.85065081  0.52573111]]\n\n\n\nVisualize effect on the unit circle\n\nThe unit circle is often used to visualize linear transformations.\n\ntheta = np.linspace(0, 2*np.pi, 200)\ncircle = np.vstack((np.cos(theta), np.sin(theta)))\n\ntransformed = A @ circle\n\nplt.plot(circle[0], circle[1], 'b--', label=\"Unit circle\")\nplt.plot(transformed[0], transformed[1], 'r-', label=\"Transformed\")\nplt.axis(\"equal\")\nplt.legend()\nplt.title(\"Action of A on the unit circle\")\nplt.show()\n\n\n\n\n\n\n\n\nThe circle becomes an ellipse. Its axes align with the singular vectors, and its radii are the singular values.\n\nCompare with decomposition steps\n\n\n# Apply V^T\nstep1 = Vt @ circle\n# Apply Σ\nSigma = np.diag(S)\nstep2 = Sigma @ step1\n# Apply U\nstep3 = U @ step2\n\nplt.plot(circle[0], circle[1], 'b--', label=\"Unit circle\")\nplt.plot(step3[0], step3[1], 'g-', label=\"U Σ V^T circle\")\nplt.axis(\"equal\")\nplt.legend()\nplt.title(\"SVD decomposition of transformation\")\nplt.show()\n\n\n\n\n\n\n\n\nBoth transformed shapes match → confirms SVD’s geometric picture.\n\n\nTry It Yourself\n\nChange \\(A\\) to a pure shear, like [[1,2],[0,1]]. How does the ellipse look?\nTry a diagonal matrix, like [[3,0],[0,1]]. Do the singular vectors match the coordinate axes?\nScale the input circle to a square and see if geometry still works.\n\n\n\nThe Takeaway\n\nSVD = rotate → stretch → rotate.\nThe unit circle becomes an ellipse: axes = singular vectors, radii = singular values.\nThis geometric lens makes SVD intuitive and explains why it’s so widely used in data, graphics, and signal processing.\n\n\n\n\n83. Relation to Eigen-Decompositions (ATA and AAT)\nSingular values and eigenvalues are closely connected. While eigen-decomposition applies only to square matrices, SVD works for any rectangular matrix. The bridge between them is:\n\\[\nA^T A v = \\sigma^2 v \\quad \\text{and} \\quad A A^T u = \\sigma^2 u\n\\]\n\n\\(v\\): right singular vector (from eigenvectors of \\(A^T A\\))\n\\(u\\): left singular vector (from eigenvectors of \\(A A^T\\))\n\\(\\sigma\\): singular values (square roots of eigenvalues of \\(A^T A\\) or \\(A A^T\\))\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nDefine a rectangular matrix\n\n\nA = np.array([[2, 0],\n              [1, 1],\n              [0, 1]])  # shape 3x2\n\n\nCompute SVD directly\n\n\nU, S, Vt = np.linalg.svd(A)\nprint(\"Singular values:\", S)\n\nSingular values: [2.30277564 1.30277564]\n\n\n\nCompare with eigenvalues of \\(A^T A\\)\n\n\nATA = A.T @ A\neigvals, eigvecs = np.linalg.eig(ATA)\n\nprint(\"Eigenvalues of A^T A:\", eigvals)\nprint(\"Square roots (sorted):\", np.sqrt(np.sort(eigvals)[::-1]))\n\nEigenvalues of A^T A: [5.30277564 1.69722436]\nSquare roots (sorted): [2.30277564 1.30277564]\n\n\nNotice: singular values from SVD = square roots of eigenvalues of \\(A^T A\\).\n\nCompare with eigenvalues of \\(A A^T\\)\n\n\nAAT = A @ A.T\neigvals2, eigvecs2 = np.linalg.eig(AAT)\n\nprint(\"Eigenvalues of A A^T:\", eigvals2)\nprint(\"Square roots:\", np.sqrt(np.sort(eigvals2)[::-1]))\n\nEigenvalues of A A^T: [ 5.30277564e+00  1.69722436e+00 -2.15148422e-17]\nSquare roots: [2.30277564 1.30277564        nan]\n\n\n/tmp/ipykernel_2638/436251338.py:5: RuntimeWarning: invalid value encountered in sqrt\n  print(\"Square roots:\", np.sqrt(np.sort(eigvals2)[::-1]))\n\n\nThey match too → confirming the relationship.\n\nVerify singular vectors\n\n\nRight singular vectors (\\(V\\)) = eigenvectors of \\(A^T A\\).\nLeft singular vectors (\\(U\\)) = eigenvectors of \\(A A^T\\).\n\n\nprint(\"Right singular vectors (V):\\n\", Vt.T)\nprint(\"Eigenvectors of A^T A:\\n\", eigvecs)\n\nprint(\"Left singular vectors (U):\\n\", U)\nprint(\"Eigenvectors of A A^T:\\n\", eigvecs2)\n\nRight singular vectors (V):\n [[-0.95709203  0.28978415]\n [-0.28978415 -0.95709203]]\nEigenvectors of A^T A:\n [[ 0.95709203 -0.28978415]\n [ 0.28978415  0.95709203]]\nLeft singular vectors (U):\n [[-0.83125078  0.44487192  0.33333333]\n [-0.54146663 -0.51222011 -0.66666667]\n [-0.12584124 -0.73465607  0.66666667]]\nEigenvectors of A A^T:\n [[-0.83125078  0.44487192  0.33333333]\n [-0.54146663 -0.51222011 -0.66666667]\n [-0.12584124 -0.73465607  0.66666667]]\n\n\n\n\nTry It Yourself\n\nTry a square symmetric matrix and compare SVD with eigen-decomposition. Do they match?\nFor a tall vs wide rectangular matrix, check whether \\(U\\) and \\(V\\) differ.\nCompute eigenvalues manually with np.linalg.eig for a random \\(A\\) and confirm singular values.\n\n\n\nThe Takeaway\n\nSingular values = square roots of eigenvalues of \\(A^T A\\) (or \\(A A^T\\)).\nRight singular vectors = eigenvectors of \\(A^T A\\).\nLeft singular vectors = eigenvectors of \\(A A^T\\).\nSVD generalizes eigen-decomposition to all matrices, rectangular or square.\n\n\n\n\n84. Low-Rank Approximation (Best Small Models)\nOne of the most useful applications of SVD is low-rank approximation: compressing a large matrix into a smaller one while keeping most of the important information.\nThe Eckart–Young theorem says: If \\(A = U \\Sigma V^T\\), then the best rank-\\(k\\) approximation (in least-squares sense) is:\n\\[\nA_k = U_k \\Sigma_k V_k^T\n\\]\nwhere we keep only the top \\(k\\) singular values (and corresponding vectors).\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nCreate a matrix with hidden low-rank structure\n\n\nnp.random.seed(0)\nU = np.random.randn(50, 5)   # 50 x 5\nV = np.random.randn(5, 30)   # 5 x 30\nA = U @ V  # true rank ≤ 5\n\n\nFull SVD\n\n\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\nprint(\"Singular values:\", S[:10])\n\nSingular values: [4.90672194e+01 4.05935057e+01 3.39228766e+01 3.07883338e+01\n 2.29261740e+01 3.97150036e-15 3.97150036e-15 3.97150036e-15\n 3.97150036e-15 3.97150036e-15]\n\n\nOnly the first ~5 should be large; the rest close to zero.\n\nBuild rank-1 approximation\n\n\nk = 1\nA1 = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\nerror1 = np.linalg.norm(A - A1)\nprint(\"Rank-1 approximation error:\", error1)\n\nRank-1 approximation error: 65.36149641872868\n\n\n\nRank-5 approximation (should be almost exact)\n\n\nk = 5\nA5 = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\nerror5 = np.linalg.norm(A - A5)\nprint(\"Rank-5 approximation error:\", error5)\n\nRank-5 approximation error: 6.37596738696176e-14\n\n\n\nVisual comparison (image compression demo)\n\nLet’s see it on an image.\n\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nimg = digits.images[0]  # 8x8 grayscale digit\n\nU, S, Vt = np.linalg.svd(img, full_matrices=False)\n\n# Keep only top 2 singular values\nk = 2\nimg2 = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n\nplt.subplot(1,2,1)\nplt.imshow(img, cmap=\"gray\")\nplt.title(\"Original\")\n\nplt.subplot(1,2,2)\nplt.imshow(img2, cmap=\"gray\")\nplt.title(\"Rank-2 Approximation\")\nplt.show()\n\n\n\n\n\n\n\n\nEven with just 2 singular values, the digit shape is recognizable.\n\n\nTry It Yourself\n\nVary \\(k\\) in the image example (1, 2, 5, 10). How much detail do you keep?\nCompare the approximation error \\(\\|A - A_k\\|\\) as \\(k\\) increases.\nApply low-rank approximation to random noisy data. Does it denoise?\n\n\n\nThe Takeaway\n\nSVD gives the best possible low-rank approximation in terms of error.\nBy truncating singular values, you compress data while keeping its essential structure.\nThis is the backbone of image compression, recommender systems, and dimensionality reduction.\n\n\n\n\n85. Principal Component Analysis (Variance and Directions)\nPrincipal Component Analysis (PCA) is one of the most important applications of SVD. It finds the directions (principal components) where data varies the most, and projects the data onto them to reduce dimensionality while preserving as much information as possible.\nMathematically:\n\nCenter the data (subtract the mean).\nCompute covariance matrix \\(C = \\frac{1}{n} X^T X\\).\nEigenvectors of \\(C\\) = principal directions.\nEigenvalues = variance explained.\nEquivalently: PCA = SVD of centered data matrix.\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\n\n\n\nStep-by-Step Code Walkthrough\n\nGenerate synthetic 2D data\n\n\nnp.random.seed(0)\nX = np.random.randn(200, 2) @ np.array([[3,1],[1,0.5]])  # stretched cloud\n\nplt.scatter(X[:,0], X[:,1], alpha=0.3)\nplt.title(\"Original data\")\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCenter the data\n\n\nX_centered = X - X.mean(axis=0)\n\n\nCompute SVD\n\n\nU, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\nprint(\"Principal directions (V):\\n\", Vt)\n\nPrincipal directions (V):\n [[-0.94430098 -0.32908307]\n [ 0.32908307 -0.94430098]]\n\n\nRows of Vt are the principal components.\n\nProject data onto first component\n\n\nX_pca1 = X_centered @ Vt.T[:,0]\n\nplt.scatter(X_pca1, np.zeros_like(X_pca1), alpha=0.3)\nplt.title(\"Data projected on first principal component\")\nplt.show()\n\n\n\n\n\n\n\n\nThis collapses data into 1D, keeping the most variance.\n\nVisualize principal axes\n\n\nplt.scatter(X_centered[:,0], X_centered[:,1], alpha=0.3)\nfor length, vector in zip(S, Vt):\n    plt.plot([0, vector[0]*length], [0, vector[1]*length], 'r-', linewidth=3)\nplt.title(\"Principal components (directions of max variance)\")\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\nThe red arrows show where the data spreads most.\n\nPCA on real data (digits)\n\n\ndigits = load_digits()\nX = digits.data  # 1797 samples, 64 features\nX_centered = X - X.mean(axis=0)\n\nU, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n\nexplained_variance = (S**2) / np.sum(S**2)\nprint(\"Explained variance ratio (first 5):\", explained_variance[:5])\n\nExplained variance ratio (first 5): [0.14890594 0.13618771 0.11794594 0.08409979 0.05782415]\n\n\n\n\nTry It Yourself\n\nReduce digits dataset to 2D using the top 2 components and plot. Do digit clusters separate?\nCompare explained variance ratio for top 10 components.\nAdd noise to data and check if PCA filters it out when projecting to fewer dimensions.\n\n\n\nThe Takeaway\n\nPCA finds directions of maximum variance using SVD.\nBy projecting onto top components, you compress data with minimal information loss.\nPCA is the backbone of dimensionality reduction, visualization, and preprocessing in machine learning.\n\n\n\n\n86. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems\nThe Moore–Penrose pseudoinverse \\(A^+\\) generalizes the inverse of a matrix. It allows solving systems \\(Ax = b\\) even when:\n\n\\(A\\) is not square, or\n\\(A\\) is singular (non-invertible).\n\nThe solution given by the pseudoinverse is the least-squares solution with minimum norm:\n\\[\nx = A^+ b\n\\]\nIf \\(A = U \\Sigma V^T\\), then:\n\\[\nA^+ = V \\Sigma^+ U^T\n\\]\nwhere \\(\\Sigma^+\\) is obtained by taking reciprocals of nonzero singular values.\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nSolve an overdetermined system (more equations than unknowns)\n\n\nA = np.array([[1,1],\n              [1,2],\n              [1,3]])  # 3x2 system\nb = np.array([1,2,2])\n\nx_ls, *_ = np.linalg.lstsq(A, b, rcond=None)\nprint(\"Least-squares solution:\", x_ls)\n\nLeast-squares solution: [0.66666667 0.5       ]\n\n\n\nCompute with pseudoinverse\n\n\nA_pinv = np.linalg.pinv(A)\nx_pinv = A_pinv @ b\nprint(\"Pseudoinverse solution:\", x_pinv)\n\nPseudoinverse solution: [0.66666667 0.5       ]\n\n\nBoth match → pseudoinverse gives least-squares solution.\n\nSolve an underdetermined system (fewer equations than unknowns)\n\n\nA = np.array([[1,2,3]])  # 1x3\nb = np.array([1])\n\nx_pinv = np.linalg.pinv(A) @ b\nprint(\"Minimum norm solution:\", x_pinv)\n\nMinimum norm solution: [0.07142857 0.14285714 0.21428571]\n\n\nHere, infinitely many solutions exist. The pseudoinverse picks the one with smallest norm.\n\nCompare with singular matrix\n\n\nA = np.array([[1,2],\n              [2,4]])  # rank deficient\nb = np.array([1,2])\n\nx_pinv = np.linalg.pinv(A) @ b\nprint(\"Solution with pseudoinverse:\", x_pinv)\n\nSolution with pseudoinverse: [0.2 0.4]\n\n\nEven when \\(A\\) is singular, pseudoinverse provides a solution.\n\nManual pseudoinverse via SVD\n\n\nA = np.array([[1,2],\n              [3,4]])\nU, S, Vt = np.linalg.svd(A)\nS_inv = np.zeros((Vt.shape[0], U.shape[0]))\nfor i in range(len(S)):\n    if S[i] &gt; 1e-10:\n        S_inv[i,i] = 1/S[i]\n\nA_pinv_manual = Vt.T @ S_inv @ U.T\nprint(\"Manual pseudoinverse:\\n\", A_pinv_manual)\nprint(\"NumPy pseudoinverse:\\n\", np.linalg.pinv(A))\n\nManual pseudoinverse:\n [[-2.   1. ]\n [ 1.5 -0.5]]\nNumPy pseudoinverse:\n [[-2.   1. ]\n [ 1.5 -0.5]]\n\n\nThey match.\n\n\nTry It Yourself\n\nCreate an overdetermined system with noise and see how pseudoinverse smooths the solution.\nCompare pseudoinverse with direct inverse (np.linalg.inv) on a square nonsingular matrix.\nZero out small singular values manually and see how solution changes.\n\n\n\nThe Takeaway\n\nThe pseudoinverse solves any linear system, square or not.\nIt provides the least-squares solution in overdetermined cases and the minimum-norm solution in underdetermined cases.\nBuilt on SVD, it is a cornerstone of regression, optimization, and numerical methods.\n\n\n\n\n87. Conditioning and Sensitivity (How Errors Amplify)\nConditioning tells us how sensitive a system is to small changes. For a linear system \\(Ax = b\\):\n\nIf \\(A\\) is well-conditioned, small changes in \\(b\\) or \\(A\\) → small changes in \\(x\\).\nIf \\(A\\) is ill-conditioned, tiny changes can cause huge swings in \\(x\\).\n\nThe condition number is defined as:\n\\[\n\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\n\\]\nFor SVD:\n\\[\n\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\n\\]\nwhere \\(\\sigma_{\\max}\\) and \\(\\sigma_{\\min}\\) are the largest and smallest singular values.\n\nLarge \\(\\kappa(A)\\) → unstable system.\nSmall \\(\\kappa(A)\\) → stable system.\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nWell-conditioned system\n\n\nA = np.array([[2,0],\n              [0,1]])\nb = np.array([1,1])\n\nx = np.linalg.solve(A, b)\ncond = np.linalg.cond(A)\nprint(\"Solution:\", x)\nprint(\"Condition number:\", cond)\n\nSolution: [0.5 1. ]\nCondition number: 2.0\n\n\nCondition number = ratio of singular values → moderate size.\n\nIll-conditioned system\n\n\nA = np.array([[1, 1.0001],\n              [1, 1.0000]])\nb = np.array([2,2])\n\nx = np.linalg.lstsq(A, b, rcond=None)[0]\ncond = np.linalg.cond(A)\nprint(\"Solution:\", x)\nprint(\"Condition number:\", cond)\n\nSolution: [ 2.00000000e+00 -7.20128227e-17]\nCondition number: 40002.0000750375\n\n\nCondition number is very large → instability.\n\nPerturb the right-hand side\n\n\nb2 = np.array([2, 2.001])  # tiny change\nx2 = np.linalg.lstsq(A, b2, rcond=None)[0]\nprint(\"Solution after tiny change:\", x2)\n\nSolution after tiny change: [ 12.001 -10.   ]\n\n\nThe solution changes drastically → shows sensitivity.\n\nRelation to singular values\n\n\nU, S, Vt = np.linalg.svd(A)\nprint(\"Singular values:\", S)\nprint(\"Condition number (SVD):\", S[0]/S[-1])\n\nSingular values: [2.000050e+00 4.999875e-05]\nCondition number (SVD): 40002.0000750375\n\n\n\nScaling experiment\n\n\nfor scale in [1,1e-2,1e-4,1e-6]:\n    A = np.array([[1,0],[0,scale]])\n    print(f\"Scale={scale}, condition number={np.linalg.cond(A)}\")\n\nScale=1, condition number=1.0\nScale=0.01, condition number=100.0\nScale=0.0001, condition number=10000.0\nScale=1e-06, condition number=1000000.0\n\n\nAs scale shrinks, condition number explodes.\n\n\nTry It Yourself\n\nGenerate random matrices and compute their condition numbers. Which are stable?\nCompare condition numbers of Hilbert matrices (notoriously ill-conditioned).\nExplore how rounding errors grow with high condition numbers.\n\n\n\nThe Takeaway\n\nCondition number = measure of problem sensitivity.\n\\(\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}\\).\nIll-conditioned problems amplify errors and are numerically unstable → why scaling, regularization, and good formulations matter.\n\n\n\n\n88. Matrix Norms and Singular Values (Measuring Size Properly)\nMatrix norms measure the size or strength of a matrix. They extend the idea of vector length to matrices. Norms are crucial for analyzing stability, error growth, and performance of algorithms.\nSome important norms:\n\nFrobenius norm:\n\n\\[\n\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\n\\]\nEquivalent to treating the matrix as a big vector.\n\nSpectral norm (operator 2-norm):\n\n\\[\n\\|A\\|_2 = \\sigma_{\\max}\n\\]\nThe largest singular value - tells how much \\(A\\) can stretch a vector.\n\n1-norm: maximum absolute column sum.\n∞-norm: maximum absolute row sum.\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild a test matrix\n\n\nA = np.array([[1, -2, 3],\n              [0,  4, 5],\n              [-1, 2, 1]])\n\n\nCompute different norms\n\n\nfro = np.linalg.norm(A, 'fro')\nspec = np.linalg.norm(A, 2)\none_norm = np.linalg.norm(A, 1)\ninf_norm = np.linalg.norm(A, np.inf)\n\nprint(\"Frobenius norm:\", fro)\nprint(\"Spectral norm:\", spec)\nprint(\"1-norm:\", one_norm)\nprint(\"Infinity norm:\", inf_norm)\n\nFrobenius norm: 7.810249675906654\nSpectral norm: 6.813953458914003\n1-norm: 9.0\nInfinity norm: 9.0\n\n\n\nCompare spectral norm with largest singular value\n\n\nU, S, Vt = np.linalg.svd(A)\nprint(\"Largest singular value:\", S[0])\nprint(\"Spectral norm:\", spec)\n\nLargest singular value: 6.8139534589140025\nSpectral norm: 6.813953458914003\n\n\nThey match → spectral norm = largest singular value.\n\nFrobenius norm from singular values\n\n\\[\n\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\dots}\n\\]\n\nfro_from_svd = np.sqrt(np.sum(S**2))\nprint(\"Frobenius norm (from SVD):\", fro_from_svd)\n\nFrobenius norm (from SVD): 7.810249675906653\n\n\n\nStretching effect demonstration\n\nPick a random vector and see how much it grows:\n\nx = np.random.randn(3)\nstretch = np.linalg.norm(A @ x) / np.linalg.norm(x)\nprint(\"Stretch factor:\", stretch)\nprint(\"Spectral norm (max possible stretch):\", spec)\n\nStretch factor: 2.7537463268177698\nSpectral norm (max possible stretch): 6.813953458914003\n\n\nThe stretch ≤ spectral norm, always.\n\n\nTry It Yourself\n\nCompare norms for diagonal matrices - do they match the largest diagonal entry?\nGenerate random matrices and see how norms differ.\nCompute Frobenius vs spectral norm for a rank-1 matrix.\n\n\n\nThe Takeaway\n\nFrobenius norm = overall energy of the matrix.\nSpectral norm = maximum stretching power (largest singular value).\nOther norms (1-norm, ∞-norm) capture row/column dominance.\nSingular values unify all these views of “matrix size.”\n\n\n\n\n89. Regularization (Ridge/Tikhonov to Tame Instability)\nWhen solving \\(Ax = b\\), if \\(A\\) is ill-conditioned (large condition number), small errors in data can cause huge errors in the solution. Regularization stabilizes the problem by adding a penalty term that discourages extreme solutions.\nThe most common form: ridge regression (a.k.a. Tikhonov regularization):\n\\[\nx_\\lambda = \\arg\\min_x \\|Ax - b\\|^2 + \\lambda \\|x\\|^2\n\\]\nClosed form:\n\\[\nx_\\lambda = (A^T A + \\lambda I)^{-1} A^T b\n\\]\nHere \\(\\lambda &gt; 0\\) controls the amount of regularization:\n\nSmall \\(\\lambda\\): solution close to least-squares.\nLarge \\(\\lambda\\): smaller coefficients, more stability.\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild an ill-conditioned system\n\n\nA = np.array([[1, 1.001],\n              [1, 0.999]])\nb = np.array([2, 2])\n\n\nSolve without regularization\n\n\nx_ls, *_ = np.linalg.lstsq(A, b, rcond=None)\nprint(\"Least squares solution:\", x_ls)\n\nLeast squares solution: [2.0000000e+00 4.6705917e-17]\n\n\nThe result may be unstable.\n\nApply ridge regularization\n\n\nlam = 0.1\nx_ridge = np.linalg.inv(A.T @ A + lam*np.eye(2)) @ A.T @ b\nprint(\"Ridge solution (λ=0.1):\", x_ridge)\n\nRidge solution (λ=0.1): [0.97561927 0.97559976]\n\n\n\nCompare effect of different λ\n\n\nlambdas = np.logspace(-4, 2, 20)\nsolutions = []\nfor lam in lambdas:\n    x_reg = np.linalg.inv(A.T @ A + lam*np.eye(2)) @ A.T @ b\n    solutions.append(np.linalg.norm(x_reg))\n\nplt.semilogx(lambdas, solutions, 'o-')\nplt.xlabel(\"λ (regularization strength)\")\nplt.ylabel(\"Solution norm\")\nplt.title(\"Effect of ridge regularization\")\nplt.show()\n\n\n\n\n\n\n\n\nAs \\(\\lambda\\) increases, the solution becomes smaller and more stable.\n\nConnection to SVD\n\nIf \\(A = U \\Sigma V^T\\):\n\\[\nx_\\lambda = \\sum_i \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} (u_i^T b) v_i\n\\]\nSmall singular values (causing instability) get damped by \\(\\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}\\).\n\n\nTry It Yourself\n\nExperiment with larger and smaller \\(\\lambda\\). What happens to the solution?\nAdd random noise to \\(b\\). Compare least-squares vs ridge stability.\nPlot how each coefficient changes with λ.\n\n\n\nThe Takeaway\n\nRegularization controls instability in ill-conditioned problems.\nRidge regression balances fit vs. stability using λ.\nIn SVD terms, regularization damps small singular values that cause wild solutions.\n\n\n\n\n90. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)\nIn practice, we often need to determine the numerical rank of a matrix - not just the theoretical rank, but how many directions carry meaningful information beyond round-off errors or noise. A useful tool for this is the Rank-Revealing QR (RRQR) factorization.\nFor a matrix \\(A\\):\n\\[\nA P = Q R\n\\]\n\n\\(Q\\): orthogonal matrix\n\\(R\\): upper triangular matrix\n\\(P\\): column permutation matrix\n\nBy reordering columns smartly, the diagonal of \\(R\\) reveals which directions are significant.\n\nSet Up Your Lab\n\nimport numpy as np\nfrom scipy.linalg import qr\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild a nearly rank-deficient matrix\n\n\nA = np.array([[1, 2, 3],\n              [2, 4.001, 6],\n              [3, 6, 9.001]])\nprint(\"Rank (theoretical):\", np.linalg.matrix_rank(A))\n\nRank (theoretical): 3\n\n\nThis matrix is almost rank 2 but with small perturbations.\n\nQR with column pivoting\n\n\nQ, R, P = qr(A, pivoting=True)\nprint(\"R:\\n\", R)\nprint(\"Column permutation:\", P)\n\nR:\n [[-1.12257740e+01 -7.48384925e+00 -3.74165738e+00]\n [ 0.00000000e+00 -1.20185042e-03 -1.84886859e-04]\n [ 0.00000000e+00  0.00000000e+00 -7.41196374e-05]]\nColumn permutation: [2 1 0]\n\n\nThe diagonal entries of \\(R\\) decrease rapidly → numerical rank is determined where they become tiny.\n\nCompare with SVD\n\n\nU, S, Vt = np.linalg.svd(A)\nprint(\"Singular values:\", S)\n\nSingular values: [1.40009286e+01 1.00000000e-03 7.14238341e-05]\n\n\nThe singular values tell the same story: one is very small → effective rank ≈ 2.\n\nThresholding for rank\n\n\ntol = 1e-3\nrank_est = np.sum(S &gt; tol)\nprint(\"Estimated rank:\", rank_est)\n\nEstimated rank: 2\n\n\n\nDiagnostics on a noisy matrix\n\n\nnp.random.seed(0)\nB = np.random.randn(50, 10) @ np.random.randn(10, 10)  # rank ≤ 10\nB[:, -1] += 1e-6 * np.random.randn(50)  # tiny noise\n\nU, S, Vt = np.linalg.svd(B)\nplt.semilogy(S, 'o-')\nplt.title(\"Singular values (log scale)\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\nplt.show()\n\n\n\n\n\n\n\n\nThe drop in singular values shows effective rank.\n\n\nTry It Yourself\n\nChange the perturbation in \\(A\\) from 0.001 to 0.000001. Does the numerical rank change?\nTest QR with pivoting on random rectangular matrices.\nCompare rank estimates from QR vs SVD for large noisy matrices.\n\n\n\nThe Takeaway\n\nRank-revealing QR is a practical tool to detect effective rank in real-world data.\nSVD gives the most precise picture (singular values), but QR with pivoting is faster.\nUnderstanding numerical rank is crucial for diagnostics, stability, and model complexity control.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  },
  {
    "objectID": "books/en-US/lab.html#chapter-10.-applications-and-computation",
    "href": "books/en-US/lab.html#chapter-10.-applications-and-computation",
    "title": "The LAB",
    "section": "Chapter 10. Applications and computation",
    "text": "Chapter 10. Applications and computation\n\n91. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)\nLinear algebra powers the geometry pipelines in computer graphics and robotics.\n\n2D transforms: rotation, scaling, translation.\n3D transforms: same ideas, but with an extra dimension.\nHomogeneous coordinates let us unify all transforms (even translations) into matrix multiplications.\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nRotation in 2D\n\n\\[\nR(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix}\n\\]\n\ntheta = np.pi/4  # 45 degrees\nR = np.array([[np.cos(theta), -np.sin(theta)],\n              [np.sin(theta),  np.cos(theta)]])\n\npoint = np.array([1, 0])\nrotated = R @ point\n\nprint(\"Original:\", point)\nprint(\"Rotated:\", rotated)\n\nOriginal: [1 0]\nRotated: [0.70710678 0.70710678]\n\n\n\nTranslation using homogeneous coordinates\n\nIn 2D:\n\\[\nT(dx, dy) =\n\\begin{bmatrix}\n1 & 0 & dx \\\\\n0 & 1 & dy \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nT = np.array([[1,0,2],\n              [0,1,1],\n              [0,0,1]])\n\np_h = np.array([1,1,1])  # homogeneous (x=1,y=1)\ntranslated = T @ p_h\nprint(\"Translated point:\", translated)\n\nTranslated point: [3 2 1]\n\n\n\nCombine rotation + translation\n\nTransformations compose by multiplying matrices.\n\nM = T @ np.block([[R, np.zeros((2,1))],\n                  [np.zeros((1,2)), 1]])\ncombined = M @ p_h\nprint(\"Combined transform (rotation+translation):\", combined)\n\nCombined transform (rotation+translation): [2.         2.41421356 1.        ]\n\n\n\n3D rotation (around z-axis)\n\n\\[\nR_z(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta & 0 \\\\\n\\sin\\theta &  \\cos\\theta & 0 \\\\\n0          & 0           & 1\n\\end{bmatrix}\n\\]\n\ntheta = np.pi/3\nRz = np.array([[np.cos(theta), -np.sin(theta), 0],\n               [np.sin(theta),  np.cos(theta), 0],\n               [0,              0,             1]])\n\npoint3d = np.array([1,0,0])\nrotated3d = Rz @ point3d\nprint(\"3D rotated point:\", rotated3d)\n\n3D rotated point: [0.5       0.8660254 0.       ]\n\n\n\nCamera projection (3D → 2D)\n\nSimple pinhole model:\n\\[\n\\begin{bmatrix}\nx' \\\\\ny'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf \\cdot x / z \\\\\nf \\cdot y / z\n\\end{bmatrix}\n\\]\n\nf = 1.0  # focal length\nP = np.array([[f,0,0],\n              [0,f,0],\n              [0,0,1]])  # projection matrix\n\npoint3d = np.array([2,3,5])\np_proj = P @ point3d\np_proj = p_proj[:2] / p_proj[2]  # divide by z\nprint(\"Projected 2D point:\", p_proj)\n\nProjected 2D point: [0.4 0.6]\n\n\n\n\nTry It Yourself\n\nRotate a square in 2D, then translate it. Plot before/after.\nRotate a 3D point cloud around x, y, and z axes.\nProject a cube into 2D using the pinhole camera model.\n\n\n\nThe Takeaway\n\nGeometry pipelines = sequences of linear transforms.\nHomogeneous coordinates unify rotation, scaling, and translation.\nCamera projection links 3D world to 2D images - a cornerstone of graphics and vision.\n\n\n\n\n92. Computer Graphics and Robotics (Homogeneous Tricks in Action)\nComputer graphics and robotics both rely on homogeneous coordinates to unify rotations, translations, scalings, and projections into a single framework. With \\(4 \\times 4\\) matrices in 3D, entire transformation pipelines can be built as matrix products.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nHomogeneous representation of a point\n\nIn 3D:\n\\[\n(x, y, z) \\mapsto (x, y, z, 1)\n\\]\n\np = np.array([1,2,3,1])  # homogeneous point\n\n\nDefine translation, rotation, and scaling matrices\n\n\nTranslation by \\((dx,dy,dz)\\):\n\n\nT = np.array([[1,0,0,2],\n              [0,1,0,1],\n              [0,0,1,3],\n              [0,0,0,1]])\n\n\nScaling by factors \\((sx, sy, sz)\\):\n\n\nS = np.diag([2, 0.5, 1.5, 1])\n\n\nRotation about z-axis (\\(\\theta = 90^\\circ\\)):\n\n\ntheta = np.pi/2\nRz = np.array([[np.cos(theta), -np.sin(theta), 0, 0],\n               [np.sin(theta),  np.cos(theta), 0, 0],\n               [0,              0,             1, 0],\n               [0,              0,             0, 1]])\n\n\nCombine transforms into a pipeline\n\n\nM = T @ Rz @ S  # first scale, then rotate, then translate\np_transformed = M @ p\nprint(\"Transformed point:\", p_transformed)\n\nTransformed point: [1.  3.  7.5 1. ]\n\n\n\nRobotics: forward kinematics of a 2-link arm\n\nEach joint is a rotation + translation.\n\ndef link(theta, length):\n    return np.array([[np.cos(theta), -np.sin(theta), 0, length*np.cos(theta)],\n                     [np.sin(theta),  np.cos(theta), 0, length*np.sin(theta)],\n                     [0,              0,             1, 0],\n                     [0,              0,             0, 1]])\n\ntheta1, theta2 = np.pi/4, np.pi/6\nL1, L2 = 2, 1.5\n\nM1 = link(theta1, L1)\nM2 = link(theta2, L2)\n\nend_effector = M1 @ M2 @ np.array([0,0,0,1])\nprint(\"End effector position:\", end_effector[:3])\n\nEnd effector position: [1.80244213 2.8631023  0.        ]\n\n\n\nGraphics: simple 3D camera projection\n\n\nf = 2.0\nP = np.array([[f,0,0,0],\n              [0,f,0,0],\n              [0,0,1,0]])\n\ncube = np.array([[x,y,z,1] for x in [0,1] for y in [0,1] for z in [0,1]])\nproj = (P @ cube.T).T\nproj2d = proj[:,:2] / proj[:,2:3]\n\nplt.scatter(proj2d[:,0], proj2d[:,1])\nplt.title(\"Projected cube\")\nplt.show()\n\n/tmp/ipykernel_2638/2038614107.py:8: RuntimeWarning: divide by zero encountered in divide\n  proj2d = proj[:,:2] / proj[:,2:3]\n/tmp/ipykernel_2638/2038614107.py:8: RuntimeWarning: invalid value encountered in divide\n  proj2d = proj[:,:2] / proj[:,2:3]\n\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nChange order of transforms (Rz @ S @ T). How does the result differ?\nAdd a third joint to the robotic arm and compute new end-effector position.\nProject the cube with different focal lengths \\(f\\).\n\n\n\nThe Takeaway\n\nHomogeneous coordinates unify all transformations.\nRobotics uses this framework for forward kinematics.\nGraphics uses it for camera and projection pipelines.\nBoth fields rely on the same linear algebra tricks - just applied differently.\n\n\n\n\n93. Graphs, Adjacency, and Laplacians (Networks via Matrices)\nGraphs can be studied with linear algebra by encoding them into matrices. Two of the most important:\n\nAdjacency matrix \\(A\\):\n\\[\nA_{ij} =\n\\begin{cases}\n1 & \\text{if edge between i and j exists} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nGraph Laplacian \\(L\\):\n\\[\nL = D - A\n\\]\nwhere \\(D\\) is the degree matrix ($D_{ii} = $ number of neighbors of node \\(i\\)).\n\nThese matrices let us analyze connectivity, diffusion, and clustering.\n\nSet Up Your Lab\n\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild a simple graph\n\n\nG = nx.Graph()\nG.add_edges_from([(0,1), (1,2), (2,3), (3,0), (0,2)])  # square with diagonal\n\nnx.draw(G, with_labels=True, node_color=\"lightblue\", node_size=800)\nplt.show()\n\n\n\n\n\n\n\n\n\nAdjacency matrix\n\n\nA = nx.to_numpy_array(G)\nprint(\"Adjacency matrix:\\n\", A)\n\nAdjacency matrix:\n [[0. 1. 1. 1.]\n [1. 0. 1. 0.]\n [1. 1. 0. 1.]\n [1. 0. 1. 0.]]\n\n\n\nDegree and Laplacian matrices\n\n\nD = np.diag(A.sum(axis=1))\nL = D - A\nprint(\"Degree matrix:\\n\", D)\nprint(\"Graph Laplacian:\\n\", L)\n\nDegree matrix:\n [[3. 0. 0. 0.]\n [0. 2. 0. 0.]\n [0. 0. 3. 0.]\n [0. 0. 0. 2.]]\nGraph Laplacian:\n [[ 3. -1. -1. -1.]\n [-1.  2. -1.  0.]\n [-1. -1.  3. -1.]\n [-1.  0. -1.  2.]]\n\n\n\nEigenvalues of Laplacian (connectivity check)\n\n\neigvals, eigvecs = np.linalg.eigh(L)\nprint(\"Laplacian eigenvalues:\", eigvals)\n\nLaplacian eigenvalues: [1.11022302e-16 2.00000000e+00 4.00000000e+00 4.00000000e+00]\n\n\n\nThe number of zero eigenvalues = number of connected components.\n\n\nSpectral embedding (clustering)\n\nUse Laplacian eigenvectors to embed nodes in low dimensions.\n\ncoords = eigvecs[:,1:3]  # skip the trivial first eigenvector\nplt.scatter(coords[:,0], coords[:,1], c=range(len(coords)), cmap=\"tab10\", s=200)\nfor i, (x,y) in enumerate(coords):\n    plt.text(x, y, str(i), fontsize=12, ha=\"center\", va=\"center\", color=\"white\")\nplt.title(\"Spectral embedding of graph\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nRemove one edge from the graph and see how Laplacian eigenvalues change.\nAdd a disconnected node - does an extra zero eigenvalue appear?\nTry a random graph and compare adjacency vs Laplacian spectra.\n\n\n\nThe Takeaway\n\nAdjacency matrices describe direct graph structure.\nLaplacians capture connectivity and diffusion.\nEigenvalues of \\(L\\) reveal graph properties like connectedness and clustering - bridging networks with linear algebra.\n\n\n\n\n94. Data Preprocessing as Linear Ops (Centering, Whitening, Scaling)\nMany machine learning and data analysis workflows begin with preprocessing, and linear algebra provides the tools.\n\nCentering: subtract the mean → move data to origin.\nScaling: divide by standard deviation → normalize feature ranges.\nWhitening: decorrelate features → make covariance matrix the identity.\n\nEach step can be written as a matrix operation.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nGenerate correlated data\n\n\nnp.random.seed(0)\nX = np.random.randn(200, 2) @ np.array([[3,1],[1,0.5]])\nplt.scatter(X[:,0], X[:,1], alpha=0.4)\nplt.title(\"Original correlated data\")\nplt.axis(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCentering (subtract mean)\n\n\nX_centered = X - X.mean(axis=0)\nprint(\"Mean after centering:\", X_centered.mean(axis=0))\n\nMean after centering: [-7.10542736e-17 -1.33226763e-17]\n\n\n\nScaling (normalize features)\n\n\nX_scaled = X_centered / X_centered.std(axis=0)\nprint(\"Std after scaling:\", X_scaled.std(axis=0))\n\nStd after scaling: [1. 1.]\n\n\n\nWhitening via eigen-decomposition\n\nCovariance of centered data:\n\nC = np.cov(X_centered.T)\neigvals, eigvecs = np.linalg.eigh(C)\n\nW = eigvecs @ np.diag(1/np.sqrt(eigvals)) @ eigvecs.T\nX_white = X_centered @ W\n\nCheck covariance:\n\nprint(\"Whitened covariance:\\n\", np.cov(X_white.T))\n\nWhitened covariance:\n [[ 1.00000000e+00 -1.16104501e-14]\n [-1.16104501e-14  1.00000000e+00]]\n\n\n\nCompare scatter plots\n\n\nplt.subplot(1,3,1)\nplt.scatter(X[:,0], X[:,1], alpha=0.4)\nplt.title(\"Original\")\n\nplt.subplot(1,3,2)\nplt.scatter(X_scaled[:,0], X_scaled[:,1], alpha=0.4)\nplt.title(\"Scaled\")\n\nplt.subplot(1,3,3)\nplt.scatter(X_white[:,0], X_white[:,1], alpha=0.4)\nplt.title(\"Whitened\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nOriginal: elongated ellipse.\nScaled: axis-aligned ellipse.\nWhitened: circular cloud (uncorrelated, unit variance).\n\n\n\nTry It Yourself\n\nAdd a third feature and apply centering, scaling, whitening.\nCompare whitening with PCA - they use the same eigen-decomposition.\nTest what happens if you skip centering before whitening.\n\n\n\nThe Takeaway\n\nCentering → mean zero.\nScaling → unit variance.\nWhitening → features uncorrelated, variance = 1. Linear algebra provides the exact matrix operations to make preprocessing systematic and reliable.\n\n\n\n\n95. Linear Regression and Classification (From Model to Matrix)\nLinear regression and classification problems can be written neatly in matrix form. This unifies data, models, and solutions under the framework of least squares and linear decision boundaries.\n\nLinear Regression Model\nFor data \\((x_i, y_i)\\):\n\\[\ny \\approx X \\beta\n\\]\n\n\\(X\\): design matrix (rows = samples, columns = features).\n\\(\\beta\\): coefficients to solve for.\nSolution (least squares):\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n\\]\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n\n\nStep-by-Step Code Walkthrough\n\nLinear regression example\n\n\nnp.random.seed(0)\nX = np.linspace(0, 10, 30).reshape(-1,1)\ny = 3*X.squeeze() + 5 + np.random.randn(30)*2\n\nConstruct design matrix with bias term:\n\nX_design = np.column_stack([np.ones_like(X), X])\nbeta_hat, *_ = np.linalg.lstsq(X_design, y, rcond=None)\nprint(\"Fitted coefficients:\", beta_hat)\n\nFitted coefficients: [6.65833151 2.84547628]\n\n\n\nVisualize regression line\n\n\ny_pred = X_design @ beta_hat\n\nplt.scatter(X, y, label=\"Data\")\nplt.plot(X, y_pred, 'r-', label=\"Fitted line\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nLogistic classification with linear decision boundary\n\n\nXc, yc = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1, n_samples=100, random_state=0)\n\nplt.scatter(Xc[:,0], Xc[:,1], c=yc, cmap=\"bwr\", alpha=0.7)\nplt.title(\"Classification data\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLogistic regression via gradient descent\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\nX_design = np.column_stack([np.ones(len(Xc)), Xc])\ny = yc\n\nw = np.zeros(X_design.shape[1])\nlr = 0.1\n\nfor _ in range(2000):\n    preds = sigmoid(X_design @ w)\n    grad = X_design.T @ (preds - y) / len(y)\n    w -= lr * grad\n\nprint(\"Learned weights:\", w)\n\nLearned weights: [-2.10451116  0.70752542  4.13295129]\n\n\n\nPlot decision boundary\n\n\nxx, yy = np.meshgrid(np.linspace(Xc[:,0].min()-1, Xc[:,0].max()+1, 200),\n                     np.linspace(Xc[:,1].min()-1, Xc[:,1].max()+1, 200))\n\ngrid = np.c_[np.ones(xx.size), xx.ravel(), yy.ravel()]\nprobs = sigmoid(grid @ w).reshape(xx.shape)\n\nplt.contourf(xx, yy, probs, levels=[0,0.5,1], alpha=0.3, cmap=\"bwr\")\nplt.scatter(Xc[:,0], Xc[:,1], c=yc, cmap=\"bwr\", edgecolor=\"k\")\nplt.title(\"Linear decision boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nAdd polynomial features to regression and refit. Does the line bend into a curve?\nChange learning rate in logistic regression - what happens?\nGenerate data that is not linearly separable. Can a linear model still classify well?\n\n\n\nThe Takeaway\n\nRegression and classification fit naturally into linear algebra with matrix formulations.\nLeast squares solves regression directly; logistic regression requires optimization.\nLinear models are simple, interpretable, and still form the foundation of modern machine learning.\n\n\n\n\n96. PCA in Practice (Dimensionality Reduction Workflow)\nPrincipal Component Analysis (PCA) is widely used to reduce dimensions, compress data, and visualize high-dimensional datasets. Here, we’ll walk through a full PCA workflow: centering, computing components, projecting, and visualizing.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\n\n\n\nStep-by-Step Code Walkthrough\n\nLoad dataset (digits)\n\n\ndigits = load_digits()\nX = digits.data  # shape (1797, 64)\ny = digits.target\nprint(\"Data shape:\", X.shape)\n\nData shape: (1797, 64)\n\n\nEach sample is an 8×8 grayscale image flattened into 64 features.\n\nCenter the data\n\n\nX_centered = X - X.mean(axis=0)\n\n\nCompute PCA via SVD\n\n\nU, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\nexplained_variance = (S**2) / (len(X) - 1)\nexplained_ratio = explained_variance / explained_variance.sum()\n\n\nPlot explained variance ratio\n\n\nplt.plot(np.cumsum(explained_ratio[:30]), 'o-')\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.title(\"PCA explained variance\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis shows how many components are needed to capture most variance.\n\nProject onto top 2 components for visualization\n\n\nX_pca2 = X_centered @ Vt[:2].T\nplt.scatter(X_pca2[:,0], X_pca2[:,1], c=y, cmap=\"tab10\", alpha=0.6, s=15)\nplt.colorbar()\nplt.title(\"Digits dataset (PCA 2D projection)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nReconstruct images from reduced dimensions\n\n\nk = 20\nX_pca20 = X_centered @ Vt[:k].T\nX_reconstructed = X_pca20 @ Vt[:k]\n\nfig, axes = plt.subplots(2, 10, figsize=(10,2))\nfor i in range(10):\n    axes[0,i].imshow(X[i].reshape(8,8), cmap=\"gray\")\n    axes[0,i].axis(\"off\")\n    axes[1,i].imshow(X_reconstructed[i].reshape(8,8), cmap=\"gray\")\n    axes[1,i].axis(\"off\")\nplt.suptitle(\"Original (top) vs PCA reconstruction (bottom, 20 comps)\")\nplt.show()\n\n\n\n\n\n\n\n\nEven with only 20/64 components, the digits remain recognizable.\n\n\nTry It Yourself\n\nChange \\(k\\) to 5, 10, 30 - how do reconstructions change?\nUse top 2 PCA components to classify digits with k-NN. How does accuracy compare to full 64 features?\nTry PCA on your own dataset (images, tabular data).\n\n\n\nThe Takeaway\n\nPCA reduces dimensions while keeping maximum variance.\nIn practice: center → decompose → select top components → project/reconstruct.\nPCA enables visualization, compression, and denoising in real-world workflows.\n\n\n\n\n97. Recommender Systems and Low-Rank Models (Fill the Missing Entries)\nRecommender systems often deal with incomplete matrices - rows are users, columns are items, entries are ratings. Most entries are missing, but the matrix is usually close to low-rank (because user preferences depend on only a few hidden factors). SVD and low-rank approximations are powerful tools to fill in these missing values.\n\nSet Up Your Lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nSimulate a user–item rating matrix\n\n\nnp.random.seed(0)\ntrue_users = np.random.randn(10, 3)   # 10 users, 3 latent features\ntrue_items = np.random.randn(3, 8)    # 8 items\nR_full = true_users @ true_items      # true low-rank ratings\n\n\nHide some ratings (simulate missing data)\n\n\nmask = np.random.rand(*R_full.shape) &gt; 0.3  # keep 70% of entries\nR_obs = np.where(mask, R_full, np.nan)\n\nprint(\"Observed ratings:\\n\", R_obs)\n\nObserved ratings:\n [[-1.10781465         nan -3.56526968         nan -2.1729387   1.43510077\n   1.46641178  0.79023284]\n [ 0.84819453         nan         nan         nan         nan         nan\n   2.30434358  3.03008138]\n [        nan  0.32479187 -0.51818422         nan  0.02013802         nan\n   1.29874918  1.33053637]\n [-1.81407786  1.24241182         nan -1.32723907         nan         nan\n  -0.31110699         nan]\n [-0.48527696         nan -1.51957106         nan -0.86984941  0.52807989\n          nan  0.33771451]\n [-0.26997359 -0.48498966         nan -2.73891459 -2.48167957  2.88740609\n  -0.24614835         nan]\n [ 3.57769701 -1.608339    4.73789234  1.13583164  3.63451505 -2.60495928\n   2.12453635  3.76472563]\n [ 0.69623809 -0.59117353 -0.28890188 -2.36431192         nan  1.50136796\n   0.74268078         nan]\n [ 0.85768141  1.33357168         nan         nan  1.65089037 -2.46456289\n   3.51030491  3.31220347]\n [-2.463496    0.60826298 -3.81241599 -2.11839267 -3.86597359  3.52934055\n  -1.76203083 -2.63130953]]\n\n\n\nSimple mean imputation (baseline)\n\n\nR_mean = np.where(np.isnan(R_obs), np.nanmean(R_obs), R_obs)\n\n\nApply SVD for low-rank approximation\n\n\n# Replace NaNs with zeros for SVD step\nR_filled = np.nan_to_num(R_obs, nan=0.0)\n\nU, S, Vt = np.linalg.svd(R_filled, full_matrices=False)\n\nk = 3  # latent dimension\nR_approx = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n\n\nCompare filled matrix with ground truth\n\n\nerror = np.nanmean((R_full - R_approx)**2)\nprint(\"Approximation error (MSE):\", error)\n\nApproximation error (MSE): 1.4862378490976194\n\n\n\nVisualize original vs reconstructed\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8,4))\naxes[0].imshow(R_full, cmap=\"viridis\")\naxes[0].set_title(\"True ratings\")\naxes[1].imshow(R_approx, cmap=\"viridis\")\naxes[1].set_title(\"Low-rank approximation\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nVary \\(k\\) (2, 3, 5). Does error go down?\nMask more entries (50%, 80%) - how does SVD reconstruction perform?\nUse iterative imputation: alternate filling missing entries with low-rank approximations.\n\n\n\nThe Takeaway\n\nRecommender systems rely on low-rank structure of user–item matrices.\nSVD provides a natural way to approximate and fill missing ratings.\nThis low-rank modeling idea underpins modern collaborative filtering systems like Netflix and Spotify recommenders.\n\n\n\n\n98. PageRank and Random Walks (Ranking with Eigenvectors)\nThe PageRank algorithm, made famous by Google, uses linear algebra and random walks on graphs to rank nodes (webpages, people, items). The idea: importance flows through links - being linked by important nodes makes you important.\n\nThe PageRank Idea\n\nStart a random walk on a graph: at each step, move to a random neighbor.\nAdd a “teleportation” step with probability \\(1 - \\alpha\\) to avoid dead ends.\nThe steady-state distribution of this walk is the PageRank vector, found as the principal eigenvector of the transition matrix.\n\n\n\nSet Up Your Lab\n\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\n\nStep-by-Step Code Walkthrough\n\nBuild a small directed graph\n\n\nG = nx.DiGraph()\nG.add_edges_from([\n    (0,1), (1,2), (2,0),  # cycle among 0–1–2\n    (2,3), (3,2),         # back-and-forth 2–3\n    (1,3), (3,4), (4,1)   # small loop with 1–3–4\n])\nnx.draw_circular(G, with_labels=True, node_color=\"lightblue\", node_size=800, arrowsize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\nBuild adjacency and transition matrix\n\n\nn = G.number_of_nodes()\nA = nx.to_numpy_array(G, nodelist=range(n))\nP = A / A.sum(axis=1, keepdims=True)  # row-stochastic transition matrix\n\n\nAdd teleportation (Google matrix)\n\n\nalpha = 0.85  # damping factor\nG_matrix = alpha * P + (1 - alpha) * np.ones((n,n)) / n\n\n\nPower iteration to compute PageRank\n\n\nr = np.ones(n) / n  # start uniform\nfor _ in range(100):\n    r = r @ G_matrix\nr /= r.sum()\nprint(\"PageRank vector:\", r)\n\nPageRank vector: [0.13219034 0.25472358 0.24044787 0.24044787 0.13219034]\n\n\n\nCompare with NetworkX built-in\n\n\npr = nx.pagerank(G, alpha=alpha)\nprint(\"NetworkX PageRank:\", pr)\n\nNetworkX PageRank: {0: 0.13219008157546333, 1: 0.2547244023837789, 2: 0.24044771723264727, 3: 0.24044771723264727, 4: 0.13219008157546333}\n\n\n\nVisualize node importance\n\n\nsizes = [5000 * r_i for r_i in r]\nnx.draw_circular(G, with_labels=True, node_size=sizes, node_color=\"lightblue\", arrowsize=15)\nplt.title(\"PageRank visualization (node size ~ importance)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTry It Yourself\n\nChange \\(\\alpha\\) (e.g., 0.6 vs 0.95). Does ranking change?\nAdd a “dangling node” with no outlinks. How does teleportation handle it?\nTry PageRank on a larger graph (like a random graph with 50 nodes).\n\n\n\nThe Takeaway\n\nPageRank is a random-walk steady state problem.\nIt reduces to finding the dominant eigenvector of the Google matrix.\nThis method generalizes beyond webpages - to influence ranking, recommendation, and network analysis.\n\n\n\n\n99. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)\nWhen working with linear algebra on computers, numbers are not exact. They live in floating-point arithmetic, and computations rely on highly optimized libraries like BLAS and LAPACK. Understanding these essentials is crucial to doing linear algebra at scale.\n\nFloating Point Basics\n\nNumbers are stored in base-2 scientific notation:\n\\[\nx = \\pm (1.b_1b_2b_3\\ldots) \\times 2^e\n\\]\nLimited precision means rounding errors.\nTwo key constants:\n\nMachine epsilon ($\\(): smallest difference detectable (\\)^{-16}$ for double).\nOverflow/underflow: too large or too small to represent.\n\n\n\n\nSet Up Your Lab\n\nimport numpy as np\n\n\n\nStep-by-Step Code Walkthrough\n\nMachine epsilon\n\n\neps = np.finfo(float).eps\nprint(\"Machine epsilon:\", eps)\n\nMachine epsilon: 2.220446049250313e-16\n\n\n\nRound-off error demo\n\n\na = 1e16\nb = 1.0\nprint(\"a + b - a:\", (a + b) - a)  # may lose b due to precision limits\n\na + b - a: 0.0\n\n\n\nStability of matrix inversion\n\n\nA = np.array([[1, 1.0001], [1.0001, 1]])\nb = np.array([2, 2.0001])\n\nx_direct = np.linalg.solve(A, b)\nx_via_inv = np.linalg.inv(A) @ b\n\nprint(\"Solve:\", x_direct)\nprint(\"Inverse method:\", x_via_inv)\n\nSolve: [1.499975 0.499975]\nInverse method: [1.499975 0.499975]\n\n\nNotice: using np.linalg.inv can be less stable - better to solve directly.\n\nConditioning of a matrix\n\n\ncond = np.linalg.cond(A)\nprint(\"Condition number:\", cond)\n\nCondition number: 20000.999999985102\n\n\n\nLarge condition number → small input changes cause big output changes.\n\n\nBLAS/LAPACK under the hood\n\n\nA = np.random.randn(500, 500)\nB = np.random.randn(500, 500)\n\n# Matrix multiplication (calls optimized BLAS under the hood)\nC = A @ B\n\nThis @ operator is not a naive loop - it calls a highly optimized C/Fortran routine.\n\n\nTry It Yourself\n\nCompare solving Ax = b with np.linalg.solve vs np.linalg.inv(A) @ b for larger, ill-conditioned systems.\nUse np.linalg.svd on a nearly singular matrix. How stable are the singular values?\nCheck performance: time A @ B for sizes 100, 500, 1000.\n\n\n\nThe Takeaway\n\nNumerical linear algebra = math + floating-point reality.\nAlways prefer stable algorithms (solve, qr, svd) over naive inversion.\nLibraries like BLAS/LAPACK make large computations fast, but understanding precision and conditioning prevents nasty surprises.\n\n\n\n\n100. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)\nThis final section ties everything together. Instead of introducing a new topic, it provides capstone labs that combine multiple ideas from the book. Working through them will give you confidence that you can apply linear algebra to real problems.\n\nProblem Set 1 - Image Compression with SVD\nTake an image, treat it as a matrix, and approximate it with low-rank SVD.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data, color\n\n# Load grayscale image\nimg = color.rgb2gray(data.astronaut())\nU, S, Vt = np.linalg.svd(img, full_matrices=False)\n\n# Approximate with rank-k\nk = 50\nimg_approx = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n\nplt.subplot(1,2,1)\nplt.imshow(img, cmap=\"gray\")\nplt.title(\"Original\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\nplt.imshow(img_approx, cmap=\"gray\")\nplt.title(f\"Rank-{k} Approximation\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nTry different \\(k\\) values (5, 20, 100). How does quality vs. compression trade off?\n\n\nProblem Set 2 - Predictive Modeling with PCA + Regression\nCombine PCA for dimensionality reduction with linear regression for prediction.\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\n\n# Load dataset\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# PCA reduce features\npca = PCA(n_components=5)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Regression on reduced space\nmodel = LinearRegression().fit(X_train_pca, y_train)\nprint(\"R^2 on test set:\", model.score(X_test_pca, y_test))\n\nR^2 on test set: 0.3691398497153572\n\n\nDoes reducing dimensions improve or hurt accuracy?\n\n\nProblem Set 3 - Graph Analysis with PageRank\nApply PageRank to a custom-built network.\n\nimport networkx as nx\n\nG = nx.barabasi_albert_graph(20, 2)  # 20 nodes, scale-free graph\npr = nx.pagerank(G, alpha=0.85)\n\nnx.draw(G, with_labels=True, node_size=[5000*pr[n] for n in G], node_color=\"lightblue\")\nplt.title(\"PageRank on a scale-free graph\")\nplt.show()\n\n\n\n\n\n\n\n\nWhich nodes dominate? How does structure affect ranking?\n\n\nProblem Set 4 - Solving Differential Equations with Eigen Decomposition\nUse eigenvalues/eigenvectors to solve a linear dynamical system.\n\nA = np.array([[0,1],[-2,-3]])\neigvals, eigvecs = np.linalg.eig(A)\n\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\\n\", eigvecs)\n\nEigenvalues: [-1. -2.]\nEigenvectors:\n [[ 0.70710678 -0.4472136 ]\n [-0.70710678  0.89442719]]\n\n\nPredict long-term behavior: will the system decay, oscillate, or grow?\n\n\nProblem Set 5 - Least Squares for Overdetermined Systems\n\nnp.random.seed(0)\nX = np.random.randn(100, 3)\nbeta_true = np.array([2, -1, 0.5])\ny = X @ beta_true + np.random.randn(100)*0.1\n\nbeta_hat, *_ = np.linalg.lstsq(X, y, rcond=None)\nprint(\"Estimated coefficients:\", beta_hat)\n\nEstimated coefficients: [ 1.99371939 -1.00708947  0.50661857]\n\n\nCompare estimated vs. true coefficients. How close are they?\n\n\nTry It Yourself\n\nCombine SVD and recommender systems - build a movie recommender with synthetic data.\nImplement Gram–Schmidt by hand and test it against np.linalg.qr.\nWrite a mini “linear algebra toolkit” with your favorite helper functions.\n\n\n\nThe Takeaway\n\nYou’ve practiced vectors, matrices, systems, eigenvalues, SVD, PCA, PageRank, and more.\nReal problems often combine multiple concepts - the labs show how everything fits together.\nNext steps: dive deeper into numerical linear algebra, explore machine learning applications, or study advanced matrix factorizations (Jordan form, tensor decompositions).\n\nThis concludes the hands-on journey. By now, you don’t just know the theory - you can use linear algebra as a working tool in Python for data, science, and engineering.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The LAB</span>"
    ]
  }
]